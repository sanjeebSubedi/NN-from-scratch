{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb9e5cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6bab00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read names from file\n",
    "with open(\"names.txt\", \"r\") as f:\n",
    "    names = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5085cfc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['emma', 'olivia', 'ava']\n",
      "32033\n"
     ]
    }
   ],
   "source": [
    "names = [name.strip() for name in names]\n",
    "print(names[:3])\n",
    "print(len(names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "625269b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "letter_set = sorted(list({l for name in names for l in name}))\n",
    "letter_set.insert(0, '.')\n",
    "len(letter_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32b7d3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = {letter: pos for pos, letter in enumerate(letter_set)}\n",
    "itos = {pos: letter for letter, pos in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ddd5d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create all possible pairs of letters. The '.' never comes in the second place except at the start when we start with two dots.\n",
    "pairs = [(a, b) for a in letter_set  for b in letter_set if b!= '.']\n",
    "pairs.insert(0, ('.', '.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36d91337",
   "metadata": {},
   "outputs": [],
   "source": [
    "ptoi = {pair: pos for pos, pair in enumerate(pairs)}\n",
    "itop = {pos: pair for pair, pos in ptoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cead5d6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "703"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b150d447",
   "metadata": {},
   "source": [
    "### E01: train a trigram language model, i.e. take two characters as an input to predict the 3rd one. Feel free to use either counting or a neural net. Evaluate the loss; Did it improve over a bigram model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "13974487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 27 by 27 by 27 matrix to count all the number of occurences of trigrams\n",
    "lookup_table = torch.ones((703, 27), dtype=torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fec3e08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the lookup table with the counts of each trigram\n",
    "for name in names:\n",
    "    name = ['.', '.'] + list(name) + ['.']\n",
    "    for char1, char2, char3 in zip(name, name[1:], name[2:]):\n",
    "        p1, p2 = ptoi[(char1, char2)],  stoi[char3]\n",
    "        lookup_table[p1, p2] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e937f829",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_occurences(char1, char2, char3):\n",
    "    return lookup_table[ptoi[(char1, char2)], stoi[char3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bdbcbfc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4411, dtype=torch.int32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_occurences('.', '.', 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "691ca7be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(703.0001)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize the count table \n",
    "lookup_table = torch.div(lookup_table, torch.sum(lookup_table, dim=1, keepdims=True))\n",
    "torch.sum(lookup_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6ad1007a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " junide\n",
      " jakasid\n",
      " prelay\n",
      " adin\n",
      " kairritoper\n",
      " sathen\n",
      " sameia\n",
      " yanileniassibduinrwin\n",
      " lessiyanayla\n",
      " te\n"
     ]
    }
   ],
   "source": [
    "# Get some new predictions using the counts table\n",
    "gen = torch.Generator().manual_seed(2147483647)\n",
    "output_count = []\n",
    "for _ in range(10):\n",
    "    prev = ('.', '.')\n",
    "    out = []\n",
    "    while True:\n",
    "        idx = torch.multinomial(lookup_table[ptoi[prev]], num_samples=1, replacement=True, generator=gen).item()\n",
    "        if idx==0:\n",
    "            break\n",
    "        out.append(itos[idx])\n",
    "        prev = (prev[1], itos[idx])\n",
    "    output_count.append(\"\".join(out))\n",
    "    print(\"\", \"\".join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "89ab5498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_likelihood=tensor(-504653.)\n",
      "nll_loss: 2.2120\n",
      "bigram_loss: 2.4544\n"
     ]
    }
   ],
   "source": [
    "# Calculate the loss (negative log likelihood loss) and compare it to the loss of the bigram model (previously done)\n",
    "log_likelihood = 0.0 \n",
    "num_samples = 0\n",
    "for name in names:\n",
    "    name = ['.', '.'] + list(name) + ['.']\n",
    "    for char1, char2, char3 in zip(name, name[1:], name[2:]):\n",
    "        prob = lookup_table[ptoi[(char1, char2)], stoi[char3]]\n",
    "        log_likelihood += torch.log(prob)\n",
    "        num_samples += 1\n",
    "nll = -log_likelihood\n",
    "print(f\"{log_likelihood=}\")\n",
    "print(f\"nll_loss: {nll/num_samples:.4f}\")\n",
    "print(f\"bigram_loss: 2.4544\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1c8477af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 228146\n"
     ]
    }
   ],
   "source": [
    "# Create trigram samples to train a gradient based model\n",
    "xs, ys = [], []\n",
    "for name in names:\n",
    "    name = ['.', '.'] + list(name) + ['.']\n",
    "    for char1, char2, char3 in zip(name, name[1:], name[2:]):\n",
    "        xs.append(ptoi[(char1, char2)])\n",
    "        ys.append(stoi[char3])\n",
    "        \n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "print(f\"Number of samples: {xs.nelement()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "89e9b857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of encoded inputs: torch.Size([228146, 703])\n",
      "Shape of weights matrix: torch.Size([703, 27])\n",
      "Shape of labels vector: torch.Size([228146])\n"
     ]
    }
   ],
   "source": [
    "# Change the inputs into one hot vectors and initialize the weights\n",
    "x_oh = torch.nn.functional.one_hot(xs, num_classes=703).float()\n",
    "W = torch.randn((703, 27), requires_grad=True)\n",
    "print(f\"Shape of encoded inputs: {x_oh.shape}\")\n",
    "print(f\"Shape of weights matrix: {W.shape}\")\n",
    "print(f\"Shape of labels vector: {ys.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "06b4ba25",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# W = W.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9d41efcd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 3.7144980430603027\n",
      "Loss: 3.0218570232391357\n",
      "Loss: 2.807922124862671\n",
      "Loss: 2.6918089389801025\n",
      "Loss: 2.6155264377593994\n",
      "Loss: 2.560713052749634\n",
      "Loss: 2.5190727710723877\n",
      "Loss: 2.4862024784088135\n",
      "Loss: 2.459519147872925\n",
      "Loss: 2.437392234802246\n",
      "Loss: 2.418728828430176\n",
      "Loss: 2.4027678966522217\n",
      "Loss: 2.38895845413208\n",
      "Loss: 2.376892566680908\n",
      "Loss: 2.3662590980529785\n",
      "Loss: 2.356815814971924\n",
      "Loss: 2.348371982574463\n",
      "Loss: 2.3407750129699707\n",
      "Loss: 2.3339014053344727\n",
      "Loss: 2.3276500701904297\n",
      "Loss: 2.3219377994537354\n",
      "Loss: 2.3166956901550293\n",
      "Loss: 2.3118667602539062\n",
      "Loss: 2.307401657104492\n",
      "Loss: 2.303260326385498\n",
      "Loss: 2.2994070053100586\n",
      "Loss: 2.2958123683929443\n",
      "Loss: 2.2924511432647705\n",
      "Loss: 2.289299964904785\n",
      "Loss: 2.2863399982452393\n",
      "Loss: 2.2835543155670166\n",
      "Loss: 2.280927896499634\n",
      "Loss: 2.278447389602661\n",
      "Loss: 2.2761011123657227\n",
      "Loss: 2.273878574371338\n",
      "Loss: 2.2717700004577637\n",
      "Loss: 2.2697675228118896\n",
      "Loss: 2.2678627967834473\n",
      "Loss: 2.266049385070801\n",
      "Loss: 2.2643210887908936\n",
      "Loss: 2.262671709060669\n",
      "Loss: 2.261096239089966\n",
      "Loss: 2.259589910507202\n",
      "Loss: 2.258148193359375\n",
      "Loss: 2.2567672729492188\n",
      "Loss: 2.2554430961608887\n",
      "Loss: 2.2541723251342773\n",
      "Loss: 2.2529520988464355\n",
      "Loss: 2.251779317855835\n",
      "Loss: 2.2506508827209473\n",
      "Loss: 2.2495648860931396\n",
      "Loss: 2.248518705368042\n",
      "Loss: 2.2475099563598633\n",
      "Loss: 2.24653697013855\n",
      "Loss: 2.2455976009368896\n",
      "Loss: 2.244690418243408\n",
      "Loss: 2.2438135147094727\n",
      "Loss: 2.2429654598236084\n",
      "Loss: 2.24214506149292\n",
      "Loss: 2.2413504123687744\n",
      "Loss: 2.2405807971954346\n",
      "Loss: 2.2398345470428467\n",
      "Loss: 2.2391114234924316\n",
      "Loss: 2.2384092807769775\n",
      "Loss: 2.2377278804779053\n",
      "Loss: 2.2370662689208984\n",
      "Loss: 2.2364237308502197\n",
      "Loss: 2.2357988357543945\n",
      "Loss: 2.2351911067962646\n",
      "Loss: 2.234600067138672\n",
      "Loss: 2.2340245246887207\n",
      "Loss: 2.233464479446411\n",
      "Loss: 2.2329187393188477\n",
      "Loss: 2.232387065887451\n",
      "Loss: 2.2318689823150635\n",
      "Loss: 2.2313637733459473\n",
      "Loss: 2.2308709621429443\n",
      "Loss: 2.2303898334503174\n",
      "Loss: 2.2299206256866455\n",
      "Loss: 2.229462146759033\n",
      "Loss: 2.2290148735046387\n",
      "Loss: 2.228577136993408\n",
      "Loss: 2.2281501293182373\n",
      "Loss: 2.2277321815490723\n",
      "Loss: 2.2273237705230713\n",
      "Loss: 2.226924180984497\n",
      "Loss: 2.2265331745147705\n",
      "Loss: 2.2261507511138916\n",
      "Loss: 2.225776433944702\n",
      "Loss: 2.225409746170044\n",
      "Loss: 2.225050687789917\n",
      "Loss: 2.224699020385742\n",
      "Loss: 2.2243545055389404\n",
      "Loss: 2.2240166664123535\n",
      "Loss: 2.2236857414245605\n",
      "Loss: 2.2233612537384033\n",
      "Loss: 2.2230429649353027\n",
      "Loss: 2.2227306365966797\n",
      "Loss: 2.2224245071411133\n",
      "Loss: 2.222123861312866\n",
      "Loss: 2.2218291759490967\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "for i in range(2001):\n",
    "    logits = torch.matmul(x_oh.to(device), W.to(device))\n",
    "    counts = logits.exp() # Exponentiate to get the counts \n",
    "    probs = torch.div(counts, torch.sum(counts, dim=1, keepdims=True)) # Normalize the counts\n",
    "    loss = -probs[torch.arange(x_oh.shape[0]), ys].log().mean() + 0.001 * (W ** 2).mean() # Calculate the nll loss\n",
    "    if i % 20 == 0:\n",
    "        print(f\"Loss: {loss.item()}\")\n",
    "    W.grad = None # clear the gradients\n",
    "    loss.backward()\n",
    "#     W.retain_grad()\n",
    "    W.data += -40 * W.grad # update the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "adced769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on bigram (gradient based): 2.4804\n",
      "Loss on trigram: 2.2218\n"
     ]
    }
   ],
   "source": [
    "# Comparing the loss of bigram model and trigram model\n",
    "print(\"Loss on bigram (gradient based): 2.4804\")\n",
    "print(f\"Loss on trigram: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2dfb25d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([703, 27])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6805dbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some names and compare with the names from counting method\n",
    "gen = torch.Generator(device=device).manual_seed(2147483647)\n",
    "output_gradient = []\n",
    "for _ in range(10):\n",
    "    out = []\n",
    "    char1 = '.'\n",
    "    char2 = '.'\n",
    "    \n",
    "    while True:\n",
    "        x_enc = torch.nn.functional.one_hot(torch.tensor([ptoi[(char1, char2)]]), num_classes=703).float().to(device)\n",
    "        \n",
    "        logits = torch.matmul(x_enc.to(device), W.to(device))\n",
    "        counts = logits.exp()\n",
    "        probs = torch.div(counts, torch.sum(counts, dim=1, keepdims=True))\n",
    "        \n",
    "        idx = torch.multinomial(probs, num_samples=1, replacement=True, generator=gen).item()\n",
    "        if idx == 0:\n",
    "            break\n",
    "        char1 = char2\n",
    "        char2 = itos[idx]\n",
    "        out.append(itos[idx])\n",
    "    output_gradient.append(\"\".join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c6267e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting based:          \tGradient based: \n",
      "----------------------------------------------------\n",
      "junide                   \tkhya                \n",
      "jakasid                  \tepwktonn            \n",
      "prelay                   \tyulagolbiahen       \n",
      "adin                     \tramsiyamicxonnan    \n",
      "kairritoper              \trine                \n",
      "sathen                   \tdelenlian           \n",
      "sameia                   \termarishan          \n",
      "yanileniassibduinrwin    \tany                 \n",
      "lessiyanayla             \taleedon             \n",
      "te                       \tlyashily            \n"
     ]
    }
   ],
   "source": [
    "print(\"{:<25}\\t{}\".format(\"Counting based: \", \"Gradient based: \"))\n",
    "print(\"----------------------------------------------------\")\n",
    "for out1, out2 in zip(output_count, output_gradient):\n",
    "    print(\"{:<25}\\t{:<20}\".format(out1, out2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a307d4",
   "metadata": {},
   "source": [
    "### E02: split up the dataset randomly into 80% train set, 10% dev set, 10% test set. Train the bigram and trigram models only on the training set. Evaluate them on dev and test splits. What can you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aaa81b80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([228146, 703]), torch.Size([228146]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_oh.shape, ys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "95865da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182517, 703]) torch.Size([182517])\n",
      "torch.Size([22815, 703]) torch.Size([22815])\n",
      "torch.Size([22814, 703]) torch.Size([22814])\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into train, validation and test sets\n",
    "train_idx, val_idx, test_idx = torch.utils.data.random_split(range(x_oh.shape[0]), [0.8, 0.1, 0.1])\n",
    "\n",
    "xs_train, ys_train = x_oh[train_idx], ys[train_idx]\n",
    "xs_val, ys_val = x_oh[val_idx], ys[val_idx]\n",
    "xs_test, ys_test = x_oh[test_idx], ys[test_idx]\n",
    "\n",
    "print(xs_train.shape, ys_train.shape)\n",
    "print(xs_val.shape, ys_val.shape)\n",
    "print(xs_test.shape, ys_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "83cac8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize new weights\n",
    "W = torch.randn((703, 27), requires_grad=True) # weights matrix\n",
    "R = 0.001 # regulatization loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "85d20070",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(inputs, weights):\n",
    "    \"\"\"Perform a matrix multiplication between the input and weights tensor and return probabilities after exponentiating and normalizing\"\"\"\n",
    "    logits = torch.matmul(inputs.to(device), weights.to(device))\n",
    "    counts = logits.exp()\n",
    "    probs = torch.div(counts, torch.sum(counts, dim=1, keepdims=True))\n",
    "    return probs\n",
    "\n",
    "def nll_loss(probs, n_inputs, labels, lbd=0.01):\n",
    "    \"\"\"Calculate the negative log likelihood loss and apply model smoothing with regularization\"\"\"\n",
    "    return -probs[torch.arange(n_inputs), labels].log().mean() + lbd * (W**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4e0b6754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.7686 \t Val Loss: 3.7647\n",
      "Training Loss: 2.8360 \t Val Loss: 2.8389\n",
      "Training Loss: 2.6344 \t Val Loss: 2.6421\n",
      "Training Loss: 2.5342 \t Val Loss: 2.5453\n",
      "Training Loss: 2.4736 \t Val Loss: 2.4873\n",
      "Training Loss: 2.4323 \t Val Loss: 2.4481\n",
      "Training Loss: 2.4022 \t Val Loss: 2.4195\n",
      "Training Loss: 2.3791 \t Val Loss: 2.3977\n",
      "Training Loss: 2.3609 \t Val Loss: 2.3806\n",
      "Training Loss: 2.3462 \t Val Loss: 2.3666\n",
      "Training Loss: 2.3339 \t Val Loss: 2.3551\n",
      "Training Loss: 2.3236 \t Val Loss: 2.3454\n",
      "Training Loss: 2.3148 \t Val Loss: 2.3371\n",
      "Training Loss: 2.3072 \t Val Loss: 2.3300\n",
      "Training Loss: 2.3005 \t Val Loss: 2.3238\n",
      "Training Loss: 2.2946 \t Val Loss: 2.3184\n",
      "Training Loss: 2.2894 \t Val Loss: 2.3136\n",
      "Training Loss: 2.2848 \t Val Loss: 2.3093\n",
      "Training Loss: 2.2806 \t Val Loss: 2.3055\n",
      "Training Loss: 2.2768 \t Val Loss: 2.3021\n",
      "Training Loss: 2.2734 \t Val Loss: 2.2990\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "for i in range(1001):\n",
    "    probs = forward_pass(xs_train, W)\n",
    "    loss = nll_loss(probs, xs_train.shape[0], ys_train)\n",
    "    if i % 50 == 0:\n",
    "        val_probs = forward_pass(xs_val, W)\n",
    "        val_loss = nll_loss(val_probs, xs_val.shape[0], ys_val)\n",
    "        print(f\"Training Loss: {loss.item():.4f} \\t Val Loss: {val_loss.item():.4f}\")\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "    W.data += -30 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "63af3055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 2.2926\n"
     ]
    }
   ],
   "source": [
    "# Calculate the loss on the test set\n",
    "test_loss = nll_loss(forward_pass(xs_test, W), len(xs_test), ys_test)\n",
    "print(f\"Test loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33dabea3",
   "metadata": {},
   "source": [
    "<p> The loss is a bit higher on the validation and test sets as compared to the train sets. But thats totally fine as long as the difference is small. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869a69a3",
   "metadata": {},
   "source": [
    "### E03: use the dev set to tune the strength of smoothing (or regularization) for the trigram model - i.e. try many possibilities and see which one works best based on the dev set loss. What patterns can you see in the train and dev set loss as you tune this strength? Take the best setting of the smoothing and evaluate on the test set once and at the end. How good of a loss do you achieve?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da71fdf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(columns=[\"lbd\", \"val_loss\"])\n",
    "\n",
    "for lbd in [0.1, 0.007, 0.004, 0.001, 0.0007, 0.0004, 0.0001, 0.00007, 0.00004, 0.00001, 0.0]:\n",
    "    val_probs = forward_pass(xs_val, W)\n",
    "    val_loss = -val_probs[torch.arange(xs_val.shape[0]), ys_val].log().mean() + lbd * (W**2).mean()\n",
    "    df = df.append({\"lbd\": lbd, \"val_loss\": val_loss.item()}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "38f7f209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lbd</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.10000</td>\n",
       "      <td>2.404053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00700</td>\n",
       "      <td>2.295467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00400</td>\n",
       "      <td>2.291964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00100</td>\n",
       "      <td>2.288461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00070</td>\n",
       "      <td>2.288111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.00040</td>\n",
       "      <td>2.287761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>2.287411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.00007</td>\n",
       "      <td>2.287376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.00004</td>\n",
       "      <td>2.287341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.00001</td>\n",
       "      <td>2.287306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>2.287294</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        lbd  val_loss\n",
       "0   0.10000  2.404053\n",
       "1   0.00700  2.295467\n",
       "2   0.00400  2.291964\n",
       "3   0.00100  2.288461\n",
       "4   0.00070  2.288111\n",
       "5   0.00040  2.287761\n",
       "6   0.00010  2.287411\n",
       "7   0.00007  2.287376\n",
       "8   0.00004  2.287341\n",
       "9   0.00001  2.287306\n",
       "10  0.00000  2.287294"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7347a0a",
   "metadata": {},
   "source": [
    "<p> Model works best without any regularization. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "93370be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 2.2810\n"
     ]
    }
   ],
   "source": [
    "# Check the loss on the test set with the suitable value regularization parameter\n",
    "test_loss = nll_loss(forward_pass(xs_test, W), len(xs_test), ys_test, 0.0)\n",
    "print(f\"Test loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9d8741",
   "metadata": {},
   "source": [
    "### E04: we saw that our 1-hot vectors merely select a row of W, so producing these vectors explicitly feels wasteful. Can you delete our use of F.one_hot in favor of simply indexing into rows of W?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659e94ac",
   "metadata": {},
   "source": [
    "<p> The vector xs contains the inputs without the one-hot encoding. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "33230fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx, val_idx = torch.utils.data.random_split(range(xs.shape[0]), [0.8, 0.2])\n",
    "xs_train, ys_train = xs[train_idx], ys[train_idx]\n",
    "xs_val, ys_val = xs[val_idx], ys[val_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7d4f71fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(inputs, weights):\n",
    "    \"\"\"Perform a matrix multiplication between the input and weights tensor and return probabilities after exponentiating and normalizing\"\"\"\n",
    "#     logits = torch.matmul(inputs.to(device), weights.to(device))\n",
    "    logits = weights[inputs] # The input matrix consists the indexes which should be plucked from the weights matrix. \n",
    "    counts = logits.exp()\n",
    "    probs = torch.div(counts, torch.sum(counts, dim=1, keepdims=True))\n",
    "    return probs\n",
    "\n",
    "def nll_loss(probs, n_inputs, labels, lbd=0.01):\n",
    "    \"\"\"Calculate the negative log likelihood loss and apply model smoothing with regularization\"\"\"\n",
    "    return -probs[torch.arange(n_inputs), labels].log().mean() + lbd * (W**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e45e163b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 2.2645 \t Val Loss: 2.2730\n",
      "Training Loss: 2.2600 \t Val Loss: 2.2726\n",
      "Training Loss: 2.2564 \t Val Loss: 2.2714\n",
      "Training Loss: 2.2533 \t Val Loss: 2.2701\n",
      "Training Loss: 2.2504 \t Val Loss: 2.2687\n",
      "Training Loss: 2.2478 \t Val Loss: 2.2673\n",
      "Training Loss: 2.2455 \t Val Loss: 2.2660\n",
      "Training Loss: 2.2433 \t Val Loss: 2.2648\n",
      "Training Loss: 2.2412 \t Val Loss: 2.2636\n",
      "Training Loss: 2.2393 \t Val Loss: 2.2624\n",
      "Training Loss: 2.2376 \t Val Loss: 2.2614\n",
      "Training Loss: 2.2359 \t Val Loss: 2.2604\n",
      "Training Loss: 2.2343 \t Val Loss: 2.2594\n",
      "Training Loss: 2.2329 \t Val Loss: 2.2585\n",
      "Training Loss: 2.2315 \t Val Loss: 2.2577\n",
      "Training Loss: 2.2302 \t Val Loss: 2.2568\n",
      "Training Loss: 2.2290 \t Val Loss: 2.2561\n",
      "Training Loss: 2.2278 \t Val Loss: 2.2553\n",
      "Training Loss: 2.2267 \t Val Loss: 2.2546\n",
      "Training Loss: 2.2256 \t Val Loss: 2.2540\n",
      "Training Loss: 2.2246 \t Val Loss: 2.2533\n"
     ]
    }
   ],
   "source": [
    "for i in range(1001):\n",
    "    probs = forward_pass(xs_train, W)\n",
    "    loss = nll_loss(probs, xs_train.shape[0], ys_train, 0.0)\n",
    "    if i % 50 == 0:\n",
    "        val_probs = forward_pass(xs_val, W)\n",
    "        val_loss = nll_loss(val_probs, xs_val.shape[0], ys_val, 0.0)\n",
    "        print(f\"Training Loss: {loss.item():.4f} \\t Val Loss: {val_loss.item():.4f}\")\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "    W.data += -30 * W.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9569763e",
   "metadata": {},
   "source": [
    "### E05: look up and use F.cross_entropy instead. You should achieve the same result. Can you think of why we'd prefer to use F.cross_entropy instead?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4a4b06bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.rand((703, 27), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "be29f404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.3491 \t Val Loss: 3.3458\n",
      "Training Loss: 2.6668 \t Val Loss: 2.6717\n",
      "Training Loss: 2.5195 \t Val Loss: 2.5287\n",
      "Training Loss: 2.4467 \t Val Loss: 2.4595\n",
      "Training Loss: 2.4021 \t Val Loss: 2.4175\n",
      "Training Loss: 2.3715 \t Val Loss: 2.3889\n",
      "Training Loss: 2.3490 \t Val Loss: 2.3680\n",
      "Training Loss: 2.3316 \t Val Loss: 2.3520\n",
      "Training Loss: 2.3178 \t Val Loss: 2.3393\n",
      "Training Loss: 2.3065 \t Val Loss: 2.3290\n",
      "Training Loss: 2.2971 \t Val Loss: 2.3205\n",
      "Training Loss: 2.2891 \t Val Loss: 2.3133\n",
      "Training Loss: 2.2822 \t Val Loss: 2.3071\n",
      "Training Loss: 2.2763 \t Val Loss: 2.3018\n",
      "Training Loss: 2.2710 \t Val Loss: 2.2972\n",
      "Training Loss: 2.2664 \t Val Loss: 2.2931\n",
      "Training Loss: 2.2623 \t Val Loss: 2.2895\n",
      "Training Loss: 2.2586 \t Val Loss: 2.2863\n",
      "Training Loss: 2.2553 \t Val Loss: 2.2834\n",
      "Training Loss: 2.2523 \t Val Loss: 2.2807\n",
      "Training Loss: 2.2495 \t Val Loss: 2.2784\n",
      "Training Loss: 2.2470 \t Val Loss: 2.2762\n",
      "Training Loss: 2.2447 \t Val Loss: 2.2742\n",
      "Training Loss: 2.2425 \t Val Loss: 2.2724\n",
      "Training Loss: 2.2406 \t Val Loss: 2.2707\n",
      "Training Loss: 2.2387 \t Val Loss: 2.2691\n",
      "Training Loss: 2.2370 \t Val Loss: 2.2677\n",
      "Training Loss: 2.2354 \t Val Loss: 2.2663\n",
      "Training Loss: 2.2339 \t Val Loss: 2.2651\n",
      "Training Loss: 2.2325 \t Val Loss: 2.2639\n",
      "Training Loss: 2.2312 \t Val Loss: 2.2628\n",
      "Training Loss: 2.2299 \t Val Loss: 2.2618\n",
      "Training Loss: 2.2287 \t Val Loss: 2.2608\n",
      "Training Loss: 2.2276 \t Val Loss: 2.2599\n",
      "Training Loss: 2.2266 \t Val Loss: 2.2591\n",
      "Training Loss: 2.2256 \t Val Loss: 2.2583\n",
      "Training Loss: 2.2246 \t Val Loss: 2.2575\n",
      "Training Loss: 2.2237 \t Val Loss: 2.2568\n",
      "Training Loss: 2.2229 \t Val Loss: 2.2561\n",
      "Training Loss: 2.2220 \t Val Loss: 2.2554\n"
     ]
    }
   ],
   "source": [
    "for i in range(2000):\n",
    "    logits = W[xs_train]\n",
    "    loss = F.cross_entropy(logits, ys_train, label_smoothing=0.001)\n",
    "    if i % 50 == 0:\n",
    "        val_logits = W[xs_val]\n",
    "        val_loss = F.cross_entropy(val_logits, ys_val, label_smoothing=0.001)\n",
    "        print(f\"Training Loss: {loss.item():.4f} \\t Val Loss: {val_loss.item():.4f}\")\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "    W.data += -30 * W.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d25a4d4",
   "metadata": {},
   "source": [
    "### E06: meta-exercise! Think of a fun/interesting exercise and complete it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49632ef6",
   "metadata": {},
   "source": [
    "<p> Train the whole dataset using cross entropy loss and check the predictions. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "00618fcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([228146]), torch.Size([228146]))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs.shape, ys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "afd06eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.rand((703, 27), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "65cb155c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 3.352999448776245\n",
      "Loss: 2.521942615509033\n",
      "Loss: 2.4047069549560547\n",
      "Loss: 2.3517673015594482\n",
      "Loss: 2.320843458175659\n",
      "Loss: 2.3003499507904053\n",
      "Loss: 2.285701274871826\n",
      "Loss: 2.2746834754943848\n",
      "Loss: 2.2660791873931885\n",
      "Loss: 2.2591638565063477\n",
      "Loss: 2.2534759044647217\n",
      "Loss: 2.248708963394165\n",
      "Loss: 2.2446517944335938\n",
      "Loss: 2.2411534786224365\n",
      "Loss: 2.238102912902832\n",
      "Loss: 2.2354178428649902\n",
      "Loss: 2.233035087585449\n",
      "Loss: 2.2309043407440186\n",
      "Loss: 2.2289879322052\n",
      "Loss: 2.2272531986236572\n",
      "Loss: 2.225675582885742\n",
      "Loss: 2.224234104156494\n",
      "Loss: 2.2229108810424805\n",
      "Loss: 2.2216923236846924\n",
      "Loss: 2.2205650806427\n",
      "Loss: 2.219519853591919\n",
      "Loss: 2.2185471057891846\n",
      "Loss: 2.217639923095703\n",
      "Loss: 2.2167911529541016\n",
      "Loss: 2.2159953117370605\n",
      "Loss: 2.215247631072998\n"
     ]
    }
   ],
   "source": [
    "for i in range(3001):\n",
    "    logits = W[xs]\n",
    "    loss = F.cross_entropy(logits, ys, label_smoothing=0.001)\n",
    "    if i % 100 == 0:\n",
    "        print(f\"Loss: {loss}\")\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "    W.data += -30 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "839df9ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "junide\n",
      "jakasid\n",
      "prelay\n",
      "adin\n",
      "kairritoper\n",
      "sathen\n",
      "sameia\n",
      "yanileniassibiainewin\n",
      "lessiyanayla\n",
      "te\n"
     ]
    }
   ],
   "source": [
    "# The loss above is nearly identical to the loss from the counting method. So we expect the model to generate same names.\n",
    "gen = torch.Generator().manual_seed(2147483647)\n",
    "for _ in range(10):\n",
    "    out = []\n",
    "    char1, char2 = '.', '.'\n",
    "    \n",
    "    while True:\n",
    "        logits = W[ptoi[char1, char2]]\n",
    "        probs = F.softmax(logits, dim=0)\n",
    "        idx = torch.multinomial(probs, num_samples=1, replacement=True, generator=gen).item()\n",
    "        if idx == 0:\n",
    "            break\n",
    "        char1 = char2\n",
    "        char2 = itos[idx]\n",
    "        out.append(itos[idx])\n",
    "    \n",
    "    print(\"\".join(out))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

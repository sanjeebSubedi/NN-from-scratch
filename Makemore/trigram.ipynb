{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb9e5cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6bab00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read names from file\n",
    "with open(\"names.txt\", \"r\") as f:\n",
    "    names = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5085cfc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['emma', 'olivia', 'ava']\n",
      "32033\n"
     ]
    }
   ],
   "source": [
    "names = [name.strip() for name in names]\n",
    "print(names[:3])\n",
    "print(len(names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "625269b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "letter_set = sorted(list({l for name in names for l in name}))\n",
    "letter_set.insert(0, '.')\n",
    "len(letter_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32b7d3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = {letter: pos for pos, letter in enumerate(letter_set)}\n",
    "itos = {pos: letter for letter, pos in stoi.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b150d447",
   "metadata": {},
   "source": [
    "### E01: train a trigram language model, i.e. take two characters as an input to predict the 3rd one. Feel free to use either counting or a neural net. Evaluate the loss; Did it improve over a bigram model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13974487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 27 by 27 by 27 matrix to count all the number of occurences of trigrams\n",
    "lookup_table = torch.ones((27, 27, 27), dtype=torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fec3e08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the lookup table with the counts of each trigram\n",
    "for name in names:\n",
    "    name = ['.', '.'] + list(name) + ['.']\n",
    "    for char1, char2, char3 in zip(name, name[1:], name[2:]):\n",
    "        p1, p2, p3 = stoi[char1], stoi[char2], stoi[char3]\n",
    "        lookup_table[p1, p2, p3] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e937f829",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_occurences(char1, char2, char3):\n",
    "    return lookup_table[stoi[char1], stoi[char2], stoi[char3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bdbcbfc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4411, dtype=torch.int32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_occurences('.', '.', 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "691ca7be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(729.0001)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize the count table \n",
    "lookup_table = torch.div(lookup_table, torch.sum(lookup_table, dim=2, keepdims=True))\n",
    "torch.sum(lookup_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ad1007a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " junide\n",
      " jakasid\n",
      " prelay\n",
      " adin\n",
      " kairritoper\n",
      " sathen\n",
      " sameia\n",
      " yanileniassibduinrwin\n",
      " lessiyanayla\n",
      " te\n"
     ]
    }
   ],
   "source": [
    "# Get some new predictions using the counts table\n",
    "gen = torch.Generator().manual_seed(2147483647)\n",
    "output_count = []\n",
    "for _ in range(10):\n",
    "    idx1, idx2 = 0, 0\n",
    "    out = []\n",
    "    while True:\n",
    "        idx3 = torch.multinomial(lookup_table[idx1, idx2], num_samples=1, replacement=True, generator=gen).item()\n",
    "        if idx3==0:\n",
    "            break\n",
    "        out.append(itos[idx3])\n",
    "        idx1 = idx2\n",
    "        idx2 = idx3\n",
    "    output_count.append(\"\".join(out))\n",
    "    print(\"\", \"\".join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89ab5498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_likelihood=tensor(-504653.)\n",
      "nll_loss: 2.2120\n",
      "bigram_loss: 2.4544\n"
     ]
    }
   ],
   "source": [
    "# Calculate the loss (negative log likelihood loss) and compare it to the loss of the bigram model (previously done)\n",
    "log_likelihood = 0.0 \n",
    "num_samples = 0\n",
    "for name in names:\n",
    "    name = ['.', '.'] + list(name) + ['.']\n",
    "    for char1, char2, char3 in zip(name, name[1:], name[2:]):\n",
    "        prob = lookup_table[stoi[char1], stoi[char2], stoi[char3]]\n",
    "        log_likelihood += torch.log(prob)\n",
    "        num_samples += 1\n",
    "nll = -log_likelihood\n",
    "print(f\"{log_likelihood=}\")\n",
    "print(f\"nll_loss: {nll/num_samples:.4f}\")\n",
    "print(f\"bigram_loss: 2.4544\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c8477af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 456292\n"
     ]
    }
   ],
   "source": [
    "# Create trigram samples to train a gradient based model\n",
    "xs, ys = [], []\n",
    "for name in names:\n",
    "    name = ['.', '.'] + list(name) + ['.']\n",
    "    for char1, char2, char3 in zip(name, name[1:], name[2:]):\n",
    "        xs.append([stoi[char1], stoi[char2]])\n",
    "        ys.append(stoi[char3])\n",
    "        \n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "print(f\"Number of samples: {xs.nelement()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89e9b857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of encoded inputs: torch.Size([228146, 54])\n",
      "Shape of weights matrix: torch.Size([54, 27])\n",
      "Shape of labels vector: torch.Size([228146])\n"
     ]
    }
   ],
   "source": [
    "# Change the inputs into one hot vectors and initialize the weights\n",
    "x_oh = torch.nn.functional.one_hot(xs, num_classes=27).float()\n",
    "# Reshape the input matrix from [_, 2, 27] to [_, 54] to make the multiplication easier\n",
    "x_oh = x_oh.view(x_oh.shape[0], x_oh.shape[1] * x_oh.shape[2])\n",
    "W = torch.randn((54, 27), requires_grad=True)\n",
    "print(f\"Shape of encoded inputs: {x_oh.shape}\")\n",
    "print(f\"Shape of weights matrix: {W.shape}\")\n",
    "print(f\"Shape of labels vector: {ys.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9d41efcd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 4.331145763397217\n",
      "Loss: 3.732630729675293\n",
      "Loss: 3.456670045852661\n",
      "Loss: 3.2706995010375977\n",
      "Loss: 3.1345431804656982\n",
      "Loss: 3.0353121757507324\n",
      "Loss: 2.957371234893799\n",
      "Loss: 2.8958206176757812\n",
      "Loss: 2.8451552391052246\n",
      "Loss: 2.802884578704834\n",
      "Loss: 2.7669413089752197\n",
      "Loss: 2.7360732555389404\n",
      "Loss: 2.709300994873047\n",
      "Loss: 2.685901403427124\n",
      "Loss: 2.665297746658325\n",
      "Loss: 2.647034168243408\n",
      "Loss: 2.630741834640503\n",
      "Loss: 2.6161224842071533\n",
      "Loss: 2.6029324531555176\n",
      "Loss: 2.5909717082977295\n",
      "Loss: 2.5800743103027344\n",
      "Loss: 2.570103645324707\n",
      "Loss: 2.5609445571899414\n",
      "Loss: 2.5524988174438477\n",
      "Loss: 2.5446856021881104\n",
      "Loss: 2.5374348163604736\n",
      "Loss: 2.530686378479004\n",
      "Loss: 2.5243892669677734\n",
      "Loss: 2.5184993743896484\n",
      "Loss: 2.51297664642334\n",
      "Loss: 2.507789134979248\n",
      "Loss: 2.50290584564209\n",
      "Loss: 2.4983017444610596\n",
      "Loss: 2.4939522743225098\n",
      "Loss: 2.489838123321533\n",
      "Loss: 2.485940456390381\n",
      "Loss: 2.4822423458099365\n",
      "Loss: 2.478729248046875\n",
      "Loss: 2.4753878116607666\n",
      "Loss: 2.4722061157226562\n",
      "Loss: 2.469172954559326\n",
      "Loss: 2.466278314590454\n",
      "Loss: 2.4635136127471924\n",
      "Loss: 2.460869550704956\n",
      "Loss: 2.4583394527435303\n",
      "Loss: 2.4559154510498047\n",
      "Loss: 2.4535915851593018\n",
      "Loss: 2.451362133026123\n",
      "Loss: 2.44922137260437\n",
      "Loss: 2.4471640586853027\n",
      "Loss: 2.445185899734497\n",
      "Loss: 2.4432826042175293\n",
      "Loss: 2.4414496421813965\n",
      "Loss: 2.4396841526031494\n",
      "Loss: 2.4379820823669434\n",
      "Loss: 2.43634033203125\n",
      "Loss: 2.434755802154541\n",
      "Loss: 2.4332258701324463\n",
      "Loss: 2.4317479133605957\n",
      "Loss: 2.430319309234619\n",
      "Loss: 2.4289376735687256\n",
      "Loss: 2.4276010990142822\n",
      "Loss: 2.426307201385498\n",
      "Loss: 2.4250545501708984\n",
      "Loss: 2.4238414764404297\n",
      "Loss: 2.4226653575897217\n",
      "Loss: 2.421525478363037\n",
      "Loss: 2.420419931411743\n",
      "Loss: 2.4193472862243652\n",
      "Loss: 2.418306589126587\n",
      "Loss: 2.4172956943511963\n",
      "Loss: 2.416314125061035\n",
      "Loss: 2.415360689163208\n",
      "Loss: 2.4144339561462402\n",
      "Loss: 2.4135329723358154\n",
      "Loss: 2.4126570224761963\n",
      "Loss: 2.4118051528930664\n",
      "Loss: 2.410975933074951\n",
      "Loss: 2.4101688861846924\n",
      "Loss: 2.4093830585479736\n",
      "Loss: 2.408618450164795\n",
      "Loss: 2.4078731536865234\n",
      "Loss: 2.407147169113159\n",
      "Loss: 2.4064395427703857\n",
      "Loss: 2.405749559402466\n",
      "Loss: 2.4050769805908203\n",
      "Loss: 2.404421091079712\n",
      "Loss: 2.403780937194824\n",
      "Loss: 2.4031567573547363\n",
      "Loss: 2.4025471210479736\n",
      "Loss: 2.4019525051116943\n",
      "Loss: 2.401371479034424\n",
      "Loss: 2.400804281234741\n",
      "Loss: 2.40024995803833\n",
      "Loss: 2.3997085094451904\n",
      "Loss: 2.399178981781006\n",
      "Loss: 2.3986623287200928\n",
      "Loss: 2.3981564044952393\n",
      "Loss: 2.3976619243621826\n",
      "Loss: 2.3971781730651855\n",
      "Loss: 2.3967056274414062\n",
      "Loss: 2.396242380142212\n",
      "Loss: 2.395789623260498\n",
      "Loss: 2.395346164703369\n",
      "Loss: 2.394912004470825\n",
      "Loss: 2.3944873809814453\n",
      "Loss: 2.394071340560913\n",
      "Loss: 2.3936636447906494\n",
      "Loss: 2.3932645320892334\n",
      "Loss: 2.3928728103637695\n",
      "Loss: 2.392489433288574\n",
      "Loss: 2.3921141624450684\n",
      "Loss: 2.3917455673217773\n",
      "Loss: 2.3913843631744385\n",
      "Loss: 2.3910298347473145\n",
      "Loss: 2.3906829357147217\n",
      "Loss: 2.3903417587280273\n",
      "Loss: 2.390007734298706\n",
      "Loss: 2.389679431915283\n",
      "Loss: 2.389357805252075\n",
      "Loss: 2.3890416622161865\n",
      "Loss: 2.3887314796447754\n",
      "Loss: 2.3884267807006836\n",
      "Loss: 2.3881278038024902\n",
      "Loss: 2.387834072113037\n",
      "Loss: 2.387545585632324\n",
      "Loss: 2.3872623443603516\n",
      "Loss: 2.386983633041382\n",
      "Loss: 2.3867101669311523\n",
      "Loss: 2.386441230773926\n",
      "Loss: 2.3861770629882812\n",
      "Loss: 2.3859174251556396\n",
      "Loss: 2.385662078857422\n",
      "Loss: 2.385411262512207\n",
      "Loss: 2.385164260864258\n",
      "Loss: 2.3849215507507324\n",
      "Loss: 2.3846824169158936\n",
      "Loss: 2.3844475746154785\n",
      "Loss: 2.384216547012329\n",
      "Loss: 2.383989095687866\n",
      "Loss: 2.383765459060669\n",
      "Loss: 2.383544921875\n",
      "Loss: 2.3833281993865967\n",
      "Loss: 2.383114814758301\n",
      "Loss: 2.382904529571533\n",
      "Loss: 2.3826980590820312\n",
      "Loss: 2.3824942111968994\n",
      "Loss: 2.382293701171875\n",
      "Loss: 2.382096290588379\n",
      "Loss: 2.381901741027832\n",
      "Loss: 2.3817100524902344\n",
      "Loss: 2.381521463394165\n",
      "Loss: 2.3813352584838867\n",
      "Loss: 2.3811521530151367\n",
      "Loss: 2.3809711933135986\n",
      "Loss: 2.380793571472168\n",
      "Loss: 2.38061785697937\n",
      "Loss: 2.3804450035095215\n",
      "Loss: 2.380274772644043\n",
      "Loss: 2.3801066875457764\n",
      "Loss: 2.379940986633301\n",
      "Loss: 2.379777193069458\n",
      "Loss: 2.3796164989471436\n",
      "Loss: 2.379457712173462\n",
      "Loss: 2.379301071166992\n",
      "Loss: 2.3791463375091553\n",
      "Loss: 2.3789944648742676\n",
      "Loss: 2.3788435459136963\n",
      "Loss: 2.378695249557495\n",
      "Loss: 2.3785488605499268\n",
      "Loss: 2.378404378890991\n",
      "Loss: 2.3782620429992676\n",
      "Loss: 2.3781213760375977\n",
      "Loss: 2.3779826164245605\n",
      "Loss: 2.377845287322998\n",
      "Loss: 2.3777103424072266\n",
      "Loss: 2.377577066421509\n",
      "Loss: 2.3774449825286865\n",
      "Loss: 2.377314805984497\n",
      "Loss: 2.3771870136260986\n",
      "Loss: 2.3770596981048584\n",
      "Loss: 2.37693452835083\n",
      "Loss: 2.3768107891082764\n",
      "Loss: 2.3766887187957764\n",
      "Loss: 2.37656831741333\n",
      "Loss: 2.3764491081237793\n",
      "Loss: 2.376331090927124\n",
      "Loss: 2.3762152194976807\n",
      "Loss: 2.3761000633239746\n",
      "Loss: 2.3759868144989014\n",
      "Loss: 2.3758749961853027\n",
      "Loss: 2.3757641315460205\n",
      "Loss: 2.375654697418213\n",
      "Loss: 2.375546932220459\n",
      "Loss: 2.3754398822784424\n",
      "Loss: 2.3753340244293213\n",
      "Loss: 2.375230073928833\n",
      "Loss: 2.375126600265503\n",
      "Loss: 2.3750247955322266\n",
      "Loss: 2.3749239444732666\n",
      "Loss: 2.374824285507202\n",
      "Loss: 2.374725580215454\n",
      "Loss: 2.3746285438537598\n",
      "Loss: 2.374532461166382\n",
      "Loss: 2.374437093734741\n",
      "Loss: 2.374343156814575\n",
      "Loss: 2.3742501735687256\n",
      "Loss: 2.3741579055786133\n",
      "Loss: 2.3740670680999756\n",
      "Loss: 2.373976945877075\n",
      "Loss: 2.3738880157470703\n",
      "Loss: 2.3737998008728027\n",
      "Loss: 2.3737127780914307\n",
      "Loss: 2.373626708984375\n",
      "Loss: 2.3735415935516357\n",
      "Loss: 2.373457431793213\n",
      "Loss: 2.3733737468719482\n",
      "Loss: 2.3732917308807373\n",
      "Loss: 2.3732097148895264\n",
      "Loss: 2.37312912940979\n",
      "Loss: 2.373049020767212\n",
      "Loss: 2.3729701042175293\n",
      "Loss: 2.372891902923584\n",
      "Loss: 2.372814655303955\n",
      "Loss: 2.3727381229400635\n",
      "Loss: 2.37266206741333\n",
      "Loss: 2.372587203979492\n",
      "Loss: 2.3725132942199707\n",
      "Loss: 2.372439384460449\n",
      "Loss: 2.3723666667938232\n",
      "Loss: 2.3722946643829346\n",
      "Loss: 2.3722236156463623\n",
      "Loss: 2.3721530437469482\n",
      "Loss: 2.3720831871032715\n",
      "Loss: 2.372013807296753\n",
      "Loss: 2.37194561958313\n",
      "Loss: 2.371878147125244\n",
      "Loss: 2.3718106746673584\n",
      "Loss: 2.371744155883789\n",
      "Loss: 2.3716788291931152\n",
      "Loss: 2.3716135025024414\n",
      "Loss: 2.371548652648926\n",
      "Loss: 2.3714849948883057\n",
      "Loss: 2.3714218139648438\n",
      "Loss: 2.371358871459961\n",
      "Loss: 2.3712968826293945\n",
      "Loss: 2.3712356090545654\n",
      "Loss: 2.3711748123168945\n",
      "Loss: 2.3711142539978027\n",
      "Loss: 2.3710546493530273\n",
      "Loss: 2.37099552154541\n",
      "Loss: 2.370936870574951\n",
      "Loss: 2.3708786964416504\n",
      "Loss: 2.370821237564087\n",
      "Loss: 2.3707640171051025\n",
      "Loss: 2.3707077503204346\n",
      "Loss: 2.3706517219543457\n",
      "Loss: 2.370596408843994\n",
      "Loss: 2.370541572570801\n",
      "Loss: 2.3704872131347656\n",
      "Loss: 2.3704333305358887\n",
      "Loss: 2.37037992477417\n",
      "Loss: 2.3703274726867676\n",
      "Loss: 2.370274782180786\n",
      "Loss: 2.370222806930542\n",
      "Loss: 2.370170831680298\n",
      "Loss: 2.37011981010437\n",
      "Loss: 2.3700692653656006\n",
      "Loss: 2.3700191974639893\n",
      "Loss: 2.369969367980957\n",
      "Loss: 2.369920015335083\n",
      "Loss: 2.3698713779449463\n",
      "Loss: 2.3698229789733887\n",
      "Loss: 2.3697750568389893\n",
      "Loss: 2.36972713470459\n",
      "Loss: 2.369680404663086\n",
      "Loss: 2.369633197784424\n",
      "Loss: 2.3695871829986572\n",
      "Loss: 2.3695411682128906\n",
      "Loss: 2.3694956302642822\n",
      "Loss: 2.369450569152832\n",
      "Loss: 2.369405746459961\n",
      "Loss: 2.369361162185669\n",
      "Loss: 2.3693172931671143\n",
      "Loss: 2.3692736625671387\n",
      "Loss: 2.369230270385742\n",
      "Loss: 2.369187593460083\n",
      "Loss: 2.3691446781158447\n",
      "Loss: 2.369102716445923\n",
      "Loss: 2.369060754776001\n",
      "Loss: 2.3690195083618164\n",
      "Loss: 2.368978261947632\n",
      "Loss: 2.3689372539520264\n",
      "Loss: 2.368896722793579\n",
      "Loss: 2.368856430053711\n",
      "Loss: 2.36881685256958\n",
      "Loss: 2.3687775135040283\n",
      "Loss: 2.3687381744384766\n",
      "Loss: 2.368699312210083\n",
      "Loss: 2.3686609268188477\n",
      "Loss: 2.368622303009033\n",
      "Loss: 2.368584394454956\n",
      "Loss: 2.368547201156616\n",
      "Loss: 2.368509531021118\n",
      "Loss: 2.3684723377227783\n",
      "Loss: 2.368435859680176\n",
      "Loss: 2.368399143218994\n",
      "Loss: 2.36836314201355\n",
      "Loss: 2.3683273792266846\n",
      "Loss: 2.3682913780212402\n",
      "Loss: 2.3682563304901123\n",
      "Loss: 2.3682212829589844\n",
      "Loss: 2.3681862354278564\n",
      "Loss: 2.368152141571045\n",
      "Loss: 2.368117570877075\n",
      "Loss: 2.3680834770202637\n",
      "Loss: 2.3680498600006104\n",
      "Loss: 2.368016481399536\n",
      "Loss: 2.367983341217041\n",
      "Loss: 2.367950439453125\n",
      "Loss: 2.367917537689209\n",
      "Loss: 2.3678853511810303\n",
      "Loss: 2.3678526878356934\n",
      "Loss: 2.367820978164673\n",
      "Loss: 2.3677892684936523\n",
      "Loss: 2.367757558822632\n",
      "Loss: 2.3677263259887695\n",
      "Loss: 2.3676953315734863\n",
      "Loss: 2.3676648139953613\n",
      "Loss: 2.3676340579986572\n",
      "Loss: 2.3676035404205322\n",
      "Loss: 2.3675732612609863\n",
      "Loss: 2.3675436973571777\n",
      "Loss: 2.36751389503479\n",
      "Loss: 2.3674843311309814\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.367455005645752\n",
      "Loss: 2.3674259185791016\n",
      "Loss: 2.3673973083496094\n",
      "Loss: 2.367368698120117\n",
      "Loss: 2.367340087890625\n",
      "Loss: 2.36731219291687\n",
      "Loss: 2.367283821105957\n",
      "Loss: 2.3672561645507812\n",
      "Loss: 2.3672285079956055\n",
      "Loss: 2.367201328277588\n",
      "Loss: 2.3671741485595703\n",
      "Loss: 2.367147207260132\n",
      "Loss: 2.3671202659606934\n",
      "Loss: 2.367093563079834\n",
      "Loss: 2.3670668601989746\n",
      "Loss: 2.3670408725738525\n",
      "Loss: 2.3670146465301514\n",
      "Loss: 2.3669888973236084\n",
      "Loss: 2.3669629096984863\n",
      "Loss: 2.3669376373291016\n",
      "Loss: 2.3669121265411377\n",
      "Loss: 2.366886615753174\n",
      "Loss: 2.3668620586395264\n",
      "Loss: 2.3668367862701416\n",
      "Loss: 2.366812229156494\n",
      "Loss: 2.366787910461426\n",
      "Loss: 2.3667635917663574\n",
      "Loss: 2.366739511489868\n",
      "Loss: 2.366715431213379\n",
      "Loss: 2.3666913509368896\n",
      "Loss: 2.3666677474975586\n",
      "Loss: 2.3666443824768066\n",
      "Loss: 2.3666210174560547\n",
      "Loss: 2.3665976524353027\n",
      "Loss: 2.36657452583313\n",
      "Loss: 2.366551637649536\n",
      "Loss: 2.3665289878845215\n",
      "Loss: 2.366506576538086\n",
      "Loss: 2.3664839267730713\n",
      "Loss: 2.366461753845215\n",
      "Loss: 2.3664395809173584\n",
      "Loss: 2.36641788482666\n",
      "Loss: 2.3663957118988037\n",
      "Loss: 2.3663742542266846\n",
      "Loss: 2.3663523197174072\n",
      "Loss: 2.366331100463867\n",
      "Loss: 2.3663101196289062\n",
      "Loss: 2.366288423538208\n",
      "Loss: 2.366267442703247\n",
      "Loss: 2.3662469387054443\n",
      "Loss: 2.3662259578704834\n",
      "Loss: 2.3662052154541016\n",
      "Loss: 2.366184949874878\n",
      "Loss: 2.3661646842956543\n",
      "Loss: 2.3661446571350098\n",
      "Loss: 2.366124391555786\n",
      "Loss: 2.3661041259765625\n",
      "Loss: 2.366084575653076\n",
      "Loss: 2.36606502532959\n",
      "Loss: 2.3660452365875244\n",
      "Loss: 2.366025447845459\n",
      "Loss: 2.366006851196289\n",
      "Loss: 2.365987539291382\n",
      "Loss: 2.3659679889678955\n",
      "Loss: 2.3659491539001465\n",
      "Loss: 2.3659307956695557\n",
      "Loss: 2.3659117221832275\n",
      "Loss: 2.3658933639526367\n",
      "Loss: 2.3658745288848877\n",
      "Loss: 2.365856647491455\n",
      "Loss: 2.365838050842285\n",
      "Loss: 2.3658199310302734\n",
      "Loss: 2.365802049636841\n",
      "Loss: 2.365784168243408\n",
      "Loss: 2.3657660484313965\n",
      "Loss: 2.365748405456543\n",
      "Loss: 2.3657307624816895\n",
      "Loss: 2.365713357925415\n",
      "Loss: 2.365696430206299\n",
      "Loss: 2.3656787872314453\n",
      "Loss: 2.36566162109375\n",
      "Loss: 2.3656444549560547\n",
      "Loss: 2.3656277656555176\n",
      "Loss: 2.3656105995178223\n",
      "Loss: 2.3655943870544434\n",
      "Loss: 2.365577459335327\n",
      "Loss: 2.365561008453369\n",
      "Loss: 2.365544557571411\n",
      "Loss: 2.3655283451080322\n",
      "Loss: 2.365511894226074\n",
      "Loss: 2.3654961585998535\n",
      "Loss: 2.3654801845550537\n",
      "Loss: 2.3654637336730957\n",
      "Loss: 2.365447759628296\n",
      "Loss: 2.3654325008392334\n",
      "Loss: 2.3654167652130127\n",
      "Loss: 2.365401029586792\n",
      "Loss: 2.3653857707977295\n",
      "Loss: 2.365370273590088\n",
      "Loss: 2.3653547763824463\n",
      "Loss: 2.365339756011963\n",
      "Loss: 2.3653244972229004\n",
      "Loss: 2.365309715270996\n",
      "Loss: 2.365294933319092\n",
      "Loss: 2.3652796745300293\n",
      "Loss: 2.365265130996704\n",
      "Loss: 2.3652503490448\n",
      "Loss: 2.3652358055114746\n",
      "Loss: 2.3652212619781494\n",
      "Loss: 2.3652069568634033\n",
      "Loss: 2.365192413330078\n",
      "Loss: 2.365178346633911\n",
      "Loss: 2.365164279937744\n",
      "Loss: 2.365149974822998\n",
      "Loss: 2.36513614654541\n",
      "Loss: 2.3651223182678223\n",
      "Loss: 2.3651082515716553\n",
      "Loss: 2.3650949001312256\n",
      "Loss: 2.3650808334350586\n",
      "Loss: 2.365067481994629\n",
      "Loss: 2.365053653717041\n",
      "Loss: 2.3650403022766113\n",
      "Loss: 2.3650269508361816\n",
      "Loss: 2.365013837814331\n",
      "Loss: 2.3650004863739014\n",
      "Loss: 2.364987373352051\n",
      "Loss: 2.3649742603302\n",
      "Loss: 2.3649611473083496\n",
      "Loss: 2.364948272705078\n",
      "Loss: 2.3649353981018066\n",
      "Loss: 2.3649227619171143\n",
      "Loss: 2.3649098873138428\n",
      "Loss: 2.3648977279663086\n",
      "Loss: 2.364885091781616\n",
      "Loss: 2.364872455596924\n",
      "Loss: 2.3648600578308105\n",
      "Loss: 2.364847421646118\n",
      "Loss: 2.364835262298584\n",
      "Loss: 2.364823341369629\n",
      "Loss: 2.364811420440674\n",
      "Loss: 2.3647990226745605\n",
      "Loss: 2.3647871017456055\n",
      "Loss: 2.3647749423980713\n",
      "Loss: 2.3647632598876953\n",
      "Loss: 2.3647513389587402\n",
      "Loss: 2.3647396564483643\n",
      "Loss: 2.3647279739379883\n",
      "Loss: 2.364716053009033\n",
      "Loss: 2.3647048473358154\n",
      "Loss: 2.3646934032440186\n",
      "Loss: 2.3646817207336426\n",
      "Loss: 2.364670753479004\n",
      "Loss: 2.364659070968628\n",
      "Loss: 2.3646481037139893\n",
      "Loss: 2.3646366596221924\n",
      "Loss: 2.364625930786133\n",
      "Loss: 2.364614725112915\n",
      "Loss: 2.3646035194396973\n",
      "Loss: 2.3645927906036377\n",
      "Loss: 2.364581823348999\n",
      "Loss: 2.3645710945129395\n",
      "Loss: 2.364560604095459\n",
      "Loss: 2.3645496368408203\n",
      "Loss: 2.3645389080047607\n",
      "Loss: 2.3645286560058594\n",
      "Loss: 2.3645179271698\n",
      "Loss: 2.3645074367523193\n",
      "Loss: 2.364496946334839\n",
      "Loss: 2.3644869327545166\n",
      "Loss: 2.3644769191741943\n",
      "Loss: 2.3644661903381348\n",
      "Loss: 2.3644564151763916\n",
      "Loss: 2.3644461631774902\n",
      "Loss: 2.364435911178589\n",
      "Loss: 2.3644261360168457\n",
      "Loss: 2.3644161224365234\n",
      "Loss: 2.3644063472747803\n",
      "Loss: 2.364396095275879\n",
      "Loss: 2.364386558532715\n",
      "Loss: 2.364377021789551\n",
      "Loss: 2.3643670082092285\n",
      "Loss: 2.3643574714660645\n",
      "Loss: 2.3643479347229004\n",
      "Loss: 2.3643381595611572\n",
      "Loss: 2.364328622817993\n",
      "Loss: 2.364319086074829\n",
      "Loss: 2.364309787750244\n",
      "Loss: 2.3643007278442383\n",
      "Loss: 2.364291191101074\n",
      "Loss: 2.3642818927764893\n",
      "Loss: 2.3642728328704834\n",
      "Loss: 2.3642635345458984\n",
      "Loss: 2.3642547130584717\n",
      "Loss: 2.364245653152466\n",
      "Loss: 2.36423659324646\n",
      "Loss: 2.364227294921875\n",
      "Loss: 2.3642187118530273\n",
      "Loss: 2.3642094135284424\n",
      "Loss: 2.364201068878174\n",
      "Loss: 2.364192008972168\n",
      "Loss: 2.3641834259033203\n",
      "Loss: 2.3641746044158936\n",
      "Loss: 2.364166498184204\n",
      "Loss: 2.3641574382781982\n",
      "Loss: 2.3641490936279297\n",
      "Loss: 2.364140748977661\n",
      "Loss: 2.3641321659088135\n",
      "Loss: 2.364123582839966\n",
      "Loss: 2.3641152381896973\n",
      "Loss: 2.3641068935394287\n",
      "Loss: 2.36409854888916\n",
      "Loss: 2.3640902042388916\n",
      "Loss: 2.364082098007202\n",
      "Loss: 2.3640739917755127\n",
      "Loss: 2.3640658855438232\n",
      "Loss: 2.364057779312134\n",
      "Loss: 2.3640496730804443\n",
      "Loss: 2.364042043685913\n",
      "Loss: 2.3640336990356445\n",
      "Loss: 2.364025831222534\n",
      "Loss: 2.3640177249908447\n",
      "Loss: 2.3640103340148926\n",
      "Loss: 2.364002227783203\n",
      "Loss: 2.363994836807251\n",
      "Loss: 2.3639869689941406\n",
      "Loss: 2.3639791011810303\n",
      "Loss: 2.36397123336792\n",
      "Loss: 2.3639638423919678\n",
      "Loss: 2.3639564514160156\n",
      "Loss: 2.3639488220214844\n",
      "Loss: 2.363941192626953\n",
      "Loss: 2.36393404006958\n",
      "Loss: 2.363926410675049\n",
      "Loss: 2.3639190196990967\n",
      "Loss: 2.3639118671417236\n",
      "Loss: 2.3639044761657715\n",
      "Loss: 2.3638973236083984\n",
      "Loss: 2.3638901710510254\n",
      "Loss: 2.3638827800750732\n",
      "Loss: 2.3638758659362793\n",
      "Loss: 2.363868474960327\n",
      "Loss: 2.363861322402954\n",
      "Loss: 2.3638546466827393\n",
      "Loss: 2.363847255706787\n",
      "Loss: 2.363840341567993\n",
      "Loss: 2.363833427429199\n",
      "Loss: 2.3638267517089844\n",
      "Loss: 2.3638195991516113\n",
      "Loss: 2.3638126850128174\n",
      "Loss: 2.3638057708740234\n",
      "Loss: 2.3637990951538086\n",
      "Loss: 2.3637924194335938\n",
      "Loss: 2.363785743713379\n",
      "Loss: 2.363779067993164\n",
      "Loss: 2.3637726306915283\n",
      "Loss: 2.3637657165527344\n",
      "Loss: 2.3637592792510986\n",
      "Loss: 2.363752603530884\n",
      "Loss: 2.363746404647827\n",
      "Loss: 2.3637399673461914\n",
      "Loss: 2.3637332916259766\n",
      "Loss: 2.363726854324341\n",
      "Loss: 2.363720417022705\n",
      "Loss: 2.3637139797210693\n",
      "Loss: 2.363708019256592\n",
      "Loss: 2.363701581954956\n",
      "Loss: 2.3636951446533203\n",
      "Loss: 2.3636889457702637\n",
      "Loss: 2.363682746887207\n",
      "Loss: 2.3636765480041504\n",
      "Loss: 2.3636703491210938\n",
      "Loss: 2.3636646270751953\n",
      "Loss: 2.3636581897735596\n",
      "Loss: 2.363651990890503\n",
      "Loss: 2.3636460304260254\n",
      "Loss: 2.363640308380127\n",
      "Loss: 2.3636341094970703\n",
      "Loss: 2.3636281490325928\n",
      "Loss: 2.3636221885681152\n",
      "Loss: 2.3636162281036377\n",
      "Loss: 2.3636107444763184\n",
      "Loss: 2.363604784011841\n",
      "Loss: 2.3635990619659424\n",
      "Loss: 2.363593339920044\n",
      "Loss: 2.3635871410369873\n",
      "Loss: 2.363581657409668\n",
      "Loss: 2.3635761737823486\n",
      "Loss: 2.363570213317871\n",
      "Loss: 2.3635647296905518\n",
      "Loss: 2.3635590076446533\n",
      "Loss: 2.363553762435913\n",
      "Loss: 2.3635478019714355\n",
      "Loss: 2.3635425567626953\n",
      "Loss: 2.363537073135376\n",
      "Loss: 2.3635313510894775\n",
      "Loss: 2.363525867462158\n",
      "Loss: 2.363520622253418\n",
      "Loss: 2.3635149002075195\n",
      "Loss: 2.3635096549987793\n",
      "Loss: 2.363504409790039\n",
      "Loss: 2.3634989261627197\n",
      "Loss: 2.3634936809539795\n",
      "Loss: 2.3634884357452393\n",
      "Loss: 2.363483190536499\n",
      "Loss: 2.363477945327759\n",
      "Loss: 2.3634729385375977\n",
      "Loss: 2.3634674549102783\n",
      "Loss: 2.3634626865386963\n",
      "Loss: 2.363457441329956\n",
      "Loss: 2.3634519577026367\n",
      "Loss: 2.3634469509124756\n",
      "Loss: 2.3634419441223145\n",
      "Loss: 2.3634369373321533\n",
      "Loss: 2.363431692123413\n",
      "Loss: 2.363426685333252\n",
      "Loss: 2.36342191696167\n",
      "Loss: 2.3634166717529297\n",
      "Loss: 2.3634119033813477\n",
      "Loss: 2.3634071350097656\n",
      "Loss: 2.3634023666381836\n",
      "Loss: 2.3633971214294434\n",
      "Loss: 2.3633925914764404\n",
      "Loss: 2.3633875846862793\n",
      "Loss: 2.3633828163146973\n",
      "Loss: 2.3633780479431152\n",
      "Loss: 2.363373279571533\n",
      "Loss: 2.3633687496185303\n",
      "Loss: 2.3633639812469482\n",
      "Loss: 2.3633594512939453\n",
      "Loss: 2.3633546829223633\n",
      "Loss: 2.363349676132202\n",
      "Loss: 2.3633453845977783\n",
      "Loss: 2.3633406162261963\n",
      "Loss: 2.3633360862731934\n",
      "Loss: 2.3633315563201904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.3633270263671875\n",
      "Loss: 2.3633222579956055\n",
      "Loss: 2.3633182048797607\n",
      "Loss: 2.3633134365081787\n",
      "Loss: 2.363309144973755\n",
      "Loss: 2.363304615020752\n",
      "Loss: 2.363300085067749\n",
      "Loss: 2.363295793533325\n",
      "Loss: 2.3632912635803223\n",
      "Loss: 2.3632869720458984\n",
      "Loss: 2.3632824420928955\n",
      "Loss: 2.363278388977051\n",
      "Loss: 2.363274097442627\n",
      "Loss: 2.363269567489624\n",
      "Loss: 2.3632652759552\n",
      "Loss: 2.3632612228393555\n",
      "Loss: 2.3632571697235107\n",
      "Loss: 2.363253116607666\n",
      "Loss: 2.363248586654663\n",
      "Loss: 2.3632445335388184\n",
      "Loss: 2.3632402420043945\n",
      "Loss: 2.3632359504699707\n",
      "Loss: 2.363231897354126\n",
      "Loss: 2.3632280826568604\n",
      "Loss: 2.3632237911224365\n",
      "Loss: 2.363219976425171\n",
      "Loss: 2.363215684890747\n",
      "Loss: 2.3632113933563232\n",
      "Loss: 2.3632078170776367\n",
      "Loss: 2.363203525543213\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "for _ in range(700):\n",
    "    logits = torch.matmul(x_oh, W)\n",
    "    counts = logits.exp() # Exponentiate to get the counts \n",
    "    probs = torch.div(counts, torch.sum(counts, dim=1, keepdims=True)) # Normalize the counts\n",
    "    loss = -probs[torch.arange(xs.shape[0]), ys].log().mean() + 0.02 * (W ** 2).mean() # Calculate the nll loss\n",
    "    print(f\"Loss: {loss.item()}\")\n",
    "    W.grad = None # clear the gradients\n",
    "    loss.backward() \n",
    "    W.data += -30 * W.grad # update the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "adced769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on bigram (gradient based): 2.4804\n",
      "Loss on trigram: 2.3632\n"
     ]
    }
   ],
   "source": [
    "# Comparing the loss of bigram model and trigram model\n",
    "print(\"Loss on bigram (gradient based): 2.4804\")\n",
    "print(f\"Loss on trigram: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6805dbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some names and compare with the names from counting method\n",
    "gen = torch.Generator().manual_seed(2147483647)\n",
    "output_gradient = []\n",
    "for _ in range(10):\n",
    "    out = []\n",
    "    idx1 = 0\n",
    "    idx2 = 0\n",
    "    \n",
    "    while True:\n",
    "        x_enc_1 = torch.nn.functional.one_hot(torch.tensor([idx1]), num_classes=27).float()\n",
    "        x_enc_2 = torch.nn.functional.one_hot(torch.tensor([idx2]), num_classes=27).float()\n",
    "        \n",
    "        logits = torch.matmul(torch.hstack((x_enc_1, x_enc_2)), W)\n",
    "        counts = logits.exp()\n",
    "        probs = torch.div(counts, torch.sum(counts, dim=1, keepdims=True))\n",
    "        \n",
    "        idx3 = torch.multinomial(probs, num_samples=1, replacement=True, generator=gen).item()\n",
    "        if idx3 == 0:\n",
    "            break\n",
    "        idx1 = idx2\n",
    "        idx2 = idx3\n",
    "        out.append(itos[idx3])\n",
    "    output_gradient.append(\"\".join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6646db73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting based:          \tGradient based: \n",
      "----------------------------------------------------\n",
      "junide                   \tjuwide              \n",
      "jakasid                  \tjanasad             \n",
      "prelay                   \tpariay              \n",
      "adin                     \tainn                \n",
      "kairritoper              \tkoi                 \n",
      "sathen                   \tritoleras           \n",
      "sameia                   \ttee                 \n",
      "yanileniassibduinrwin    \tkalania             \n",
      "lessiyanayla             \tyanileniassibdainrwi\n",
      "te                       \tta                  \n"
     ]
    }
   ],
   "source": [
    "print(\"{:<25}\\t{}\".format(\"Counting based: \", \"Gradient based: \"))\n",
    "print(\"----------------------------------------------------\")\n",
    "for out1, out2 in zip(output_count, output_gradient):\n",
    "    print(\"{:<25}\\t{:<20}\".format(out1, out2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a307d4",
   "metadata": {},
   "source": [
    "### E02: split up the dataset randomly into 80% train set, 10% dev set, 10% test set. Train the bigram and trigram models only on the training set. Evaluate them on dev and test splits. What can you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aaa81b80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([228146, 54]), torch.Size([228146]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_oh.shape, ys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "95865da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182517, 54]) torch.Size([182517])\n",
      "torch.Size([22815, 54]) torch.Size([22815])\n",
      "torch.Size([22814, 54]) torch.Size([22814])\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into train, validation and test sets\n",
    "train_idx, val_idx, test_idx = torch.utils.data.random_split(range(x_oh.shape[0]), [0.8, 0.1, 0.1])\n",
    "\n",
    "xs_train, ys_train = x_oh[train_idx], ys[train_idx]\n",
    "xs_val, ys_val = x_oh[val_idx], ys[val_idx]\n",
    "xs_test, ys_test = x_oh[test_idx], ys[test_idx]\n",
    "\n",
    "print(xs_train.shape, ys_train.shape)\n",
    "print(xs_val.shape, ys_val.shape)\n",
    "print(xs_test.shape, ys_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "83cac8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize new weights\n",
    "W = torch.randn((54, 27), requires_grad=True) # weights matrix\n",
    "R = 0.01 # regulatization loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "85d20070",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(inputs, weights):\n",
    "    \"\"\"Perform a matrix multiplication between the input and weights tensor and return probabilities after exponentiating and normalizing\"\"\"\n",
    "    logits = torch.matmul(inputs, weights)\n",
    "    counts = logits.exp()\n",
    "    probs = torch.div(counts, torch.sum(counts, dim=1, keepdims=True))\n",
    "    return probs\n",
    "\n",
    "def nll_loss(probs, n_inputs, labels):\n",
    "    \"\"\"Calculate the negative log likelihood loss and apply model smoothing with regularization\"\"\"\n",
    "    return -probs[torch.arange(n_inputs), labels].log().mean() + R * (W**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4e0b6754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 4.3237 \t Val Loss: 4.3352\n",
      "Training Loss: 2.4400 \t Val Loss: 2.4290\n",
      "Training Loss: 2.3918 \t Val Loss: 2.3834\n",
      "Training Loss: 2.3763 \t Val Loss: 2.3689\n",
      "Training Loss: 2.3687 \t Val Loss: 2.3619\n",
      "Training Loss: 2.3643 \t Val Loss: 2.3578\n",
      "Training Loss: 2.3614 \t Val Loss: 2.3551\n",
      "Training Loss: 2.3594 \t Val Loss: 2.3533\n",
      "Training Loss: 2.3580 \t Val Loss: 2.3520\n",
      "Training Loss: 2.3569 \t Val Loss: 2.3510\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "for i in range(500):\n",
    "    probs = forward_pass(xs_train, W)\n",
    "    loss = nll_loss(probs, xs_train.shape[0], ys_train)\n",
    "    if i % 50 == 0 or i == 999:\n",
    "        val_probs = forward_pass(xs_val, W)\n",
    "        val_loss = nll_loss(val_probs, xs_val.shape[0], ys_val)\n",
    "        print(f\"Training Loss: {loss.item():.4f} \\t Val Loss: {val_loss.item():.4f}\")\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "    W.data += -30 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "63af3055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 2.3576\n"
     ]
    }
   ],
   "source": [
    "# Calculate the loss on the test set\n",
    "test_loss = nll_loss(forward_pass(xs_test, W), len(xs_test), ys_test)\n",
    "print(f\"Test loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869a69a3",
   "metadata": {},
   "source": [
    "### E03: use the dev set to tune the strength of smoothing (or regularization) for the trigram model - i.e. try many possibilities and see which one works best based on the dev set loss. What patterns can you see in the train and dev set loss as you tune this strength? Take the best setting of the smoothing and evaluate on the test set once and at the end. How good of a loss do you achieve?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb9e5cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6bab00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read names from file\n",
    "with open(\"names.txt\", \"r\") as f:\n",
    "    names = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5085cfc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = [name.strip() for name in names]\n",
    "names[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0dd46142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the names\n",
    "import re\n",
    "names = [re.sub('[-,.]', '', name) for name in names]\n",
    "names = [re.sub(r'\\(.*\\)', '', name) for name in names]\n",
    "names = [name.lower() for name in names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1ef26cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bacc06cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "letter_set = {'.'}\n",
    "\n",
    "for name in names:\n",
    "    for l in list(name):\n",
    "        letter_set.add(l)\n",
    "        \n",
    "letter_set = sorted(list(letter_set))\n",
    "len(letter_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32b7d3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = {letter: pos for pos, letter in enumerate(letter_set)}\n",
    "itos = {pos: letter for letter, pos in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13974487",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_table = torch.ones((27, 27, 27), dtype=torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fec3e08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in names:\n",
    "    name = ['.', '.'] + list(name) + ['.']\n",
    "    for char1, char2, char3 in zip(name, name[1:], name[2:]):\n",
    "        p1, p2, p3 = stoi[char1], stoi[char2], stoi[char3]\n",
    "        lookup_table[p1, p2, p3] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e937f829",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_occurences(char1, char2, char3):\n",
    "    return lookup_table[stoi[char1], stoi[char2], stoi[char3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bdbcbfc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4411, dtype=torch.int32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_occurences('.', '.', 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "691ca7be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(729.0001)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lookup_table = torch.div(lookup_table, torch.sum(lookup_table, dim=2, keepdims=True))\n",
    "torch.sum(lookup_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ad1007a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:  cam\n",
      "Name:  ainor\n",
      "Name:  slea\n",
      "Name:  em\n",
      "Name:  mon\n",
      "Name:  eiagianaven\n",
      "Name:  kair\n",
      "Name:  uzana\n",
      "Name:  kentham\n",
      "Name:  jara\n"
     ]
    }
   ],
   "source": [
    "gen = torch.Generator().manual_seed(2147483648)\n",
    "for _ in range(10):\n",
    "    idx1, idx2 = 0, 0\n",
    "    out = []\n",
    "    while True:\n",
    "        idx3 = torch.multinomial(lookup_table[idx1, idx2], num_samples=1, replacement=True, generator=gen).item()\n",
    "        if idx3==0:\n",
    "            break\n",
    "        out.append(itos[idx3])\n",
    "        idx1 = idx2\n",
    "        idx2 = idx3\n",
    "    print(\"Name: \", \"\".join(out))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89ab5498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nll_loss: 2.2119739055633545\n"
     ]
    }
   ],
   "source": [
    "log_likelihood = 0.0 \n",
    "num_samples = 0\n",
    "for name in names:\n",
    "    name = ['.', '.'] + list(name) + ['.']\n",
    "    for char1, char2, char3 in zip(name, name[1:], name[2:]):\n",
    "        prob = lookup_table[stoi[char1], stoi[char2], stoi[char3]]\n",
    "        log_likelihood += torch.log(prob)\n",
    "        num_samples += 1\n",
    "nll = -log_likelihood\n",
    "print(f\"nll_loss: {nll/num_samples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c8477af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 456292\n"
     ]
    }
   ],
   "source": [
    "xs, ys = [], []\n",
    "for name in names:\n",
    "    name = ['.', '.'] + list(name) + ['.']\n",
    "    for char1, char2, char3 in zip(name, name[1:], name[2:]):\n",
    "        xs.append([stoi[char1], stoi[char2]])\n",
    "        ys.append(stoi[char3])\n",
    "        \n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "print(f\"Number of samples: {xs.nelement()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a8aacd62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0],\n",
       "        [ 0,  5],\n",
       "        [ 5, 13],\n",
       "        ...,\n",
       "        [26, 25],\n",
       "        [25, 26],\n",
       "        [26, 24]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89e9b857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of encoded inputs: torch.Size([228146, 2, 27])\n",
      "Shape of weights matrix: torch.Size([54, 27])\n"
     ]
    }
   ],
   "source": [
    "gen = torch.Generator().manual_seed(214748364)\n",
    "x_oh = torch.nn.functional.one_hot(xs, num_classes=27).float()\n",
    "weights = torch.randn((54, 27), requires_grad=True)\n",
    "print(f\"Shape of encoded inputs: {x_oh.shape}\")\n",
    "print(f\"Shape of weights matrix: {weights.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ddd0c84b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([228146])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bbcc1ad3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([228146, 54])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_oh_reshaped = x_oh.view(x_oh.shape[0], x_oh.shape[1] * x_oh.shape[2])\n",
    "x_oh_reshaped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9d41efcd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 4.330939292907715\n",
      "Loss: 3.7552990913391113\n",
      "Loss: 3.449273109436035\n",
      "Loss: 3.249814987182617\n",
      "Loss: 3.1080565452575684\n",
      "Loss: 3.0071299076080322\n",
      "Loss: 2.9291675090789795\n",
      "Loss: 2.867516040802002\n",
      "Loss: 2.817561388015747\n",
      "Loss: 2.7765862941741943\n",
      "Loss: 2.7424776554107666\n",
      "Loss: 2.713681936264038\n",
      "Loss: 2.689020872116089\n",
      "Loss: 2.66762113571167\n",
      "Loss: 2.648831367492676\n",
      "Loss: 2.6321637630462646\n",
      "Loss: 2.6172494888305664\n",
      "Loss: 2.6038060188293457\n",
      "Loss: 2.5916123390197754\n",
      "Loss: 2.580493211746216\n",
      "Loss: 2.5703063011169434\n",
      "Loss: 2.560936212539673\n",
      "Loss: 2.552286386489868\n",
      "Loss: 2.544275999069214\n",
      "Loss: 2.5368359088897705\n",
      "Loss: 2.5299081802368164\n",
      "Loss: 2.5234410762786865\n",
      "Loss: 2.5173912048339844\n",
      "Loss: 2.5117194652557373\n",
      "Loss: 2.50639271736145\n",
      "Loss: 2.501380443572998\n",
      "Loss: 2.496656656265259\n",
      "Loss: 2.4921979904174805\n",
      "Loss: 2.4879822731018066\n",
      "Loss: 2.4839913845062256\n",
      "Loss: 2.480208396911621\n",
      "Loss: 2.4766173362731934\n",
      "Loss: 2.4732048511505127\n",
      "Loss: 2.4699580669403076\n",
      "Loss: 2.4668657779693604\n",
      "Loss: 2.4639174938201904\n",
      "Loss: 2.461103677749634\n",
      "Loss: 2.4584157466888428\n",
      "Loss: 2.455845594406128\n",
      "Loss: 2.4533863067626953\n",
      "Loss: 2.451030969619751\n",
      "Loss: 2.448773145675659\n",
      "Loss: 2.4466073513031006\n",
      "Loss: 2.444528579711914\n",
      "Loss: 2.4425315856933594\n",
      "Loss: 2.4406120777130127\n",
      "Loss: 2.4387645721435547\n",
      "Loss: 2.4369871616363525\n",
      "Loss: 2.435274600982666\n",
      "Loss: 2.433624029159546\n",
      "Loss: 2.432032585144043\n",
      "Loss: 2.4304962158203125\n",
      "Loss: 2.4290127754211426\n",
      "Loss: 2.427579402923584\n",
      "Loss: 2.426193952560425\n",
      "Loss: 2.424853801727295\n",
      "Loss: 2.4235568046569824\n",
      "Loss: 2.4223008155822754\n",
      "Loss: 2.42108416557312\n",
      "Loss: 2.4199047088623047\n",
      "Loss: 2.4187607765197754\n",
      "Loss: 2.4176509380340576\n",
      "Loss: 2.4165735244750977\n",
      "Loss: 2.41552734375\n",
      "Loss: 2.41451096534729\n",
      "Loss: 2.413522481918335\n",
      "Loss: 2.4125618934631348\n",
      "Loss: 2.4116270542144775\n",
      "Loss: 2.410717487335205\n",
      "Loss: 2.4098317623138428\n",
      "Loss: 2.408968925476074\n",
      "Loss: 2.4081287384033203\n",
      "Loss: 2.4073097705841064\n",
      "Loss: 2.4065113067626953\n",
      "Loss: 2.4057321548461914\n",
      "Loss: 2.4049723148345947\n",
      "Loss: 2.404230833053589\n",
      "Loss: 2.4035069942474365\n",
      "Loss: 2.4027998447418213\n",
      "Loss: 2.402109384536743\n",
      "Loss: 2.4014344215393066\n",
      "Loss: 2.4007749557495117\n",
      "Loss: 2.400130271911621\n",
      "Loss: 2.3994994163513184\n",
      "Loss: 2.398883104324341\n",
      "Loss: 2.3982791900634766\n",
      "Loss: 2.3976891040802\n",
      "Loss: 2.397110939025879\n",
      "Loss: 2.396544933319092\n",
      "Loss: 2.3959906101226807\n",
      "Loss: 2.3954477310180664\n",
      "Loss: 2.39491605758667\n",
      "Loss: 2.3943943977355957\n",
      "Loss: 2.393883466720581\n",
      "Loss: 2.3933825492858887\n",
      "Loss: 2.3928911685943604\n",
      "Loss: 2.3924098014831543\n",
      "Loss: 2.391937017440796\n",
      "Loss: 2.3914735317230225\n",
      "Loss: 2.3910186290740967\n",
      "Loss: 2.3905715942382812\n",
      "Loss: 2.3901333808898926\n",
      "Loss: 2.3897032737731934\n",
      "Loss: 2.389280319213867\n",
      "Loss: 2.3888654708862305\n",
      "Loss: 2.3884575366973877\n",
      "Loss: 2.388056993484497\n",
      "Loss: 2.3876631259918213\n",
      "Loss: 2.3872764110565186\n",
      "Loss: 2.3868963718414307\n",
      "Loss: 2.3865222930908203\n",
      "Loss: 2.386154890060425\n",
      "Loss: 2.385793685913086\n",
      "Loss: 2.3854379653930664\n",
      "Loss: 2.3850889205932617\n",
      "Loss: 2.3847451210021973\n",
      "Loss: 2.384406566619873\n",
      "Loss: 2.3840739727020264\n",
      "Loss: 2.383746385574341\n",
      "Loss: 2.3834238052368164\n",
      "Loss: 2.3831069469451904\n",
      "Loss: 2.3827948570251465\n",
      "Loss: 2.3824872970581055\n",
      "Loss: 2.3821842670440674\n",
      "Loss: 2.3818864822387695\n",
      "Loss: 2.3815929889678955\n",
      "Loss: 2.3813037872314453\n",
      "Loss: 2.381019115447998\n",
      "Loss: 2.3807387351989746\n",
      "Loss: 2.380462408065796\n",
      "Loss: 2.380190134048462\n",
      "Loss: 2.3799221515655518\n",
      "Loss: 2.379657745361328\n",
      "Loss: 2.379397392272949\n",
      "Loss: 2.379140615463257\n",
      "Loss: 2.37888765335083\n",
      "Loss: 2.37863826751709\n",
      "Loss: 2.378391981124878\n",
      "Loss: 2.3781495094299316\n",
      "Loss: 2.377910614013672\n",
      "Loss: 2.3776748180389404\n",
      "Loss: 2.3774421215057373\n",
      "Loss: 2.3772127628326416\n",
      "Loss: 2.3769867420196533\n",
      "Loss: 2.376763343811035\n",
      "Loss: 2.3765435218811035\n",
      "Loss: 2.376326084136963\n",
      "Loss: 2.3761117458343506\n",
      "Loss: 2.3759005069732666\n",
      "Loss: 2.3756918907165527\n",
      "Loss: 2.375486135482788\n",
      "Loss: 2.3752827644348145\n",
      "Loss: 2.375082492828369\n",
      "Loss: 2.3748843669891357\n",
      "Loss: 2.3746888637542725\n",
      "Loss: 2.3744962215423584\n",
      "Loss: 2.3743059635162354\n",
      "Loss: 2.374117851257324\n",
      "Loss: 2.373932361602783\n",
      "Loss: 2.373748540878296\n",
      "Loss: 2.373567819595337\n",
      "Loss: 2.3733890056610107\n",
      "Loss: 2.3732125759124756\n",
      "Loss: 2.3730382919311523\n",
      "Loss: 2.372865915298462\n",
      "Loss: 2.3726954460144043\n",
      "Loss: 2.372527599334717\n",
      "Loss: 2.372361660003662\n",
      "Loss: 2.3721978664398193\n",
      "Loss: 2.3720357418060303\n",
      "Loss: 2.371875762939453\n",
      "Loss: 2.3717169761657715\n",
      "Loss: 2.37156081199646\n",
      "Loss: 2.3714065551757812\n",
      "Loss: 2.371253490447998\n",
      "Loss: 2.371102809906006\n",
      "Loss: 2.37095308303833\n",
      "Loss: 2.3708062171936035\n",
      "Loss: 2.3706600666046143\n",
      "Loss: 2.370515823364258\n",
      "Loss: 2.370373487472534\n",
      "Loss: 2.370232582092285\n",
      "Loss: 2.3700931072235107\n",
      "Loss: 2.3699557781219482\n",
      "Loss: 2.369819164276123\n",
      "Loss: 2.3696846961975098\n",
      "Loss: 2.369551420211792\n",
      "Loss: 2.369419813156128\n",
      "Loss: 2.3692893981933594\n",
      "Loss: 2.3691608905792236\n",
      "Loss: 2.3690333366394043\n",
      "Loss: 2.3689072132110596\n",
      "Loss: 2.3687822818756104\n",
      "Loss: 2.3686587810516357\n",
      "Loss: 2.368536949157715\n",
      "Loss: 2.3684160709381104\n",
      "Loss: 2.3682966232299805\n",
      "Loss: 2.368178367614746\n",
      "Loss: 2.3680615425109863\n",
      "Loss: 2.367945671081543\n",
      "Loss: 2.367831230163574\n",
      "Loss: 2.3677175045013428\n",
      "Loss: 2.367605447769165\n",
      "Loss: 2.3674943447113037\n",
      "Loss: 2.367384195327759\n",
      "Loss: 2.3672757148742676\n",
      "Loss: 2.3671679496765137\n",
      "Loss: 2.367061138153076\n",
      "Loss: 2.3669557571411133\n",
      "Loss: 2.3668510913848877\n",
      "Loss: 2.3667478561401367\n",
      "Loss: 2.366645574569702\n",
      "Loss: 2.366544246673584\n",
      "Loss: 2.366443634033203\n",
      "Loss: 2.366344451904297\n",
      "Loss: 2.366245746612549\n",
      "Loss: 2.3661482334136963\n",
      "Loss: 2.36605167388916\n",
      "Loss: 2.3659565448760986\n",
      "Loss: 2.3658618927001953\n",
      "Loss: 2.36576771736145\n",
      "Loss: 2.365675210952759\n",
      "Loss: 2.3655829429626465\n",
      "Loss: 2.365492105484009\n",
      "Loss: 2.3654017448425293\n",
      "Loss: 2.365312337875366\n",
      "Loss: 2.3652236461639404\n",
      "Loss: 2.365135908126831\n",
      "Loss: 2.365048885345459\n",
      "Loss: 2.3649630546569824\n",
      "Loss: 2.364877700805664\n",
      "Loss: 2.364793300628662\n",
      "Loss: 2.3647093772888184\n",
      "Loss: 2.364626407623291\n",
      "Loss: 2.364543914794922\n",
      "Loss: 2.3644626140594482\n",
      "Loss: 2.364382028579712\n",
      "Loss: 2.364301919937134\n",
      "Loss: 2.364222526550293\n",
      "Loss: 2.3641438484191895\n",
      "Loss: 2.3640661239624023\n",
      "Loss: 2.3639891147613525\n",
      "Loss: 2.363912343978882\n",
      "Loss: 2.3638365268707275\n",
      "Loss: 2.3637614250183105\n",
      "Loss: 2.3636868000030518\n",
      "Loss: 2.3636131286621094\n",
      "Loss: 2.363539695739746\n",
      "Loss: 2.36346697807312\n",
      "Loss: 2.3633952140808105\n",
      "Loss: 2.363323926925659\n",
      "Loss: 2.363253116607666\n",
      "Loss: 2.36318302154541\n",
      "Loss: 2.3631131649017334\n",
      "Loss: 2.363044500350952\n",
      "Loss: 2.362976551055908\n",
      "Loss: 2.3629086017608643\n",
      "Loss: 2.3628413677215576\n",
      "Loss: 2.3627748489379883\n",
      "Loss: 2.362708568572998\n",
      "Loss: 2.362643003463745\n",
      "Loss: 2.3625779151916504\n",
      "Loss: 2.362513303756714\n",
      "Loss: 2.362449884414673\n",
      "Loss: 2.3623862266540527\n",
      "Loss: 2.362323522567749\n",
      "Loss: 2.3622610569000244\n",
      "Loss: 2.362199306488037\n",
      "Loss: 2.362137794494629\n",
      "Loss: 2.362076997756958\n",
      "Loss: 2.362016439437866\n",
      "Loss: 2.361956834793091\n",
      "Loss: 2.3618974685668945\n",
      "Loss: 2.3618385791778564\n",
      "Loss: 2.3617799282073975\n",
      "Loss: 2.361721992492676\n",
      "Loss: 2.361664295196533\n",
      "Loss: 2.361607313156128\n",
      "Loss: 2.361550807952881\n",
      "Loss: 2.361494302749634\n",
      "Loss: 2.361438751220703\n",
      "Loss: 2.3613836765289307\n",
      "Loss: 2.3613288402557373\n",
      "Loss: 2.361274242401123\n",
      "Loss: 2.361220598220825\n",
      "Loss: 2.361166477203369\n",
      "Loss: 2.3611135482788086\n",
      "Loss: 2.361060619354248\n",
      "Loss: 2.361008644104004\n",
      "Loss: 2.3609561920166016\n",
      "Loss: 2.3609046936035156\n",
      "Loss: 2.360853672027588\n",
      "Loss: 2.36080265045166\n",
      "Loss: 2.360752582550049\n",
      "Loss: 2.3607022762298584\n",
      "Loss: 2.3606526851654053\n",
      "Loss: 2.3606035709381104\n",
      "Loss: 2.3605546951293945\n",
      "Loss: 2.3605058193206787\n",
      "Loss: 2.3604578971862793\n",
      "Loss: 2.360409736633301\n",
      "Loss: 2.3603625297546387\n",
      "Loss: 2.3603150844573975\n",
      "Loss: 2.3602685928344727\n",
      "Loss: 2.360222101211548\n",
      "Loss: 2.360175848007202\n",
      "Loss: 2.360130548477173\n",
      "Loss: 2.3600847721099854\n",
      "Loss: 2.3600399494171143\n",
      "Loss: 2.359995126724243\n",
      "Loss: 2.359950304031372\n",
      "Loss: 2.3599061965942383\n",
      "Loss: 2.3598625659942627\n",
      "Loss: 2.3598194122314453\n",
      "Loss: 2.3597757816314697\n",
      "Loss: 2.3597331047058105\n",
      "Loss: 2.3596904277801514\n",
      "Loss: 2.3596479892730713\n",
      "Loss: 2.3596062660217285\n",
      "Loss: 2.359564781188965\n",
      "Loss: 2.359523057937622\n",
      "Loss: 2.3594822883605957\n",
      "Loss: 2.3594412803649902\n",
      "Loss: 2.359400987625122\n",
      "Loss: 2.359360456466675\n",
      "Loss: 2.359320640563965\n",
      "Loss: 2.359280824661255\n",
      "Loss: 2.359241485595703\n",
      "Loss: 2.3592026233673096\n",
      "Loss: 2.359163522720337\n",
      "Loss: 2.3591251373291016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.3590869903564453\n",
      "Loss: 2.35904860496521\n",
      "Loss: 2.359011173248291\n",
      "Loss: 2.358973503112793\n",
      "Loss: 2.358936071395874\n",
      "Loss: 2.3588995933532715\n",
      "Loss: 2.3588626384735107\n",
      "Loss: 2.358825922012329\n",
      "Loss: 2.3587899208068848\n",
      "Loss: 2.3587536811828613\n",
      "Loss: 2.358717679977417\n",
      "Loss: 2.358682155609131\n",
      "Loss: 2.358646869659424\n",
      "Loss: 2.358612060546875\n",
      "Loss: 2.358577251434326\n",
      "Loss: 2.3585424423217773\n",
      "Loss: 2.358508348464966\n",
      "Loss: 2.358474016189575\n",
      "Loss: 2.3584399223327637\n",
      "Loss: 2.3584063053131104\n",
      "Loss: 2.358372926712036\n",
      "Loss: 2.358339548110962\n",
      "Loss: 2.358306646347046\n",
      "Loss: 2.35827374458313\n",
      "Loss: 2.358241081237793\n",
      "Loss: 2.3582088947296143\n",
      "Loss: 2.3581762313842773\n",
      "Loss: 2.358144521713257\n",
      "Loss: 2.3581130504608154\n",
      "Loss: 2.358081340789795\n",
      "Loss: 2.3580501079559326\n",
      "Loss: 2.358018636703491\n",
      "Loss: 2.357987880706787\n",
      "Loss: 2.357957124710083\n",
      "Loss: 2.3579261302948\n",
      "Loss: 2.357896089553833\n",
      "Loss: 2.357866048812866\n",
      "Loss: 2.3578360080718994\n",
      "Loss: 2.357806444168091\n",
      "Loss: 2.357776641845703\n",
      "Loss: 2.3577473163604736\n",
      "Loss: 2.357717752456665\n",
      "Loss: 2.357689142227173\n",
      "Loss: 2.3576600551605225\n",
      "Loss: 2.357631206512451\n",
      "Loss: 2.357602834701538\n",
      "Loss: 2.357574462890625\n",
      "Loss: 2.357546329498291\n",
      "Loss: 2.357518434524536\n",
      "Loss: 2.3574907779693604\n",
      "Loss: 2.3574628829956055\n",
      "Loss: 2.357435703277588\n",
      "Loss: 2.3574085235595703\n",
      "Loss: 2.3573811054229736\n",
      "Loss: 2.3573544025421143\n",
      "Loss: 2.357327461242676\n",
      "Loss: 2.3573007583618164\n",
      "Loss: 2.357274293899536\n",
      "Loss: 2.357248306274414\n",
      "Loss: 2.357222080230713\n",
      "Loss: 2.35719633102417\n",
      "Loss: 2.357170343399048\n",
      "Loss: 2.357144832611084\n",
      "Loss: 2.357119083404541\n",
      "Loss: 2.3570940494537354\n",
      "Loss: 2.3570685386657715\n",
      "Loss: 2.357043504714966\n",
      "Loss: 2.3570187091827393\n",
      "Loss: 2.3569939136505127\n",
      "Loss: 2.3569698333740234\n",
      "Loss: 2.356945037841797\n",
      "Loss: 2.3569209575653076\n",
      "Loss: 2.3568966388702393\n",
      "Loss: 2.35687255859375\n",
      "Loss: 2.35684871673584\n",
      "Loss: 2.356825351715088\n",
      "Loss: 2.3568012714385986\n",
      "Loss: 2.356778144836426\n",
      "Loss: 2.356754779815674\n",
      "Loss: 2.356731414794922\n",
      "Loss: 2.3567087650299072\n",
      "Loss: 2.3566858768463135\n",
      "Loss: 2.3566629886627197\n",
      "Loss: 2.356640577316284\n",
      "Loss: 2.3566181659698486\n",
      "Loss: 2.356595516204834\n",
      "Loss: 2.3565733432769775\n",
      "Loss: 2.3565514087677\n",
      "Loss: 2.356529474258423\n",
      "Loss: 2.3565073013305664\n",
      "Loss: 2.3564858436584473\n",
      "Loss: 2.356464385986328\n",
      "Loss: 2.35644268989563\n",
      "Loss: 2.35642147064209\n",
      "Loss: 2.356400489807129\n",
      "Loss: 2.3563790321350098\n",
      "Loss: 2.356358289718628\n",
      "Loss: 2.356337547302246\n",
      "Loss: 2.356316328048706\n",
      "Loss: 2.3562960624694824\n",
      "Loss: 2.3562753200531006\n",
      "Loss: 2.356255054473877\n",
      "Loss: 2.3562350273132324\n",
      "Loss: 2.356214761734009\n",
      "Loss: 2.3561947345733643\n",
      "Loss: 2.356174945831299\n",
      "Loss: 2.3561551570892334\n",
      "Loss: 2.356135368347168\n",
      "Loss: 2.3561158180236816\n",
      "Loss: 2.3560962677001953\n",
      "Loss: 2.356076955795288\n",
      "Loss: 2.356057643890381\n",
      "Loss: 2.3560385704040527\n",
      "Loss: 2.3560192584991455\n",
      "Loss: 2.3560004234313965\n",
      "Loss: 2.3559818267822266\n",
      "Loss: 2.3559629917144775\n",
      "Loss: 2.3559443950653076\n",
      "Loss: 2.355926036834717\n",
      "Loss: 2.355907678604126\n",
      "Loss: 2.355889320373535\n",
      "Loss: 2.3558712005615234\n",
      "Loss: 2.3558530807495117\n",
      "Loss: 2.355835199356079\n",
      "Loss: 2.3558170795440674\n",
      "Loss: 2.3557991981506348\n",
      "Loss: 2.3557817935943604\n",
      "Loss: 2.355764150619507\n",
      "Loss: 2.3557465076446533\n",
      "Loss: 2.355729341506958\n",
      "Loss: 2.3557116985321045\n",
      "Loss: 2.3556947708129883\n",
      "Loss: 2.355677604675293\n",
      "Loss: 2.3556604385375977\n",
      "Loss: 2.3556435108184814\n",
      "Loss: 2.355626344680786\n",
      "Loss: 2.355609655380249\n",
      "Loss: 2.355592966079712\n",
      "Loss: 2.355576515197754\n",
      "Loss: 2.355559825897217\n",
      "Loss: 2.355543613433838\n",
      "Loss: 2.355527400970459\n",
      "Loss: 2.355510950088501\n",
      "Loss: 2.355494976043701\n",
      "Loss: 2.3554787635803223\n",
      "Loss: 2.3554627895355225\n",
      "Loss: 2.355447292327881\n",
      "Loss: 2.355431318283081\n",
      "Loss: 2.3554153442382812\n",
      "Loss: 2.3553998470306396\n",
      "Loss: 2.355384349822998\n",
      "Loss: 2.3553690910339355\n",
      "Loss: 2.355353593826294\n",
      "Loss: 2.3553383350372314\n",
      "Loss: 2.355323076248169\n",
      "Loss: 2.3553078174591064\n",
      "Loss: 2.355293035507202\n",
      "Loss: 2.3552780151367188\n",
      "Loss: 2.3552629947662354\n",
      "Loss: 2.355248212814331\n",
      "Loss: 2.3552334308624268\n",
      "Loss: 2.3552186489105225\n",
      "Loss: 2.3552043437957764\n",
      "Loss: 2.355189561843872\n",
      "Loss: 2.355175256729126\n",
      "Loss: 2.35516095161438\n",
      "Loss: 2.3551464080810547\n",
      "Loss: 2.3551323413848877\n",
      "Loss: 2.3551182746887207\n",
      "Loss: 2.355104446411133\n",
      "Loss: 2.355090379714966\n",
      "Loss: 2.355076313018799\n",
      "Loss: 2.355062484741211\n",
      "Loss: 2.355048656463623\n",
      "Loss: 2.355034828186035\n",
      "Loss: 2.3550214767456055\n",
      "Loss: 2.3550076484680176\n",
      "Loss: 2.354994297027588\n",
      "Loss: 2.354980945587158\n",
      "Loss: 2.3549673557281494\n",
      "Loss: 2.354954242706299\n",
      "Loss: 2.354940891265869\n",
      "Loss: 2.3549277782440186\n",
      "Loss: 2.354914665222168\n",
      "Loss: 2.3549015522003174\n",
      "Loss: 2.354888677597046\n",
      "Loss: 2.3548758029937744\n",
      "Loss: 2.354862928390503\n",
      "Loss: 2.3548502922058105\n",
      "Loss: 2.354837656021118\n",
      "Loss: 2.354825019836426\n",
      "Loss: 2.3548126220703125\n",
      "Loss: 2.35479998588562\n",
      "Loss: 2.3547873497009277\n",
      "Loss: 2.3547751903533936\n",
      "Loss: 2.3547627925872803\n",
      "Loss: 2.354750394821167\n",
      "Loss: 2.354738473892212\n",
      "Loss: 2.3547260761260986\n",
      "Loss: 2.3547141551971436\n",
      "Loss: 2.3547017574310303\n",
      "Loss: 2.3546900749206543\n",
      "Loss: 2.354678153991699\n",
      "Loss: 2.354666233062744\n",
      "Loss: 2.3546547889709473\n",
      "Loss: 2.3546431064605713\n",
      "Loss: 2.354630947113037\n",
      "Loss: 2.3546195030212402\n",
      "Loss: 2.3546078205108643\n",
      "Loss: 2.3545966148376465\n",
      "Loss: 2.3545849323272705\n",
      "Loss: 2.3545737266540527\n",
      "Loss: 2.354562520980835\n",
      "Loss: 2.354551315307617\n",
      "Loss: 2.3545398712158203\n",
      "Loss: 2.3545289039611816\n",
      "Loss: 2.354517698287964\n",
      "Loss: 2.3545069694519043\n",
      "Loss: 2.3544957637786865\n",
      "Loss: 2.3544845581054688\n",
      "Loss: 2.354473829269409\n",
      "Loss: 2.3544628620147705\n",
      "Loss: 2.35445237159729\n",
      "Loss: 2.3544414043426514\n",
      "Loss: 2.354430913925171\n",
      "Loss: 2.3544201850891113\n",
      "Loss: 2.354409694671631\n",
      "Loss: 2.3543989658355713\n",
      "Loss: 2.35438871383667\n",
      "Loss: 2.3543777465820312\n",
      "Loss: 2.35436749458313\n",
      "Loss: 2.3543574810028076\n",
      "Loss: 2.3543474674224854\n",
      "Loss: 2.354336977005005\n",
      "Loss: 2.3543267250061035\n",
      "Loss: 2.3543167114257812\n",
      "Loss: 2.35430645942688\n",
      "Loss: 2.3542964458465576\n",
      "Loss: 2.3542864322662354\n",
      "Loss: 2.354276657104492\n",
      "Loss: 2.35426664352417\n",
      "Loss: 2.3542568683624268\n",
      "Loss: 2.3542470932006836\n",
      "Loss: 2.3542373180389404\n",
      "Loss: 2.3542275428771973\n",
      "Loss: 2.354217767715454\n",
      "Loss: 2.3542087078094482\n",
      "Loss: 2.354198694229126\n",
      "Loss: 2.354189395904541\n",
      "Loss: 2.354179859161377\n",
      "Loss: 2.354170799255371\n",
      "Loss: 2.354161024093628\n",
      "Loss: 2.354151487350464\n",
      "Loss: 2.354142427444458\n",
      "Loss: 2.354132890701294\n",
      "Loss: 2.354123830795288\n",
      "Loss: 2.3541147708892822\n",
      "Loss: 2.3541054725646973\n",
      "Loss: 2.3540964126586914\n",
      "Loss: 2.3540873527526855\n",
      "Loss: 2.354078531265259\n",
      "Loss: 2.354069232940674\n",
      "Loss: 2.354060411453247\n",
      "Loss: 2.3540515899658203\n",
      "Loss: 2.3540427684783936\n",
      "Loss: 2.354034185409546\n",
      "Loss: 2.35402512550354\n",
      "Loss: 2.3540163040161133\n",
      "Loss: 2.3540079593658447\n",
      "Loss: 2.353999376296997\n",
      "Loss: 2.3539905548095703\n",
      "Loss: 2.3539822101593018\n",
      "Loss: 2.353973150253296\n",
      "Loss: 2.3539648056030273\n",
      "Loss: 2.353956699371338\n",
      "Loss: 2.3539481163024902\n",
      "Loss: 2.3539397716522217\n",
      "Loss: 2.353931427001953\n",
      "Loss: 2.3539233207702637\n",
      "Loss: 2.353914976119995\n",
      "Loss: 2.3539066314697266\n",
      "Loss: 2.353898525238037\n",
      "Loss: 2.3538901805877686\n",
      "Loss: 2.3538825511932373\n",
      "Loss: 2.3538742065429688\n",
      "Loss: 2.3538661003112793\n",
      "Loss: 2.35385799407959\n",
      "Loss: 2.3538503646850586\n",
      "Loss: 2.353842258453369\n",
      "Loss: 2.353834390640259\n",
      "Loss: 2.3538265228271484\n",
      "Loss: 2.353818655014038\n",
      "Loss: 2.353811025619507\n",
      "Loss: 2.3538029193878174\n",
      "Loss: 2.353795051574707\n",
      "Loss: 2.353787899017334\n",
      "Loss: 2.3537800312042236\n",
      "Loss: 2.3537726402282715\n",
      "Loss: 2.353764772415161\n",
      "Loss: 2.353757619857788\n",
      "Loss: 2.353749990463257\n",
      "Loss: 2.3537421226501465\n",
      "Loss: 2.3537349700927734\n",
      "Loss: 2.3537275791168213\n",
      "Loss: 2.353720188140869\n",
      "Loss: 2.353712558746338\n",
      "Loss: 2.3537051677703857\n",
      "Loss: 2.3536980152130127\n",
      "Loss: 2.3536911010742188\n",
      "Loss: 2.3536834716796875\n",
      "Loss: 2.3536763191223145\n",
      "Loss: 2.3536694049835205\n",
      "Loss: 2.3536617755889893\n",
      "Loss: 2.3536548614501953\n",
      "Loss: 2.3536477088928223\n",
      "Loss: 2.3536410331726074\n",
      "Loss: 2.3536341190338135\n",
      "Loss: 2.3536272048950195\n",
      "Loss: 2.3536200523376465\n",
      "Loss: 2.3536131381988525\n",
      "Loss: 2.3536059856414795\n",
      "Loss: 2.3535995483398438\n",
      "Loss: 2.3535923957824707\n",
      "Loss: 2.353585720062256\n",
      "Loss: 2.353579044342041\n",
      "Loss: 2.353572130203247\n",
      "Loss: 2.3535656929016113\n",
      "Loss: 2.3535590171813965\n",
      "Loss: 2.3535523414611816\n",
      "Loss: 2.3535451889038086\n",
      "Loss: 2.353538751602173\n",
      "Loss: 2.353532314300537\n",
      "Loss: 2.3535256385803223\n",
      "Loss: 2.3535194396972656\n",
      "Loss: 2.353512763977051\n",
      "Loss: 2.353506326675415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.3534996509552\n",
      "Loss: 2.3534932136535645\n",
      "Loss: 2.353487253189087\n",
      "Loss: 2.353480577468872\n",
      "Loss: 2.3534741401672363\n",
      "Loss: 2.3534679412841797\n",
      "Loss: 2.353461742401123\n",
      "Loss: 2.3534555435180664\n",
      "Loss: 2.3534493446350098\n",
      "Loss: 2.3534433841705322\n",
      "Loss: 2.3534369468688965\n",
      "Loss: 2.3534305095672607\n",
      "Loss: 2.3534247875213623\n",
      "Loss: 2.3534185886383057\n",
      "Loss: 2.3534128665924072\n",
      "Loss: 2.3534061908721924\n",
      "Loss: 2.353400468826294\n",
      "Loss: 2.3533945083618164\n",
      "Loss: 2.353388786315918\n",
      "Loss: 2.3533825874328613\n",
      "Loss: 2.353376865386963\n",
      "Loss: 2.3533709049224854\n",
      "Loss: 2.353364944458008\n",
      "Loss: 2.3533589839935303\n",
      "Loss: 2.3533530235290527\n",
      "Loss: 2.3533477783203125\n",
      "Loss: 2.353341579437256\n",
      "Loss: 2.3533360958099365\n"
     ]
    }
   ],
   "source": [
    "for _ in range(700):\n",
    "    logits = torch.matmul(x_oh_reshaped, weights)\n",
    "    counts = logits.exp()\n",
    "    probs = torch.div(counts, torch.sum(counts, dim=1, keepdims=True))\n",
    "    loss = -probs[torch.arange(xs.shape[0]), ys].log().mean() + 0.01 * (weights ** 2).mean()\n",
    "    print(f\"Loss: {loss.item()}\")\n",
    "    weights.grad = None\n",
    "    loss.backward()\n",
    "    weights.data += -30 * weights.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6805dbe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can\n",
      "ahior\n",
      "slea\n",
      "em\n",
      "molariagialaven\n",
      "kali\n",
      "ustia\n",
      "kentham\n",
      "jara\n",
      "cyl\n"
     ]
    }
   ],
   "source": [
    "gen = torch.Generator().manual_seed(2147483648)\n",
    "for _ in range(10):\n",
    "    out = []\n",
    "    idx1 = 0\n",
    "    idx2 = 0\n",
    "    \n",
    "    while True:\n",
    "        x_enc_1 = torch.nn.functional.one_hot(torch.tensor([idx1]), num_classes=27).float()\n",
    "        x_enc_2 = torch.nn.functional.one_hot(torch.tensor([idx2]), num_classes=27).float()\n",
    "        \n",
    "        logits = torch.matmul(torch.hstack((x_enc_1, x_enc_2)), weights)\n",
    "        counts = logits.exp()\n",
    "        probs = torch.div(counts, torch.sum(counts, dim=1, keepdims=True))\n",
    "        \n",
    "        idx3 = torch.multinomial(probs, num_samples=1, replacement=True, generator=gen).item()\n",
    "        if idx3 == 0:\n",
    "            break\n",
    "        idx1 = idx2\n",
    "        idx2 = idx3\n",
    "        out.append(itos[idx3])\n",
    "        \n",
    "    print(\"\".join(out))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb9e5cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6bab00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read names from file\n",
    "with open(\"names.txt\", \"r\") as f:\n",
    "    names = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5085cfc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = [name.strip() for name in names]\n",
    "names[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0dd46142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the names\n",
    "import re\n",
    "names = [re.sub('[-,.]', '', name) for name in names]\n",
    "names = [re.sub(r'\\(.*\\)', '', name) for name in names]\n",
    "names = [name.lower() for name in names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1ef26cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bacc06cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "letter_set = {'.'}\n",
    "\n",
    "for name in names:\n",
    "    for l in list(name):\n",
    "        letter_set.add(l)\n",
    "        \n",
    "letter_set = sorted(list(letter_set))\n",
    "len(letter_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32b7d3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = {letter: pos for pos, letter in enumerate(letter_set)}\n",
    "itos = {pos: letter for letter, pos in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13974487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1]],\n",
       "\n",
       "        [[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1]],\n",
       "\n",
       "        [[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1]],\n",
       "\n",
       "        [[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1]],\n",
       "\n",
       "        [[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1]]], dtype=torch.int32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lookup_table = torch.ones((27, 27, 27), dtype=torch.int32)\n",
    "lookup_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fec3e08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in names:\n",
    "    name = ['.', '.'] + list(name) + ['.']\n",
    "    for char1, char2, char3 in zip(name, name[1:], name[2:]):\n",
    "        p1, p2, p3 = stoi[char1], stoi[char2], stoi[char3]\n",
    "        lookup_table[p1, p2, p3] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e937f829",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_occurences(char1, char2, char3):\n",
    "    return lookup_table[stoi[char1], stoi[char2], stoi[char3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bdbcbfc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4411, dtype=torch.int32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_occurences('.', '.', 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "691ca7be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(729.0001)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lookup_table = torch.div(lookup_table, torch.sum(lookup_table, dim=2, keepdims=True))\n",
    "torch.sum(lookup_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ad1007a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:  junide\n",
      "Name:  jakasid\n",
      "Name:  prelay\n",
      "Name:  adin\n",
      "Name:  kairritoper\n",
      "Name:  sathen\n",
      "Name:  sameia\n",
      "Name:  yanileniassibduinrwin\n",
      "Name:  lessiyanayla\n",
      "Name:  te\n"
     ]
    }
   ],
   "source": [
    "gen = torch.Generator().manual_seed(2147483647)\n",
    "for _ in range(10):\n",
    "    idx1, idx2 = 0, 0\n",
    "    out = []\n",
    "    while True:\n",
    "        idx3 = torch.multinomial(lookup_table[idx1, idx2], num_samples=1, replacement=True, generator=gen).item()\n",
    "        if idx3==0:\n",
    "            break\n",
    "        out.append(itos[idx3])\n",
    "        idx1 = idx2\n",
    "        idx2 = idx3\n",
    "    print(\"Name: \", \"\".join(out))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89ab5498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nll_loss: 2.206512451171875\n"
     ]
    }
   ],
   "source": [
    "log_likelihood = 0.0 \n",
    "num_samples = 0\n",
    "for name in names[:2]:\n",
    "    name = ['.', '.'] + list(name) + ['.']\n",
    "    for char1, char2, char3 in zip(name, name[1:], name[2:]):\n",
    "        prob = lookup_table[stoi[char1], stoi[char2], stoi[char3]]\n",
    "        log_likelihood += torch.log(prob)\n",
    "        num_samples += 1\n",
    "nll = -log_likelihood\n",
    "print(f\"nll_loss: {nll/num_samples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c8477af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 456292\n"
     ]
    }
   ],
   "source": [
    "xs, ys = [], []\n",
    "for name in names:\n",
    "    name = ['.', '.'] + list(name) + ['.']\n",
    "    for char1, char2, char3 in zip(name, name[1:], name[2:]):\n",
    "        xs.append([stoi[char1], stoi[char2]])\n",
    "        ys.append(stoi[char3])\n",
    "        \n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "print(f\"Number of samples: {xs.nelement()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a8aacd62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0],\n",
       "        [ 0,  5],\n",
       "        [ 5, 13],\n",
       "        ...,\n",
       "        [26, 25],\n",
       "        [25, 26],\n",
       "        [26, 24]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89e9b857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of encoded inputs: torch.Size([228146, 2, 27])\n",
      "Shape of weights matrix: torch.Size([54, 27])\n"
     ]
    }
   ],
   "source": [
    "gen = torch.Generator().manual_seed(2147483647)\n",
    "x_oh = torch.nn.functional.one_hot(xs, num_classes=27).float()\n",
    "weights = torch.randn((54, 27), requires_grad=True, generator=gen)\n",
    "print(f\"Shape of encoded inputs: {x_oh.shape}\")\n",
    "print(f\"Shape of weights matrix: {weights.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b5b51f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([228146, 2, 27]), torch.Size([54, 27]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_oh.shape, weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ddd0c84b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([228146])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bbcc1ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_oh_reshaped = x_oh.view(x_oh.shape[0], x_oh.shape[1] * x_oh.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4fe51dd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([228146, 54])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_oh_reshaped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9d41efcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 4.232541561126709\n",
      "Loss: 3.553056001663208\n",
      "Loss: 3.27421236038208\n",
      "Loss: 3.0445430278778076\n",
      "Loss: 2.9447267055511475\n",
      "Loss: 2.859506845474243\n",
      "Loss: 2.814803123474121\n",
      "Loss: 2.7571210861206055\n",
      "Loss: 2.7277557849884033\n",
      "Loss: 2.6887495517730713\n",
      "Loss: 2.6680307388305664\n",
      "Loss: 2.6388931274414062\n",
      "Loss: 2.624164342880249\n",
      "Loss: 2.601041555404663\n",
      "Loss: 2.590730905532837\n",
      "Loss: 2.571537971496582\n",
      "Loss: 2.5645530223846436\n",
      "Loss: 2.548041343688965\n",
      "Loss: 2.543558120727539\n",
      "Loss: 2.528939723968506\n",
      "Loss: 2.526331663131714\n",
      "Loss: 2.5130996704101562\n",
      "Loss: 2.511906862258911\n",
      "Loss: 2.4997267723083496\n",
      "Loss: 2.499626874923706\n",
      "Loss: 2.4882683753967285\n",
      "Loss: 2.4890353679656982\n",
      "Loss: 2.47832989692688\n",
      "Loss: 2.4798033237457275\n",
      "Loss: 2.4696247577667236\n",
      "Loss: 2.4716858863830566\n",
      "Loss: 2.46193528175354\n",
      "Loss: 2.464493989944458\n",
      "Loss: 2.4550940990448\n",
      "Loss: 2.458078145980835\n",
      "Loss: 2.4489693641662598\n",
      "Loss: 2.4523208141326904\n",
      "Loss: 2.4434547424316406\n",
      "Loss: 2.4471256732940674\n",
      "Loss: 2.438464641571045\n",
      "Loss: 2.442415237426758\n",
      "Loss: 2.4339280128479004\n",
      "Loss: 2.4381251335144043\n",
      "Loss: 2.4297878742218018\n",
      "Loss: 2.434203624725342\n",
      "Loss: 2.4259958267211914\n",
      "Loss: 2.4306063652038574\n",
      "Loss: 2.4225103855133057\n",
      "Loss: 2.427295207977295\n",
      "Loss: 2.4192981719970703\n",
      "Loss: 2.424238920211792\n",
      "Loss: 2.4163284301757812\n",
      "Loss: 2.4214096069335938\n",
      "Loss: 2.413576364517212\n",
      "Loss: 2.4187843799591064\n",
      "Loss: 2.411020040512085\n",
      "Loss: 2.4163429737091064\n",
      "Loss: 2.408639669418335\n",
      "Loss: 2.414067029953003\n",
      "Loss: 2.406418800354004\n",
      "Loss: 2.411940097808838\n",
      "Loss: 2.4043421745300293\n",
      "Loss: 2.409949541091919\n",
      "Loss: 2.4023969173431396\n",
      "Loss: 2.4080824851989746\n",
      "Loss: 2.40057110786438\n",
      "Loss: 2.406327486038208\n",
      "Loss: 2.3988542556762695\n",
      "Loss: 2.4046757221221924\n",
      "Loss: 2.3972373008728027\n",
      "Loss: 2.4031178951263428\n",
      "Loss: 2.3957126140594482\n",
      "Loss: 2.401646852493286\n",
      "Loss: 2.3942716121673584\n",
      "Loss: 2.4002554416656494\n",
      "Loss: 2.3929078578948975\n",
      "Loss: 2.3989367485046387\n",
      "Loss: 2.391615629196167\n",
      "Loss: 2.3976855278015137\n",
      "Loss: 2.3903892040252686\n",
      "Loss: 2.3964974880218506\n",
      "Loss: 2.389224052429199\n",
      "Loss: 2.39536714553833\n",
      "Loss: 2.388115406036377\n",
      "Loss: 2.3942902088165283\n",
      "Loss: 2.387058973312378\n",
      "Loss: 2.393263339996338\n",
      "Loss: 2.386051654815674\n",
      "Loss: 2.392282724380493\n",
      "Loss: 2.385089874267578\n",
      "Loss: 2.3913462162017822\n",
      "Loss: 2.3841700553894043\n",
      "Loss: 2.3904495239257812\n",
      "Loss: 2.3832905292510986\n",
      "Loss: 2.3895912170410156\n",
      "Loss: 2.3824474811553955\n",
      "Loss: 2.388767957687378\n",
      "Loss: 2.3816394805908203\n",
      "Loss: 2.387979030609131\n",
      "Loss: 2.3808646202087402\n",
      "Loss: 2.387220859527588\n",
      "Loss: 2.380120038986206\n",
      "Loss: 2.3864920139312744\n",
      "Loss: 2.379404306411743\n",
      "Loss: 2.385791063308716\n",
      "Loss: 2.3787155151367188\n",
      "Loss: 2.3851163387298584\n",
      "Loss: 2.3780527114868164\n",
      "Loss: 2.3844659328460693\n",
      "Loss: 2.377413749694824\n",
      "Loss: 2.3838393688201904\n",
      "Loss: 2.376797914505005\n",
      "Loss: 2.383235216140747\n",
      "Loss: 2.376204013824463\n",
      "Loss: 2.3826513290405273\n",
      "Loss: 2.3756301403045654\n",
      "Loss: 2.382087469100952\n",
      "Loss: 2.3750760555267334\n",
      "Loss: 2.381542921066284\n",
      "Loss: 2.3745405673980713\n",
      "Loss: 2.381016254425049\n",
      "Loss: 2.3740222454071045\n",
      "Loss: 2.3805060386657715\n",
      "Loss: 2.373520851135254\n",
      "Loss: 2.3800127506256104\n",
      "Loss: 2.373035192489624\n",
      "Loss: 2.3795344829559326\n",
      "Loss: 2.372565269470215\n",
      "Loss: 2.3790712356567383\n",
      "Loss: 2.3721089363098145\n",
      "Loss: 2.3786215782165527\n",
      "Loss: 2.371666669845581\n",
      "Loss: 2.378185749053955\n",
      "Loss: 2.3712375164031982\n",
      "Loss: 2.377763032913208\n",
      "Loss: 2.370821237564087\n",
      "Loss: 2.377351999282837\n",
      "Loss: 2.3704168796539307\n",
      "Loss: 2.376953363418579\n",
      "Loss: 2.370023727416992\n",
      "Loss: 2.37656569480896\n",
      "Loss: 2.3696420192718506\n",
      "Loss: 2.3761887550354004\n",
      "Loss: 2.3692705631256104\n",
      "Loss: 2.375822067260742\n",
      "Loss: 2.3689095973968506\n",
      "Loss: 2.3754658699035645\n",
      "Loss: 2.368558406829834\n",
      "Loss: 2.3751182556152344\n",
      "Loss: 2.368216037750244\n",
      "Loss: 2.3747806549072266\n",
      "Loss: 2.36788272857666\n",
      "Loss: 2.3744516372680664\n",
      "Loss: 2.367558479309082\n",
      "Loss: 2.374130964279175\n",
      "Loss: 2.3672425746917725\n",
      "Loss: 2.3738186359405518\n",
      "Loss: 2.3669347763061523\n",
      "Loss: 2.373514413833618\n",
      "Loss: 2.3666341304779053\n",
      "Loss: 2.3732171058654785\n",
      "Loss: 2.3663411140441895\n",
      "Loss: 2.37292742729187\n",
      "Loss: 2.3660552501678467\n",
      "Loss: 2.3726446628570557\n",
      "Loss: 2.365776538848877\n",
      "Loss: 2.372368812561035\n",
      "Loss: 2.365504503250122\n",
      "Loss: 2.3720993995666504\n",
      "Loss: 2.365238666534424\n",
      "Loss: 2.3718364238739014\n",
      "Loss: 2.3649790287017822\n",
      "Loss: 2.371580123901367\n",
      "Loss: 2.3647255897521973\n",
      "Loss: 2.371328830718994\n",
      "Loss: 2.3644773960113525\n",
      "Loss: 2.3710832595825195\n",
      "Loss: 2.3642351627349854\n",
      "Loss: 2.3708436489105225\n",
      "Loss: 2.3639981746673584\n",
      "Loss: 2.3706092834472656\n",
      "Loss: 2.363767147064209\n",
      "Loss: 2.370380401611328\n",
      "Loss: 2.3635408878326416\n",
      "Loss: 2.3701560497283936\n",
      "Loss: 2.3633193969726562\n",
      "Loss: 2.36993670463562\n",
      "Loss: 2.363102674484253\n",
      "Loss: 2.3697221279144287\n",
      "Loss: 2.3628909587860107\n",
      "Loss: 2.3695123195648193\n",
      "Loss: 2.3626832962036133\n",
      "Loss: 2.369306802749634\n",
      "Loss: 2.3624801635742188\n",
      "Loss: 2.3691048622131348\n",
      "Loss: 2.362281084060669\n",
      "Loss: 2.368907928466797\n",
      "Loss: 2.362086057662964\n",
      "Loss: 2.3687150478363037\n",
      "Loss: 2.3618955612182617\n",
      "Loss: 2.368525743484497\n",
      "Loss: 2.361708402633667\n",
      "Loss: 2.368340253829956\n",
      "Loss: 2.361525058746338\n",
      "Loss: 2.3681588172912598\n",
      "Loss: 2.3613455295562744\n",
      "Loss: 2.36798095703125\n",
      "Loss: 2.3611695766448975\n",
      "Loss: 2.3678064346313477\n",
      "Loss: 2.360996961593628\n",
      "Loss: 2.3676352500915527\n",
      "Loss: 2.360827684402466\n",
      "Loss: 2.367467164993286\n",
      "Loss: 2.3606619834899902\n",
      "Loss: 2.367302894592285\n",
      "Loss: 2.3604989051818848\n",
      "Loss: 2.3671412467956543\n",
      "Loss: 2.360339403152466\n",
      "Loss: 2.3669824600219727\n",
      "Loss: 2.360182523727417\n",
      "Loss: 2.3668270111083984\n",
      "Loss: 2.3600285053253174\n",
      "Loss: 2.3666744232177734\n",
      "Loss: 2.359877347946167\n",
      "Loss: 2.3665246963500977\n",
      "Loss: 2.359729051589966\n",
      "Loss: 2.366377592086792\n",
      "Loss: 2.359583616256714\n",
      "Loss: 2.3662331104278564\n",
      "Loss: 2.359440803527832\n",
      "Loss: 2.366091251373291\n",
      "Loss: 2.3593006134033203\n",
      "Loss: 2.3659517765045166\n",
      "Loss: 2.3591625690460205\n",
      "Loss: 2.3658149242401123\n",
      "Loss: 2.3590269088745117\n",
      "Loss: 2.365680694580078\n",
      "Loss: 2.358893871307373\n",
      "Loss: 2.3655476570129395\n",
      "Loss: 2.3587629795074463\n",
      "Loss: 2.3654184341430664\n",
      "Loss: 2.3586344718933105\n",
      "Loss: 2.365290403366089\n",
      "Loss: 2.3585081100463867\n",
      "Loss: 2.3651652336120605\n",
      "Loss: 2.3583836555480957\n",
      "Loss: 2.365041494369507\n",
      "Loss: 2.358261823654175\n",
      "Loss: 2.3649203777313232\n",
      "Loss: 2.3581411838531494\n",
      "Loss: 2.3648009300231934\n",
      "Loss: 2.3580234050750732\n",
      "Loss: 2.3646838665008545\n",
      "Loss: 2.357907295227051\n",
      "Loss: 2.3645684719085693\n",
      "Loss: 2.357792615890503\n",
      "Loss: 2.364454746246338\n",
      "Loss: 2.357680320739746\n",
      "Loss: 2.36434268951416\n",
      "Loss: 2.357569456100464\n",
      "Loss: 2.3642325401306152\n",
      "Loss: 2.3574604988098145\n",
      "Loss: 2.3641245365142822\n",
      "Loss: 2.357353448867798\n",
      "Loss: 2.3640177249908447\n",
      "Loss: 2.3572475910186768\n",
      "Loss: 2.36391282081604\n",
      "Loss: 2.3571441173553467\n",
      "Loss: 2.363809585571289\n",
      "Loss: 2.357041597366333\n",
      "Loss: 2.3637077808380127\n",
      "Loss: 2.3569412231445312\n",
      "Loss: 2.36360764503479\n",
      "Loss: 2.356842041015625\n",
      "Loss: 2.363509178161621\n",
      "Loss: 2.3567440509796143\n",
      "Loss: 2.3634121417999268\n",
      "Loss: 2.3566479682922363\n",
      "Loss: 2.363316535949707\n",
      "Loss: 2.356552839279175\n",
      "Loss: 2.3632218837738037\n",
      "Loss: 2.356459856033325\n",
      "Loss: 2.363129138946533\n",
      "Loss: 2.356367826461792\n",
      "Loss: 2.363037586212158\n",
      "Loss: 2.3562772274017334\n",
      "Loss: 2.362947463989258\n",
      "Loss: 2.3561880588531494\n",
      "Loss: 2.362858533859253\n",
      "Loss: 2.356099843978882\n",
      "Loss: 2.3627712726593018\n",
      "Loss: 2.356013059616089\n",
      "Loss: 2.362684488296509\n",
      "Loss: 2.3559274673461914\n",
      "Loss: 2.3625991344451904\n",
      "Loss: 2.3558430671691895\n",
      "Loss: 2.362515687942505\n",
      "Loss: 2.355760097503662\n",
      "Loss: 2.3624324798583984\n",
      "Loss: 2.3556783199310303\n",
      "Loss: 2.362351417541504\n",
      "Loss: 2.3555970191955566\n",
      "Loss: 2.3622708320617676\n",
      "Loss: 2.355517625808716\n",
      "Loss: 2.3621912002563477\n",
      "Loss: 2.3554389476776123\n",
      "Loss: 2.3621132373809814\n",
      "Loss: 2.355361223220825\n",
      "Loss: 2.3620362281799316\n",
      "Loss: 2.355285167694092\n",
      "Loss: 2.3619601726531982\n",
      "Loss: 2.3552095890045166\n",
      "Loss: 2.3618850708007812\n",
      "Loss: 2.355134963989258\n",
      "Loss: 2.3618104457855225\n",
      "Loss: 2.3550617694854736\n",
      "Loss: 2.3617377281188965\n",
      "Loss: 2.3549892902374268\n",
      "Loss: 2.3616650104522705\n",
      "Loss: 2.3549177646636963\n",
      "Loss: 2.361593723297119\n",
      "Loss: 2.354846954345703\n",
      "Loss: 2.3615236282348633\n",
      "Loss: 2.3547775745391846\n",
      "Loss: 2.361454486846924\n",
      "Loss: 2.354708433151245\n",
      "Loss: 2.3613855838775635\n",
      "Loss: 2.3546409606933594\n",
      "Loss: 2.361318588256836\n",
      "Loss: 2.354573965072632\n",
      "Loss: 2.3612515926361084\n",
      "Loss: 2.3545079231262207\n",
      "Loss: 2.3611860275268555\n",
      "Loss: 2.354442596435547\n",
      "Loss: 2.3611207008361816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.3543779850006104\n",
      "Loss: 2.3610565662384033\n",
      "Loss: 2.3543145656585693\n",
      "Loss: 2.3609933853149414\n",
      "Loss: 2.3542513847351074\n",
      "Loss: 2.3609306812286377\n",
      "Loss: 2.354189872741699\n",
      "Loss: 2.3608686923980713\n",
      "Loss: 2.35412859916687\n",
      "Loss: 2.3608078956604004\n",
      "Loss: 2.3540680408477783\n",
      "Loss: 2.3607475757598877\n",
      "Loss: 2.354008197784424\n",
      "Loss: 2.3606879711151123\n",
      "Loss: 2.3539493083953857\n",
      "Loss: 2.3606293201446533\n",
      "Loss: 2.3538906574249268\n",
      "Loss: 2.3605709075927734\n",
      "Loss: 2.3538331985473633\n",
      "Loss: 2.36051344871521\n",
      "Loss: 2.353776216506958\n",
      "Loss: 2.360456943511963\n",
      "Loss: 2.35371994972229\n",
      "Loss: 2.360400438308716\n",
      "Loss: 2.3536643981933594\n",
      "Loss: 2.3603451251983643\n",
      "Loss: 2.353609561920166\n",
      "Loss: 2.360290765762329\n",
      "Loss: 2.35355544090271\n",
      "Loss: 2.360236406326294\n",
      "Loss: 2.353501796722412\n",
      "Loss: 2.360182762145996\n",
      "Loss: 2.3534488677978516\n",
      "Loss: 2.3601300716400146\n",
      "Loss: 2.353396415710449\n",
      "Loss: 2.3600780963897705\n",
      "Loss: 2.353344678878784\n",
      "Loss: 2.3600263595581055\n",
      "Loss: 2.3532934188842773\n",
      "Loss: 2.3599753379821777\n",
      "Loss: 2.353243112564087\n",
      "Loss: 2.3599250316619873\n",
      "Loss: 2.3531932830810547\n",
      "Loss: 2.359875440597534\n",
      "Loss: 2.3531439304351807\n",
      "Loss: 2.35982608795166\n",
      "Loss: 2.353095054626465\n",
      "Loss: 2.3597772121429443\n",
      "Loss: 2.3530466556549072\n",
      "Loss: 2.359729051589966\n",
      "Loss: 2.352998971939087\n",
      "Loss: 2.3596816062927246\n",
      "Loss: 2.352952241897583\n",
      "Loss: 2.3596346378326416\n",
      "Loss: 2.352905511856079\n",
      "Loss: 2.359588384628296\n",
      "Loss: 2.3528592586517334\n",
      "Loss: 2.35954213142395\n",
      "Loss: 2.352813720703125\n",
      "Loss: 2.359496593475342\n",
      "Loss: 2.352768659591675\n",
      "Loss: 2.3594517707824707\n",
      "Loss: 2.352724075317383\n",
      "Loss: 2.3594071865081787\n",
      "Loss: 2.352679967880249\n",
      "Loss: 2.359363317489624\n",
      "Loss: 2.3526363372802734\n",
      "Loss: 2.3593196868896484\n",
      "Loss: 2.352593183517456\n",
      "Loss: 2.359276533126831\n",
      "Loss: 2.352550506591797\n",
      "Loss: 2.35923433303833\n",
      "Loss: 2.352508306503296\n",
      "Loss: 2.35919189453125\n",
      "Loss: 2.352466344833374\n",
      "Loss: 2.359149932861328\n",
      "Loss: 2.3524250984191895\n",
      "Loss: 2.3591089248657227\n",
      "Loss: 2.352384328842163\n",
      "Loss: 2.3590681552886963\n",
      "Loss: 2.352343797683716\n",
      "Loss: 2.359027862548828\n",
      "Loss: 2.352303981781006\n",
      "Loss: 2.358988046646118\n",
      "Loss: 2.352264642715454\n",
      "Loss: 2.358948230743408\n",
      "Loss: 2.3522253036499023\n",
      "Loss: 2.3589093685150146\n",
      "Loss: 2.352186441421509\n",
      "Loss: 2.3588707447052\n",
      "Loss: 2.3521482944488525\n",
      "Loss: 2.3588321208953857\n",
      "Loss: 2.3521101474761963\n",
      "Loss: 2.358794689178467\n",
      "Loss: 2.3520727157592773\n",
      "Loss: 2.358757495880127\n",
      "Loss: 2.3520359992980957\n",
      "Loss: 2.358720064163208\n",
      "Loss: 2.351998805999756\n",
      "Loss: 2.3586833477020264\n",
      "Loss: 2.3519625663757324\n",
      "Loss: 2.358647346496582\n",
      "Loss: 2.351926565170288\n",
      "Loss: 2.358611583709717\n",
      "Loss: 2.351890802383423\n",
      "Loss: 2.3585762977600098\n",
      "Loss: 2.351855993270874\n",
      "Loss: 2.3585410118103027\n",
      "Loss: 2.3518214225769043\n",
      "Loss: 2.358506202697754\n",
      "Loss: 2.3517866134643555\n",
      "Loss: 2.358471393585205\n",
      "Loss: 2.351752281188965\n",
      "Loss: 2.3584372997283936\n",
      "Loss: 2.3517189025878906\n",
      "Loss: 2.3584039211273193\n",
      "Loss: 2.351684808731079\n",
      "Loss: 2.358370304107666\n",
      "Loss: 2.351651906967163\n",
      "Loss: 2.358337163925171\n",
      "Loss: 2.351619243621826\n",
      "Loss: 2.358304262161255\n",
      "Loss: 2.3515865802764893\n",
      "Loss: 2.3582723140716553\n",
      "Loss: 2.3515543937683105\n",
      "Loss: 2.3582396507263184\n",
      "Loss: 2.351522445678711\n",
      "Loss: 2.358208179473877\n",
      "Loss: 2.3514909744262695\n",
      "Loss: 2.3581762313842773\n",
      "Loss: 2.3514599800109863\n",
      "Loss: 2.358144998550415\n",
      "Loss: 2.351428747177124\n",
      "Loss: 2.35811448097229\n",
      "Loss: 2.351398229598999\n",
      "Loss: 2.358084201812744\n",
      "Loss: 2.351367950439453\n",
      "Loss: 2.3580539226531982\n",
      "Loss: 2.3513379096984863\n",
      "Loss: 2.3580241203308105\n",
      "Loss: 2.3513081073760986\n",
      "Loss: 2.3579938411712646\n",
      "Loss: 2.35127854347229\n",
      "Loss: 2.357964277267456\n",
      "Loss: 2.3512496948242188\n",
      "Loss: 2.3579351902008057\n",
      "Loss: 2.3512208461761475\n",
      "Loss: 2.3579068183898926\n",
      "Loss: 2.351191997528076\n",
      "Loss: 2.3578779697418213\n",
      "Loss: 2.351163864135742\n",
      "Loss: 2.3578498363494873\n",
      "Loss: 2.3511359691619873\n",
      "Loss: 2.3578219413757324\n",
      "Loss: 2.351107597351074\n",
      "Loss: 2.3577942848205566\n",
      "Loss: 2.3510806560516357\n",
      "Loss: 2.357766628265381\n",
      "Loss: 2.35105299949646\n",
      "Loss: 2.3577394485473633\n",
      "Loss: 2.3510260581970215\n",
      "Loss: 2.3577122688293457\n",
      "Loss: 2.350999116897583\n",
      "Loss: 2.3576853275299072\n",
      "Loss: 2.3509726524353027\n",
      "Loss: 2.357658863067627\n",
      "Loss: 2.3509466648101807\n",
      "Loss: 2.357632637023926\n",
      "Loss: 2.3509202003479004\n",
      "Loss: 2.357606887817383\n",
      "Loss: 2.3508944511413574\n",
      "Loss: 2.3575809001922607\n",
      "Loss: 2.3508691787719727\n",
      "Loss: 2.357555627822876\n",
      "Loss: 2.350843667984009\n",
      "Loss: 2.357529878616333\n",
      "Loss: 2.350818634033203\n",
      "Loss: 2.3575048446655273\n",
      "Loss: 2.3507933616638184\n",
      "Loss: 2.3574798107147217\n",
      "Loss: 2.350768804550171\n",
      "Loss: 2.357455253601074\n",
      "Loss: 2.3507444858551025\n",
      "Loss: 2.357430934906006\n",
      "Loss: 2.3507204055786133\n",
      "Loss: 2.3574066162109375\n",
      "Loss: 2.350696563720703\n",
      "Loss: 2.3573827743530273\n",
      "Loss: 2.350672721862793\n",
      "Loss: 2.3573594093322754\n",
      "Loss: 2.350649118423462\n",
      "Loss: 2.3573355674743652\n",
      "Loss: 2.350625514984131\n",
      "Loss: 2.357311725616455\n",
      "Loss: 2.350602149963379\n",
      "Loss: 2.3572888374328613\n",
      "Loss: 2.350579261779785\n",
      "Loss: 2.3572657108306885\n",
      "Loss: 2.3505566120147705\n",
      "Loss: 2.357243299484253\n",
      "Loss: 2.350533962249756\n",
      "Loss: 2.3572206497192383\n",
      "Loss: 2.350511312484741\n",
      "Loss: 2.3571979999542236\n",
      "Loss: 2.350489377975464\n",
      "Loss: 2.357175827026367\n",
      "Loss: 2.3504674434661865\n",
      "Loss: 2.35715389251709\n",
      "Loss: 2.350445508956909\n",
      "Loss: 2.3571319580078125\n",
      "Loss: 2.350424289703369\n",
      "Loss: 2.3571105003356934\n",
      "Loss: 2.350402593612671\n",
      "Loss: 2.357089042663574\n",
      "Loss: 2.3503811359405518\n",
      "Loss: 2.3570680618286133\n",
      "Loss: 2.35036039352417\n",
      "Loss: 2.357046604156494\n",
      "Loss: 2.350339412689209\n",
      "Loss: 2.3570261001586914\n",
      "Loss: 2.350318431854248\n",
      "Loss: 2.3570051193237305\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m weights\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m weights\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m40\u001b[39m \u001b[38;5;241m*\u001b[39m weights\u001b[38;5;241m.\u001b[39mgrad\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.10/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.10/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for _ in range(1000):\n",
    "    logits = torch.matmul(x_oh_reshaped, weights)\n",
    "    counts = logits.exp()\n",
    "    probs = torch.div(counts, torch.sum(counts, dim=1, keepdims=True))\n",
    "#     print(probs.shape)\n",
    "    loss = 0.0\n",
    "#     for i in range(2):\n",
    "    loss += -probs[torch.arange(xs.shape[0]), ys].log().mean()\n",
    "#     loss = -probs[torch.arange(xs.shape[0]), torch.arange(xs.shape[1]), ys].log().mean()\n",
    "    print(f\"Loss: {loss.item()}\")\n",
    "    weights.grad = None\n",
    "    loss.backward()\n",
    "    weights.data += -40 * weights.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6805dbe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "junide\n",
      "janasad\n",
      "pres\n",
      "yon\n",
      "na\n",
      "koi\n",
      "ritoleras\n",
      "tee\n",
      "kilania\n",
      "yanileniassibdainrwi\n"
     ]
    }
   ],
   "source": [
    "gen = torch.Generator().manual_seed(2147483647)\n",
    "for _ in range(10):\n",
    "    out = []\n",
    "    idx1 = 0\n",
    "    idx2 = 0\n",
    "    \n",
    "    while True:\n",
    "        x_enc_1 = torch.nn.functional.one_hot(torch.tensor([idx1]), num_classes=27).float()\n",
    "        x_enc_2 = torch.nn.functional.one_hot(torch.tensor([idx2]), num_classes=27).float()\n",
    "        \n",
    "        logits = torch.matmul(torch.hstack((x_enc_1, x_enc_2)), weights)\n",
    "        counts = logits.exp()\n",
    "        probs = torch.div(counts, torch.sum(counts, dim=1, keepdims=True))\n",
    "        \n",
    "        idx3 = torch.multinomial(probs, num_samples=1, replacement=True, generator=gen).item()\n",
    "        if idx3 == 0:\n",
    "#             print(\"break\")\n",
    "            break\n",
    "        idx1 = idx2\n",
    "        idx2 = idx3\n",
    "        out.append(itos[idx3])\n",
    "        \n",
    "#     print(probs.shape)\n",
    "    print(\"\".join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f750dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ab = torch.tensor([[1,2], [3, 4]])\n",
    "ab.view(1,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "307dab73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 54])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx1 = 0\n",
    "idx2 = 0\n",
    "\n",
    "x_enc_1 = torch.nn.functional.one_hot(torch.tensor([idx1]), num_classes=27).float()\n",
    "x_enc_2 = torch.nn.functional.one_hot(torch.tensor([idx2]), num_classes=27).float()\n",
    "torch.hstack((x_enc_1, x_enc_2)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38963c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.hstack()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb9e5cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6bab00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read names from file\n",
    "with open(\"names.txt\", \"r\") as f:\n",
    "    names = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5085cfc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['emma', 'olivia', 'ava']\n",
      "32033\n"
     ]
    }
   ],
   "source": [
    "names = [name.strip() for name in names]\n",
    "print(names[:3])\n",
    "print(len(names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "625269b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "letter_set = sorted(list({l for name in names for l in name}))\n",
    "letter_set.insert(0, '.')\n",
    "len(letter_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32b7d3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = {letter: pos for pos, letter in enumerate(letter_set)}\n",
    "itos = {pos: letter for letter, pos in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ddd5d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = [(a, b) for a in letter_set  for b in letter_set if b!= '.']\n",
    "pairs.insert(0, ('.', '.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36d91337",
   "metadata": {},
   "outputs": [],
   "source": [
    "ptoi = {pair: pos for pos, pair in enumerate(pairs)}\n",
    "itop = {pos: pair for pair, pos in ptoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cead5d6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "703"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b150d447",
   "metadata": {},
   "source": [
    "### E01: train a trigram language model, i.e. take two characters as an input to predict the 3rd one. Feel free to use either counting or a neural net. Evaluate the loss; Did it improve over a bigram model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13974487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 27 by 27 by 27 matrix to count all the number of occurences of trigrams\n",
    "lookup_table = torch.ones((703, 27), dtype=torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fec3e08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the lookup table with the counts of each trigram\n",
    "for name in names:\n",
    "    name = ['.', '.'] + list(name) + ['.']\n",
    "    for char1, char2, char3 in zip(name, name[1:], name[2:]):\n",
    "        p1, p2 = ptoi[(char1, char2)],  stoi[char3]\n",
    "        lookup_table[p1, p2] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e937f829",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_occurences(char1, char2, char3):\n",
    "    return lookup_table[ptoi[(char1, char2)], stoi[char3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bdbcbfc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4411, dtype=torch.int32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_occurences('.', '.', 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "691ca7be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(703.0001)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize the count table \n",
    "lookup_table = torch.div(lookup_table, torch.sum(lookup_table, dim=1, keepdims=True))\n",
    "torch.sum(lookup_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ad1007a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " junide\n",
      " jakasid\n",
      " prelay\n",
      " adin\n",
      " kairritoper\n",
      " sathen\n",
      " sameia\n",
      " yanileniassibduinrwin\n",
      " lessiyanayla\n",
      " te\n"
     ]
    }
   ],
   "source": [
    "# Get some new predictions using the counts table\n",
    "gen = torch.Generator().manual_seed(2147483647)\n",
    "output_count = []\n",
    "for _ in range(10):\n",
    "    prev = ('.', '.')\n",
    "    out = []\n",
    "    while True:\n",
    "        idx = torch.multinomial(lookup_table[ptoi[prev]], num_samples=1, replacement=True, generator=gen).item()\n",
    "        if idx==0:\n",
    "            break\n",
    "        out.append(itos[idx])\n",
    "        prev = (prev[1], itos[idx])\n",
    "    output_count.append(\"\".join(out))\n",
    "    print(\"\", \"\".join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89ab5498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_likelihood=tensor(-504653.)\n",
      "nll_loss: 2.2120\n",
      "bigram_loss: 2.4544\n"
     ]
    }
   ],
   "source": [
    "# Calculate the loss (negative log likelihood loss) and compare it to the loss of the bigram model (previously done)\n",
    "log_likelihood = 0.0 \n",
    "num_samples = 0\n",
    "for name in names:\n",
    "    name = ['.', '.'] + list(name) + ['.']\n",
    "    for char1, char2, char3 in zip(name, name[1:], name[2:]):\n",
    "        prob = lookup_table[ptoi[(char1, char2)], stoi[char3]]\n",
    "        log_likelihood += torch.log(prob)\n",
    "        num_samples += 1\n",
    "nll = -log_likelihood\n",
    "print(f\"{log_likelihood=}\")\n",
    "print(f\"nll_loss: {nll/num_samples:.4f}\")\n",
    "print(f\"bigram_loss: 2.4544\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1c8477af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 228146\n"
     ]
    }
   ],
   "source": [
    "# Create trigram samples to train a gradient based model\n",
    "xs, ys = [], []\n",
    "for name in names:\n",
    "    name = ['.', '.'] + list(name) + ['.']\n",
    "    for char1, char2, char3 in zip(name, name[1:], name[2:]):\n",
    "        xs.append(ptoi[(char1, char2)])\n",
    "        ys.append(stoi[char3])\n",
    "        \n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "print(f\"Number of samples: {xs.nelement()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "89e9b857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of encoded inputs: torch.Size([228146, 703])\n",
      "Shape of weights matrix: torch.Size([703, 27])\n",
      "Shape of labels vector: torch.Size([228146])\n"
     ]
    }
   ],
   "source": [
    "# Change the inputs into one hot vectors and initialize the weights\n",
    "x_oh = torch.nn.functional.one_hot(xs, num_classes=703).float()\n",
    "# Reshape the input matrix from [_, 2, 27] to [_, 54] to make the multiplication easier\n",
    "# x_oh = x_oh.view(x_oh.shape[0], x_oh.shape[1] * x_oh.shape[2])\n",
    "W = torch.randn((703, 27), requires_grad=True)\n",
    "print(f\"Shape of encoded inputs: {x_oh.shape}\")\n",
    "print(f\"Shape of weights matrix: {W.shape}\")\n",
    "print(f\"Shape of labels vector: {ys.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "06b4ba25",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "W = W.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "9d41efcd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.198673725128174\n",
      "Loss: 2.198671579360962\n",
      "Loss: 2.198669672012329\n",
      "Loss: 2.198667526245117\n",
      "Loss: 2.1986653804779053\n",
      "Loss: 2.1986632347106934\n",
      "Loss: 2.1986608505249023\n",
      "Loss: 2.1986589431762695\n",
      "Loss: 2.1986567974090576\n",
      "Loss: 2.1986546516418457\n",
      "Loss: 2.198652505874634\n",
      "Loss: 2.198650360107422\n",
      "Loss: 2.198648452758789\n",
      "Loss: 2.198646068572998\n",
      "Loss: 2.1986441612243652\n",
      "Loss: 2.1986420154571533\n",
      "Loss: 2.1986398696899414\n",
      "Loss: 2.1986377239227295\n",
      "Loss: 2.1986355781555176\n",
      "Loss: 2.1986334323883057\n",
      "Loss: 2.1986312866210938\n",
      "Loss: 2.198629379272461\n",
      "Loss: 2.19862699508667\n",
      "Loss: 2.198624849319458\n",
      "Loss: 2.198622703552246\n",
      "Loss: 2.1986207962036133\n",
      "Loss: 2.1986186504364014\n",
      "Loss: 2.1986162662506104\n",
      "Loss: 2.1986143589019775\n",
      "Loss: 2.1986122131347656\n",
      "Loss: 2.1986100673675537\n",
      "Loss: 2.198608160018921\n",
      "Loss: 2.19860577583313\n",
      "Loss: 2.198603630065918\n",
      "Loss: 2.198601722717285\n",
      "Loss: 2.1985995769500732\n",
      "Loss: 2.1985974311828613\n",
      "Loss: 2.1985952854156494\n",
      "Loss: 2.1985929012298584\n",
      "Loss: 2.1985909938812256\n",
      "Loss: 2.1985888481140137\n",
      "Loss: 2.198586940765381\n",
      "Loss: 2.19858455657959\n",
      "Loss: 2.198582649230957\n",
      "Loss: 2.198580741882324\n",
      "Loss: 2.1985785961151123\n",
      "Loss: 2.1985762119293213\n",
      "Loss: 2.1985743045806885\n",
      "Loss: 2.1985721588134766\n",
      "Loss: 2.1985700130462646\n",
      "Loss: 2.198568105697632\n",
      "Loss: 2.19856595993042\n",
      "Loss: 2.198564052581787\n",
      "Loss: 2.198561429977417\n",
      "Loss: 2.198559522628784\n",
      "Loss: 2.1985573768615723\n",
      "Loss: 2.1985552310943604\n",
      "Loss: 2.1985533237457275\n",
      "Loss: 2.1985514163970947\n",
      "Loss: 2.1985490322113037\n",
      "Loss: 2.198546886444092\n",
      "Loss: 2.19854474067688\n",
      "Loss: 2.198542833328247\n",
      "Loss: 2.1985409259796143\n",
      "Loss: 2.1985385417938232\n",
      "Loss: 2.1985366344451904\n",
      "Loss: 2.1985344886779785\n",
      "Loss: 2.1985323429107666\n",
      "Loss: 2.1985301971435547\n",
      "Loss: 2.198528289794922\n",
      "Loss: 2.19852614402771\n",
      "Loss: 2.198524236679077\n",
      "Loss: 2.1985220909118652\n",
      "Loss: 2.1985199451446533\n",
      "Loss: 2.1985177993774414\n",
      "Loss: 2.1985156536102295\n",
      "Loss: 2.1985135078430176\n",
      "Loss: 2.1985116004943848\n",
      "Loss: 2.198509454727173\n",
      "Loss: 2.19850754737854\n",
      "Loss: 2.198505401611328\n",
      "Loss: 2.198503255844116\n",
      "Loss: 2.1985011100769043\n",
      "Loss: 2.1984989643096924\n",
      "Loss: 2.1984970569610596\n",
      "Loss: 2.1984949111938477\n",
      "Loss: 2.198493003845215\n",
      "Loss: 2.198490858078003\n",
      "Loss: 2.198488712310791\n",
      "Loss: 2.198486566543579\n",
      "Loss: 2.198484420776367\n",
      "Loss: 2.1984825134277344\n",
      "Loss: 2.1984803676605225\n",
      "Loss: 2.1984784603118896\n",
      "Loss: 2.1984763145446777\n",
      "Loss: 2.198474168777466\n",
      "Loss: 2.198472261428833\n",
      "Loss: 2.198470115661621\n",
      "Loss: 2.1984682083129883\n",
      "Loss: 2.1984658241271973\n",
      "Loss: 2.1984639167785645\n",
      "Loss: 2.1984617710113525\n",
      "Loss: 2.1984598636627197\n",
      "Loss: 2.198457717895508\n",
      "Loss: 2.198455810546875\n",
      "Loss: 2.198453903198242\n",
      "Loss: 2.1984517574310303\n",
      "Loss: 2.1984498500823975\n",
      "Loss: 2.1984474658966064\n",
      "Loss: 2.1984455585479736\n",
      "Loss: 2.1984434127807617\n",
      "Loss: 2.198441505432129\n",
      "Loss: 2.198439359664917\n",
      "Loss: 2.198437452316284\n",
      "Loss: 2.1984353065490723\n",
      "Loss: 2.1984331607818604\n",
      "Loss: 2.1984310150146484\n",
      "Loss: 2.1984291076660156\n",
      "Loss: 2.1984269618988037\n",
      "Loss: 2.198425054550171\n",
      "Loss: 2.198422908782959\n",
      "Loss: 2.198421001434326\n",
      "Loss: 2.1984188556671143\n",
      "Loss: 2.1984169483184814\n",
      "Loss: 2.1984145641326904\n",
      "Loss: 2.1984126567840576\n",
      "Loss: 2.1984105110168457\n",
      "Loss: 2.198408603668213\n",
      "Loss: 2.198406457901001\n",
      "Loss: 2.198404550552368\n",
      "Loss: 2.1984024047851562\n",
      "Loss: 2.1984007358551025\n",
      "Loss: 2.1983985900878906\n",
      "Loss: 2.1983964443206787\n",
      "Loss: 2.198394298553467\n",
      "Loss: 2.198392391204834\n",
      "Loss: 2.198390245437622\n",
      "Loss: 2.1983883380889893\n",
      "Loss: 2.1983861923217773\n",
      "Loss: 2.1983840465545654\n",
      "Loss: 2.1983821392059326\n",
      "Loss: 2.1983802318573\n",
      "Loss: 2.198378086090088\n",
      "Loss: 2.198375940322876\n",
      "Loss: 2.198374032974243\n",
      "Loss: 2.1983721256256104\n",
      "Loss: 2.1983699798583984\n",
      "Loss: 2.1983680725097656\n",
      "Loss: 2.1983659267425537\n",
      "Loss: 2.198363780975342\n",
      "Loss: 2.198361873626709\n",
      "Loss: 2.198359727859497\n",
      "Loss: 2.1983580589294434\n",
      "Loss: 2.1983561515808105\n",
      "Loss: 2.1983537673950195\n",
      "Loss: 2.1983516216278076\n",
      "Loss: 2.198349714279175\n",
      "Loss: 2.198347806930542\n",
      "Loss: 2.198345899581909\n",
      "Loss: 2.1983437538146973\n",
      "Loss: 2.1983418464660645\n",
      "Loss: 2.1983397006988525\n",
      "Loss: 2.1983377933502197\n",
      "Loss: 2.198335647583008\n",
      "Loss: 2.198333501815796\n",
      "Loss: 2.198331594467163\n",
      "Loss: 2.1983296871185303\n",
      "Loss: 2.1983275413513184\n",
      "Loss: 2.1983253955841064\n",
      "Loss: 2.1983234882354736\n",
      "Loss: 2.198321580886841\n",
      "Loss: 2.198319673538208\n",
      "Loss: 2.198317766189575\n",
      "Loss: 2.1983156204223633\n",
      "Loss: 2.1983134746551514\n",
      "Loss: 2.1983115673065186\n",
      "Loss: 2.1983096599578857\n",
      "Loss: 2.198307514190674\n",
      "Loss: 2.198305606842041\n",
      "Loss: 2.198303699493408\n",
      "Loss: 2.1983015537261963\n",
      "Loss: 2.1982994079589844\n",
      "Loss: 2.1982977390289307\n",
      "Loss: 2.1982955932617188\n",
      "Loss: 2.198293447494507\n",
      "Loss: 2.198291301727295\n",
      "Loss: 2.198289394378662\n",
      "Loss: 2.1982874870300293\n",
      "Loss: 2.1982855796813965\n",
      "Loss: 2.1982836723327637\n",
      "Loss: 2.1982815265655518\n",
      "Loss: 2.198279619216919\n",
      "Loss: 2.198277473449707\n",
      "Loss: 2.198275566101074\n",
      "Loss: 2.1982734203338623\n",
      "Loss: 2.1982717514038086\n",
      "Loss: 2.1982693672180176\n",
      "Loss: 2.1982674598693848\n",
      "Loss: 2.198265552520752\n",
      "Loss: 2.198263645172119\n",
      "Loss: 2.1982617378234863\n",
      "Loss: 2.1982595920562744\n",
      "Loss: 2.1982576847076416\n",
      "Loss: 2.1982555389404297\n",
      "Loss: 2.198253631591797\n",
      "Loss: 2.198251724243164\n",
      "Loss: 2.1982498168945312\n",
      "Loss: 2.1982479095458984\n",
      "Loss: 2.1982457637786865\n",
      "Loss: 2.1982436180114746\n",
      "Loss: 2.198241710662842\n",
      "Loss: 2.198239803314209\n",
      "Loss: 2.198237657546997\n",
      "Loss: 2.1982359886169434\n",
      "Loss: 2.1982338428497314\n",
      "Loss: 2.1982319355010986\n",
      "Loss: 2.1982297897338867\n",
      "Loss: 2.198227882385254\n",
      "Loss: 2.198225975036621\n",
      "Loss: 2.1982240676879883\n",
      "Loss: 2.1982219219207764\n",
      "Loss: 2.1982200145721436\n",
      "Loss: 2.1982178688049316\n",
      "Loss: 2.198215961456299\n",
      "Loss: 2.198214054107666\n",
      "Loss: 2.198211908340454\n",
      "Loss: 2.1982100009918213\n",
      "Loss: 2.1982083320617676\n",
      "Loss: 2.1982061862945557\n",
      "Loss: 2.1982040405273438\n",
      "Loss: 2.198202133178711\n",
      "Loss: 2.198200225830078\n",
      "Loss: 2.1981985569000244\n",
      "Loss: 2.1981964111328125\n",
      "Loss: 2.1981945037841797\n",
      "Loss: 2.198192596435547\n",
      "Loss: 2.198190450668335\n",
      "Loss: 2.198188543319702\n",
      "Loss: 2.1981863975524902\n",
      "Loss: 2.1981847286224365\n",
      "Loss: 2.1981825828552246\n",
      "Loss: 2.198180675506592\n",
      "Loss: 2.198179006576538\n",
      "Loss: 2.198176860809326\n",
      "Loss: 2.1981749534606934\n",
      "Loss: 2.1981728076934814\n",
      "Loss: 2.1981709003448486\n",
      "Loss: 2.198168992996216\n",
      "Loss: 2.198167085647583\n",
      "Loss: 2.19816517829895\n",
      "Loss: 2.1981632709503174\n",
      "Loss: 2.1981611251831055\n",
      "Loss: 2.1981589794158936\n",
      "Loss: 2.1981570720672607\n",
      "Loss: 2.198155164718628\n",
      "Loss: 2.198153257369995\n",
      "Loss: 2.1981513500213623\n",
      "Loss: 2.1981494426727295\n",
      "Loss: 2.1981475353240967\n",
      "Loss: 2.198145627975464\n",
      "Loss: 2.198143720626831\n",
      "Loss: 2.198141574859619\n",
      "Loss: 2.1981396675109863\n",
      "Loss: 2.1981377601623535\n",
      "Loss: 2.1981358528137207\n",
      "Loss: 2.198133945465088\n",
      "Loss: 2.198131799697876\n",
      "Loss: 2.198129892349243\n",
      "Loss: 2.1981282234191895\n",
      "Loss: 2.1981260776519775\n",
      "Loss: 2.1981241703033447\n",
      "Loss: 2.198122262954712\n",
      "Loss: 2.198120355606079\n",
      "Loss: 2.1981184482574463\n",
      "Loss: 2.1981163024902344\n",
      "Loss: 2.1981146335601807\n",
      "Loss: 2.198112726211548\n",
      "Loss: 2.198110818862915\n",
      "Loss: 2.198108673095703\n",
      "Loss: 2.1981067657470703\n",
      "Loss: 2.1981048583984375\n",
      "Loss: 2.1981029510498047\n",
      "Loss: 2.1981008052825928\n",
      "Loss: 2.198099136352539\n",
      "Loss: 2.198096990585327\n",
      "Loss: 2.1980950832366943\n",
      "Loss: 2.1980931758880615\n",
      "Loss: 2.1980912685394287\n",
      "Loss: 2.198089361190796\n",
      "Loss: 2.198087453842163\n",
      "Loss: 2.1980857849121094\n",
      "Loss: 2.1980836391448975\n",
      "Loss: 2.1980814933776855\n",
      "Loss: 2.198079824447632\n",
      "Loss: 2.198077917098999\n",
      "Loss: 2.198076009750366\n",
      "Loss: 2.1980738639831543\n",
      "Loss: 2.1980721950531006\n",
      "Loss: 2.1980700492858887\n",
      "Loss: 2.198068141937256\n",
      "Loss: 2.198065996170044\n",
      "Loss: 2.1980643272399902\n",
      "Loss: 2.1980624198913574\n",
      "Loss: 2.1980607509613037\n",
      "Loss: 2.198058605194092\n",
      "Loss: 2.198056697845459\n",
      "Loss: 2.198054790496826\n",
      "Loss: 2.1980528831481934\n",
      "Loss: 2.1980509757995605\n",
      "Loss: 2.1980490684509277\n",
      "Loss: 2.198046922683716\n",
      "Loss: 2.198045253753662\n",
      "Loss: 2.1980433464050293\n",
      "Loss: 2.1980412006378174\n",
      "Loss: 2.1980395317077637\n",
      "Loss: 2.1980373859405518\n",
      "Loss: 2.198035717010498\n",
      "Loss: 2.1980338096618652\n",
      "Loss: 2.1980316638946533\n",
      "Loss: 2.1980297565460205\n",
      "Loss: 2.198028087615967\n",
      "Loss: 2.198026180267334\n",
      "Loss: 2.198024272918701\n",
      "Loss: 2.1980223655700684\n",
      "Loss: 2.1980204582214355\n",
      "Loss: 2.1980185508728027\n",
      "Loss: 2.19801664352417\n",
      "Loss: 2.198014497756958\n",
      "Loss: 2.1980128288269043\n",
      "Loss: 2.1980109214782715\n",
      "Loss: 2.1980090141296387\n",
      "Loss: 2.198007106781006\n",
      "Loss: 2.198005199432373\n",
      "Loss: 2.1980032920837402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.1980013847351074\n",
      "Loss: 2.1979994773864746\n",
      "Loss: 2.197997570037842\n",
      "Loss: 2.197995901107788\n",
      "Loss: 2.1979939937591553\n",
      "Loss: 2.1979920864105225\n",
      "Loss: 2.1979899406433105\n",
      "Loss: 2.1979880332946777\n",
      "Loss: 2.197986125946045\n",
      "Loss: 2.197984218597412\n",
      "Loss: 2.1979823112487793\n",
      "Loss: 2.1979806423187256\n",
      "Loss: 2.1979787349700928\n",
      "Loss: 2.197976589202881\n",
      "Loss: 2.197974920272827\n",
      "Loss: 2.1979727745056152\n",
      "Loss: 2.1979711055755615\n",
      "Loss: 2.1979691982269287\n",
      "Loss: 2.197967290878296\n",
      "Loss: 2.197965383529663\n",
      "Loss: 2.1979634761810303\n",
      "Loss: 2.1979618072509766\n",
      "Loss: 2.1979598999023438\n",
      "Loss: 2.197957992553711\n",
      "Loss: 2.197956085205078\n",
      "Loss: 2.1979541778564453\n",
      "Loss: 2.1979522705078125\n",
      "Loss: 2.1979503631591797\n",
      "Loss: 2.197948455810547\n",
      "Loss: 2.197946548461914\n",
      "Loss: 2.1979446411132812\n",
      "Loss: 2.1979427337646484\n",
      "Loss: 2.1979410648345947\n",
      "Loss: 2.197939157485962\n",
      "Loss: 2.197937250137329\n",
      "Loss: 2.1979353427886963\n",
      "Loss: 2.1979334354400635\n",
      "Loss: 2.1979315280914307\n",
      "Loss: 2.197929859161377\n",
      "Loss: 2.197927951812744\n",
      "Loss: 2.1979260444641113\n",
      "Loss: 2.1979241371154785\n",
      "Loss: 2.1979222297668457\n",
      "Loss: 2.197920083999634\n",
      "Loss: 2.19791841506958\n",
      "Loss: 2.1979167461395264\n",
      "Loss: 2.1979148387908936\n",
      "Loss: 2.1979126930236816\n",
      "Loss: 2.197910785675049\n",
      "Loss: 2.197909116744995\n",
      "Loss: 2.1979072093963623\n",
      "Loss: 2.1979053020477295\n",
      "Loss: 2.1979033946990967\n",
      "Loss: 2.197901487350464\n",
      "Loss: 2.19789981842041\n",
      "Loss: 2.1978979110717773\n",
      "Loss: 2.1978960037231445\n",
      "Loss: 2.197894334793091\n",
      "Loss: 2.197892189025879\n",
      "Loss: 2.197890520095825\n",
      "Loss: 2.1978886127471924\n",
      "Loss: 2.1978867053985596\n",
      "Loss: 2.197885036468506\n",
      "Loss: 2.197883129119873\n",
      "Loss: 2.1978812217712402\n",
      "Loss: 2.1978793144226074\n",
      "Loss: 2.1978774070739746\n",
      "Loss: 2.197875499725342\n",
      "Loss: 2.197873830795288\n",
      "Loss: 2.1978719234466553\n",
      "Loss: 2.1978700160980225\n",
      "Loss: 2.1978683471679688\n",
      "Loss: 2.197866439819336\n",
      "Loss: 2.197864532470703\n",
      "Loss: 2.1978626251220703\n",
      "Loss: 2.1978607177734375\n",
      "Loss: 2.197859048843384\n",
      "Loss: 2.197857141494751\n",
      "Loss: 2.197855234146118\n",
      "Loss: 2.1978533267974854\n",
      "Loss: 2.1978516578674316\n",
      "Loss: 2.197849750518799\n",
      "Loss: 2.197847843170166\n",
      "Loss: 2.197845935821533\n",
      "Loss: 2.1978442668914795\n",
      "Loss: 2.1978423595428467\n",
      "Loss: 2.197840452194214\n",
      "Loss: 2.19783878326416\n",
      "Loss: 2.1978368759155273\n",
      "Loss: 2.1978349685668945\n",
      "Loss: 2.1978330612182617\n",
      "Loss: 2.197831392288208\n",
      "Loss: 2.197829484939575\n",
      "Loss: 2.1978275775909424\n",
      "Loss: 2.1978256702423096\n",
      "Loss: 2.197824001312256\n",
      "Loss: 2.197822093963623\n",
      "Loss: 2.1978201866149902\n",
      "Loss: 2.1978182792663574\n",
      "Loss: 2.1978166103363037\n",
      "Loss: 2.197814702987671\n",
      "Loss: 2.197812795639038\n",
      "Loss: 2.1978108882904053\n",
      "Loss: 2.1978092193603516\n",
      "Loss: 2.1978073120117188\n",
      "Loss: 2.197805643081665\n",
      "Loss: 2.1978037357330322\n",
      "Loss: 2.1978018283843994\n",
      "Loss: 2.1978001594543457\n",
      "Loss: 2.197798252105713\n",
      "Loss: 2.19779634475708\n",
      "Loss: 2.1977944374084473\n",
      "Loss: 2.1977927684783936\n",
      "Loss: 2.1977908611297607\n",
      "Loss: 2.197788953781128\n",
      "Loss: 2.197787284851074\n",
      "Loss: 2.1977853775024414\n",
      "Loss: 2.1977834701538086\n",
      "Loss: 2.197781801223755\n",
      "Loss: 2.197780132293701\n",
      "Loss: 2.1977782249450684\n",
      "Loss: 2.1977763175964355\n",
      "Loss: 2.1977744102478027\n",
      "Loss: 2.19777250289917\n",
      "Loss: 2.197770833969116\n",
      "Loss: 2.1977689266204834\n",
      "Loss: 2.1977672576904297\n",
      "Loss: 2.197765350341797\n",
      "Loss: 2.197763442993164\n",
      "Loss: 2.1977617740631104\n",
      "Loss: 2.1977598667144775\n",
      "Loss: 2.197758197784424\n",
      "Loss: 2.197756290435791\n",
      "Loss: 2.197754383087158\n",
      "Loss: 2.1977524757385254\n",
      "Loss: 2.1977508068084717\n",
      "Loss: 2.197748899459839\n",
      "Loss: 2.197747230529785\n",
      "Loss: 2.1977455615997314\n",
      "Loss: 2.1977436542510986\n",
      "Loss: 2.197741746902466\n",
      "Loss: 2.197739839553833\n",
      "Loss: 2.1977379322052\n",
      "Loss: 2.1977360248565674\n",
      "Loss: 2.1977343559265137\n",
      "Loss: 2.197732448577881\n",
      "Loss: 2.197730779647827\n",
      "Loss: 2.1977291107177734\n",
      "Loss: 2.1977272033691406\n",
      "Loss: 2.197725534439087\n",
      "Loss: 2.197723865509033\n",
      "Loss: 2.1977219581604004\n",
      "Loss: 2.1977200508117676\n",
      "Loss: 2.197718381881714\n",
      "Loss: 2.197716474533081\n",
      "Loss: 2.1977145671844482\n",
      "Loss: 2.1977128982543945\n",
      "Loss: 2.1977109909057617\n",
      "Loss: 2.197709321975708\n",
      "Loss: 2.1977076530456543\n",
      "Loss: 2.1977057456970215\n",
      "Loss: 2.1977038383483887\n",
      "Loss: 2.197701930999756\n",
      "Loss: 2.197700262069702\n",
      "Loss: 2.1976985931396484\n",
      "Loss: 2.1976966857910156\n",
      "Loss: 2.197695016860962\n",
      "Loss: 2.197693109512329\n",
      "Loss: 2.1976912021636963\n",
      "Loss: 2.1976892948150635\n",
      "Loss: 2.1976876258850098\n",
      "Loss: 2.197685956954956\n",
      "Loss: 2.1976840496063232\n",
      "Loss: 2.1976823806762695\n",
      "Loss: 2.1976804733276367\n",
      "Loss: 2.197678804397583\n",
      "Loss: 2.19767689704895\n",
      "Loss: 2.1976752281188965\n",
      "Loss: 2.1976733207702637\n",
      "Loss: 2.19767165184021\n",
      "Loss: 2.197669744491577\n",
      "Loss: 2.1976678371429443\n",
      "Loss: 2.1976661682128906\n",
      "Loss: 2.197664499282837\n",
      "Loss: 2.197662591934204\n",
      "Loss: 2.1976606845855713\n",
      "Loss: 2.1976590156555176\n",
      "Loss: 2.197657346725464\n",
      "Loss: 2.197655439376831\n",
      "Loss: 2.1976537704467773\n",
      "Loss: 2.1976521015167236\n",
      "Loss: 2.197650194168091\n",
      "Loss: 2.197648525238037\n",
      "Loss: 2.1976466178894043\n",
      "Loss: 2.1976447105407715\n",
      "Loss: 2.1976430416107178\n",
      "Loss: 2.197641134262085\n",
      "Loss: 2.1976394653320312\n",
      "Loss: 2.1976377964019775\n",
      "Loss: 2.1976358890533447\n",
      "Loss: 2.197634220123291\n",
      "Loss: 2.197632312774658\n",
      "Loss: 2.1976306438446045\n",
      "Loss: 2.1976287364959717\n",
      "Loss: 2.197627067565918\n",
      "Loss: 2.197625160217285\n",
      "Loss: 2.1976234912872314\n",
      "Loss: 2.1976215839385986\n",
      "Loss: 2.197619915008545\n",
      "Loss: 2.197618246078491\n",
      "Loss: 2.1976163387298584\n",
      "Loss: 2.1976146697998047\n",
      "Loss: 2.197612762451172\n",
      "Loss: 2.197610855102539\n",
      "Loss: 2.1976091861724854\n",
      "Loss: 2.1976075172424316\n",
      "Loss: 2.197605609893799\n",
      "Loss: 2.197603940963745\n",
      "Loss: 2.1976020336151123\n",
      "Loss: 2.1976006031036377\n",
      "Loss: 2.197598695755005\n",
      "Loss: 2.197597026824951\n",
      "Loss: 2.1975951194763184\n",
      "Loss: 2.1975934505462646\n",
      "Loss: 2.197591543197632\n",
      "Loss: 2.197589874267578\n",
      "Loss: 2.1975879669189453\n",
      "Loss: 2.1975862979888916\n",
      "Loss: 2.197584390640259\n",
      "Loss: 2.197582721710205\n",
      "Loss: 2.1975810527801514\n",
      "Loss: 2.1975791454315186\n",
      "Loss: 2.197577476501465\n",
      "Loss: 2.197575807571411\n",
      "Loss: 2.1975741386413574\n",
      "Loss: 2.1975722312927246\n",
      "Loss: 2.197570323944092\n",
      "Loss: 2.197568655014038\n",
      "Loss: 2.1975669860839844\n",
      "Loss: 2.1975650787353516\n",
      "Loss: 2.197563409805298\n",
      "Loss: 2.197561740875244\n",
      "Loss: 2.1975600719451904\n",
      "Loss: 2.1975581645965576\n",
      "Loss: 2.197556495666504\n",
      "Loss: 2.197554588317871\n",
      "Loss: 2.1975529193878174\n",
      "Loss: 2.1975512504577637\n",
      "Loss: 2.197549343109131\n",
      "Loss: 2.197547674179077\n",
      "Loss: 2.1975460052490234\n",
      "Loss: 2.1975440979003906\n",
      "Loss: 2.197542428970337\n",
      "Loss: 2.197540760040283\n",
      "Loss: 2.1975388526916504\n",
      "Loss: 2.1975371837615967\n",
      "Loss: 2.197535514831543\n",
      "Loss: 2.19753360748291\n",
      "Loss: 2.1975319385528564\n",
      "Loss: 2.1975302696228027\n",
      "Loss: 2.19752836227417\n",
      "Loss: 2.197526693344116\n",
      "Loss: 2.1975247859954834\n",
      "Loss: 2.1975231170654297\n",
      "Loss: 2.197521448135376\n",
      "Loss: 2.1975197792053223\n",
      "Loss: 2.1975181102752686\n",
      "Loss: 2.1975162029266357\n",
      "Loss: 2.197514533996582\n",
      "Loss: 2.1975128650665283\n",
      "Loss: 2.1975109577178955\n",
      "Loss: 2.197509288787842\n",
      "Loss: 2.197507619857788\n",
      "Loss: 2.1975057125091553\n",
      "Loss: 2.1975042819976807\n",
      "Loss: 2.197502374649048\n",
      "Loss: 2.197500705718994\n",
      "Loss: 2.1974987983703613\n",
      "Loss: 2.1974971294403076\n",
      "Loss: 2.197495222091675\n",
      "Loss: 2.197493553161621\n",
      "Loss: 2.1974918842315674\n",
      "Loss: 2.1974902153015137\n",
      "Loss: 2.197488784790039\n",
      "Loss: 2.1974868774414062\n",
      "Loss: 2.1974849700927734\n",
      "Loss: 2.1974833011627197\n",
      "Loss: 2.197481632232666\n",
      "Loss: 2.197479724884033\n",
      "Loss: 2.1974780559539795\n",
      "Loss: 2.197476387023926\n",
      "Loss: 2.197474718093872\n",
      "Loss: 2.1974728107452393\n",
      "Loss: 2.1974711418151855\n",
      "Loss: 2.197469472885132\n",
      "Loss: 2.197467803955078\n",
      "Loss: 2.1974663734436035\n",
      "Loss: 2.1974644660949707\n",
      "Loss: 2.197462797164917\n",
      "Loss: 2.197460889816284\n",
      "Loss: 2.1974592208862305\n",
      "Loss: 2.1974575519561768\n",
      "Loss: 2.197455883026123\n",
      "Loss: 2.1974539756774902\n",
      "Loss: 2.1974523067474365\n",
      "Loss: 2.197450637817383\n",
      "Loss: 2.197448968887329\n",
      "Loss: 2.1974470615386963\n",
      "Loss: 2.1974453926086426\n",
      "Loss: 2.197443723678589\n",
      "Loss: 2.197441816329956\n",
      "Loss: 2.1974401473999023\n",
      "Loss: 2.1974387168884277\n",
      "Loss: 2.197437047958374\n",
      "Loss: 2.197435140609741\n",
      "Loss: 2.1974334716796875\n",
      "Loss: 2.1974315643310547\n",
      "Loss: 2.19743013381958\n",
      "Loss: 2.1974282264709473\n",
      "Loss: 2.1974265575408936\n",
      "Loss: 2.19742488861084\n",
      "Loss: 2.197422981262207\n",
      "Loss: 2.1974215507507324\n",
      "Loss: 2.1974196434020996\n",
      "Loss: 2.197417974472046\n",
      "Loss: 2.197416305541992\n",
      "Loss: 2.1974146366119385\n",
      "Loss: 2.1974129676818848\n",
      "Loss: 2.197411298751831\n",
      "Loss: 2.1974093914031982\n",
      "Loss: 2.1974077224731445\n",
      "Loss: 2.197406053543091\n",
      "Loss: 2.197404146194458\n",
      "Loss: 2.1974027156829834\n",
      "Loss: 2.1974010467529297\n",
      "Loss: 2.197399377822876\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.197397470474243\n",
      "Loss: 2.1973958015441895\n",
      "Loss: 2.1973941326141357\n",
      "Loss: 2.197392225265503\n",
      "Loss: 2.197390556335449\n",
      "Loss: 2.1973891258239746\n",
      "Loss: 2.197387456893921\n",
      "Loss: 2.197385549545288\n",
      "Loss: 2.1973841190338135\n",
      "Loss: 2.1973822116851807\n",
      "Loss: 2.197380781173706\n",
      "Loss: 2.1973788738250732\n",
      "Loss: 2.1973772048950195\n",
      "Loss: 2.197375535964966\n",
      "Loss: 2.197373867034912\n",
      "Loss: 2.1973719596862793\n",
      "Loss: 2.1973702907562256\n",
      "Loss: 2.197368621826172\n",
      "Loss: 2.197366714477539\n",
      "Loss: 2.1973652839660645\n",
      "Loss: 2.1973636150360107\n",
      "Loss: 2.197361946105957\n",
      "Loss: 2.197360038757324\n",
      "Loss: 2.1973583698272705\n",
      "Loss: 2.197356700897217\n",
      "Loss: 2.197355031967163\n",
      "Loss: 2.1973533630371094\n",
      "Loss: 2.1973516941070557\n",
      "Loss: 2.197350025177002\n",
      "Loss: 2.1973483562469482\n",
      "Loss: 2.1973464488983154\n",
      "Loss: 2.197345018386841\n",
      "Loss: 2.197343349456787\n",
      "Loss: 2.1973416805267334\n",
      "Loss: 2.1973400115966797\n",
      "Loss: 2.197338342666626\n",
      "Loss: 2.197336435317993\n",
      "Loss: 2.1973350048065186\n",
      "Loss: 2.1973330974578857\n",
      "Loss: 2.197331428527832\n",
      "Loss: 2.1973297595977783\n",
      "Loss: 2.1973280906677246\n",
      "Loss: 2.19732666015625\n",
      "Loss: 2.197324752807617\n",
      "Loss: 2.1973230838775635\n",
      "Loss: 2.1973214149475098\n",
      "Loss: 2.197319746017456\n",
      "Loss: 2.1973180770874023\n",
      "Loss: 2.1973164081573486\n",
      "Loss: 2.197314739227295\n",
      "Loss: 2.197313070297241\n",
      "Loss: 2.1973114013671875\n",
      "Loss: 2.1973094940185547\n",
      "Loss: 2.19730806350708\n",
      "Loss: 2.1973063945770264\n",
      "Loss: 2.1973047256469727\n",
      "Loss: 2.197303056716919\n",
      "Loss: 2.1973013877868652\n",
      "Loss: 2.1972994804382324\n",
      "Loss: 2.197298049926758\n",
      "Loss: 2.197296142578125\n",
      "Loss: 2.1972947120666504\n",
      "Loss: 2.1972930431365967\n",
      "Loss: 2.197291135787964\n",
      "Loss: 2.19728946685791\n",
      "Loss: 2.1972880363464355\n",
      "Loss: 2.1972861289978027\n",
      "Loss: 2.197284698486328\n",
      "Loss: 2.1972827911376953\n",
      "Loss: 2.1972811222076416\n",
      "Loss: 2.197279691696167\n",
      "Loss: 2.1972780227661133\n",
      "Loss: 2.1972763538360596\n",
      "Loss: 2.197274684906006\n",
      "Loss: 2.197272777557373\n",
      "Loss: 2.1972711086273193\n",
      "Loss: 2.1972696781158447\n",
      "Loss: 2.197268009185791\n",
      "Loss: 2.1972663402557373\n",
      "Loss: 2.1972646713256836\n",
      "Loss: 2.19726300239563\n",
      "Loss: 2.197261333465576\n",
      "Loss: 2.1972596645355225\n",
      "Loss: 2.1972577571868896\n",
      "Loss: 2.197256326675415\n",
      "Loss: 2.1972546577453613\n",
      "Loss: 2.1972529888153076\n",
      "Loss: 2.197251319885254\n",
      "Loss: 2.1972496509552\n",
      "Loss: 2.1972479820251465\n",
      "Loss: 2.1972463130950928\n",
      "Loss: 2.197244882583618\n",
      "Loss: 2.1972432136535645\n",
      "Loss: 2.1972413063049316\n",
      "Loss: 2.197239875793457\n",
      "Loss: 2.1972382068634033\n",
      "Loss: 2.1972365379333496\n",
      "Loss: 2.197234630584717\n",
      "Loss: 2.197233200073242\n",
      "Loss: 2.1972315311431885\n",
      "Loss: 2.1972296237945557\n",
      "Loss: 2.197228193283081\n",
      "Loss: 2.1972265243530273\n",
      "Loss: 2.1972248554229736\n",
      "Loss: 2.19722318649292\n",
      "Loss: 2.197221517562866\n",
      "Loss: 2.1972198486328125\n",
      "Loss: 2.197218179702759\n",
      "Loss: 2.197216749191284\n",
      "Loss: 2.1972150802612305\n",
      "Loss: 2.1972131729125977\n",
      "Loss: 2.197211742401123\n",
      "Loss: 2.1972100734710693\n",
      "Loss: 2.1972084045410156\n",
      "Loss: 2.197206735610962\n",
      "Loss: 2.1972053050994873\n",
      "Loss: 2.1972036361694336\n",
      "Loss: 2.197201728820801\n",
      "Loss: 2.197200059890747\n",
      "Loss: 2.1971983909606934\n",
      "Loss: 2.1971967220306396\n",
      "Loss: 2.197195291519165\n",
      "Loss: 2.1971936225891113\n",
      "Loss: 2.1971919536590576\n",
      "Loss: 2.197190284729004\n",
      "Loss: 2.19718861579895\n",
      "Loss: 2.1971871852874756\n",
      "Loss: 2.1971852779388428\n",
      "Loss: 2.197183847427368\n",
      "Loss: 2.1971821784973145\n",
      "Loss: 2.1971805095672607\n",
      "Loss: 2.197178840637207\n",
      "Loss: 2.1971771717071533\n",
      "Loss: 2.1971755027770996\n",
      "Loss: 2.197174072265625\n",
      "Loss: 2.1971724033355713\n",
      "Loss: 2.1971707344055176\n",
      "Loss: 2.197169065475464\n",
      "Loss: 2.19716739654541\n",
      "Loss: 2.1971657276153564\n",
      "Loss: 2.1971640586853027\n",
      "Loss: 2.197162628173828\n",
      "Loss: 2.1971607208251953\n",
      "Loss: 2.1971592903137207\n",
      "Loss: 2.197157859802246\n",
      "Loss: 2.1971559524536133\n",
      "Loss: 2.1971542835235596\n",
      "Loss: 2.197152853012085\n",
      "Loss: 2.1971511840820312\n",
      "Loss: 2.1971492767333984\n",
      "Loss: 2.197147846221924\n",
      "Loss: 2.197146415710449\n",
      "Loss: 2.1971447467803955\n",
      "Loss: 2.1971428394317627\n",
      "Loss: 2.197141408920288\n",
      "Loss: 2.1971399784088135\n",
      "Loss: 2.1971383094787598\n",
      "Loss: 2.197136402130127\n",
      "Loss: 2.1971347332000732\n",
      "Loss: 2.1971333026885986\n",
      "Loss: 2.197131395339966\n",
      "Loss: 2.1971302032470703\n",
      "Loss: 2.1971282958984375\n",
      "Loss: 2.197126626968384\n",
      "Loss: 2.197125196456909\n",
      "Loss: 2.1971235275268555\n",
      "Loss: 2.1971218585968018\n",
      "Loss: 2.197120189666748\n",
      "Loss: 2.1971185207366943\n",
      "Loss: 2.197117328643799\n",
      "Loss: 2.197115421295166\n",
      "Loss: 2.1971137523651123\n",
      "Loss: 2.1971123218536377\n",
      "Loss: 2.197110414505005\n",
      "Loss: 2.197108745574951\n",
      "Loss: 2.1971073150634766\n",
      "Loss: 2.197105646133423\n",
      "Loss: 2.197103977203369\n",
      "Loss: 2.1971025466918945\n",
      "Loss: 2.197100877761841\n",
      "Loss: 2.197099208831787\n",
      "Loss: 2.1970977783203125\n",
      "Loss: 2.197096109390259\n",
      "Loss: 2.197094440460205\n",
      "Loss: 2.1970927715301514\n",
      "Loss: 2.1970913410186768\n",
      "Loss: 2.197089672088623\n",
      "Loss: 2.1970880031585693\n",
      "Loss: 2.1970863342285156\n",
      "Loss: 2.197084903717041\n",
      "Loss: 2.1970832347869873\n",
      "Loss: 2.1970815658569336\n",
      "Loss: 2.19707989692688\n",
      "Loss: 2.1970784664154053\n",
      "Loss: 2.1970767974853516\n",
      "Loss: 2.197075128555298\n",
      "Loss: 2.197073459625244\n",
      "Loss: 2.1970717906951904\n",
      "Loss: 2.197070360183716\n",
      "Loss: 2.197068691253662\n",
      "Loss: 2.1970670223236084\n",
      "Loss: 2.197065591812134\n",
      "Loss: 2.19706392288208\n",
      "Loss: 2.1970622539520264\n",
      "Loss: 2.1970608234405518\n",
      "Loss: 2.197059154510498\n",
      "Loss: 2.1970574855804443\n",
      "Loss: 2.1970560550689697\n",
      "Loss: 2.197054147720337\n",
      "Loss: 2.1970527172088623\n",
      "Loss: 2.1970512866973877\n",
      "Loss: 2.197049379348755\n",
      "Loss: 2.1970479488372803\n",
      "Loss: 2.1970462799072266\n",
      "Loss: 2.197044610977173\n",
      "Loss: 2.1970431804656982\n",
      "Loss: 2.1970417499542236\n",
      "Loss: 2.19704008102417\n",
      "Loss: 2.197038412094116\n",
      "Loss: 2.1970367431640625\n",
      "Loss: 2.197035312652588\n",
      "Loss: 2.197033643722534\n",
      "Loss: 2.1970319747924805\n",
      "Loss: 2.1970303058624268\n",
      "Loss: 2.197028875350952\n",
      "Loss: 2.1970272064208984\n",
      "Loss: 2.1970255374908447\n",
      "Loss: 2.19702410697937\n",
      "Loss: 2.1970221996307373\n",
      "Loss: 2.1970207691192627\n",
      "Loss: 2.197019100189209\n",
      "Loss: 2.1970174312591553\n",
      "Loss: 2.1970162391662598\n",
      "Loss: 2.197014331817627\n",
      "Loss: 2.1970129013061523\n",
      "Loss: 2.1970112323760986\n",
      "Loss: 2.197009563446045\n",
      "Loss: 2.1970081329345703\n",
      "Loss: 2.1970067024230957\n",
      "Loss: 2.197005033493042\n",
      "Loss: 2.1970033645629883\n",
      "Loss: 2.1970019340515137\n",
      "Loss: 2.19700026512146\n",
      "Loss: 2.1969985961914062\n",
      "Loss: 2.1969971656799316\n",
      "Loss: 2.196995258331299\n",
      "Loss: 2.196993827819824\n",
      "Loss: 2.1969921588897705\n",
      "Loss: 2.196990728378296\n",
      "Loss: 2.196989059448242\n",
      "Loss: 2.1969876289367676\n",
      "Loss: 2.196985960006714\n",
      "Loss: 2.19698429107666\n",
      "Loss: 2.1969828605651855\n",
      "Loss: 2.196981191635132\n",
      "Loss: 2.1969797611236572\n",
      "Loss: 2.1969780921936035\n",
      "Loss: 2.196976661682129\n",
      "Loss: 2.196974992752075\n",
      "Loss: 2.1969733238220215\n",
      "Loss: 2.196971893310547\n",
      "Loss: 2.196970224380493\n",
      "Loss: 2.1969685554504395\n",
      "Loss: 2.196967124938965\n",
      "Loss: 2.196965456008911\n",
      "Loss: 2.1969640254974365\n",
      "Loss: 2.196962356567383\n",
      "Loss: 2.196960926055908\n",
      "Loss: 2.1969592571258545\n",
      "Loss: 2.196957588195801\n",
      "Loss: 2.196956157684326\n",
      "Loss: 2.1969544887542725\n",
      "Loss: 2.1969528198242188\n",
      "Loss: 2.196951389312744\n",
      "Loss: 2.1969497203826904\n",
      "Loss: 2.196948528289795\n",
      "Loss: 2.196946620941162\n",
      "Loss: 2.1969451904296875\n",
      "Loss: 2.196943521499634\n",
      "Loss: 2.19694185256958\n",
      "Loss: 2.1969406604766846\n",
      "Loss: 2.196938991546631\n",
      "Loss: 2.196937322616577\n",
      "Loss: 2.1969356536865234\n",
      "Loss: 2.1969339847564697\n",
      "Loss: 2.196932554244995\n",
      "Loss: 2.1969308853149414\n",
      "Loss: 2.196929454803467\n",
      "Loss: 2.196928024291992\n",
      "Loss: 2.1969263553619385\n",
      "Loss: 2.196924924850464\n",
      "Loss: 2.196923017501831\n",
      "Loss: 2.1969215869903564\n",
      "Loss: 2.196920156478882\n",
      "Loss: 2.196918487548828\n",
      "Loss: 2.1969170570373535\n",
      "Loss: 2.1969153881073\n",
      "Loss: 2.196913957595825\n",
      "Loss: 2.1969122886657715\n",
      "Loss: 2.196910858154297\n",
      "Loss: 2.196908950805664\n",
      "Loss: 2.1969075202941895\n",
      "Loss: 2.196906089782715\n",
      "Loss: 2.1969046592712402\n",
      "Loss: 2.1969029903411865\n",
      "Loss: 2.196901321411133\n",
      "Loss: 2.196899652481079\n",
      "Loss: 2.1968984603881836\n",
      "Loss: 2.19689679145813\n",
      "Loss: 2.196895122528076\n",
      "Loss: 2.1968936920166016\n",
      "Loss: 2.196892023086548\n",
      "Loss: 2.196890354156494\n",
      "Loss: 2.1968889236450195\n",
      "Loss: 2.196887493133545\n",
      "Loss: 2.196885824203491\n",
      "Loss: 2.1968841552734375\n",
      "Loss: 2.196882724761963\n",
      "Loss: 2.1968812942504883\n",
      "Loss: 2.1968798637390137\n",
      "Loss: 2.19687819480896\n",
      "Loss: 2.1968765258789062\n",
      "Loss: 2.1968748569488525\n",
      "Loss: 2.196873426437378\n",
      "Loss: 2.1968719959259033\n",
      "Loss: 2.1968703269958496\n",
      "Loss: 2.196868896484375\n",
      "Loss: 2.1968672275543213\n",
      "Loss: 2.1968657970428467\n",
      "Loss: 2.196864366531372\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "for _ in range(1000):\n",
    "    logits = torch.matmul(x_oh.to(device), W.to(device))\n",
    "    counts = logits.exp() # Exponentiate to get the counts \n",
    "    probs = torch.div(counts, torch.sum(counts, dim=1, keepdims=True)) # Normalize the counts\n",
    "    loss = -probs[torch.arange(x_oh.shape[0]), ys].log().mean() #+ 0.01 * (W ** 2).mean() # Calculate the nll loss\n",
    "    print(f\"Loss: {loss.item()}\")\n",
    "    W.grad = None # clear the gradients\n",
    "    loss.backward()\n",
    "#     W.retain_grad()\n",
    "    W.data += -40 * W.grad # update the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "adced769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on bigram (gradient based): 2.4804\n",
      "Loss on trigram: 2.1969\n"
     ]
    }
   ],
   "source": [
    "# Comparing the loss of bigram model and trigram model\n",
    "print(\"Loss on bigram (gradient based): 2.4804\")\n",
    "print(f\"Loss on trigram: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "2dfb25d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([703, 27])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "6805dbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some names and compare with the names from counting method\n",
    "gen = torch.Generator(device=device).manual_seed(2147483647)\n",
    "output_gradient = []\n",
    "for _ in range(10):\n",
    "    out = []\n",
    "    char1 = '.'\n",
    "    char2 = '.'\n",
    "    \n",
    "    while True:\n",
    "        x_enc = torch.nn.functional.one_hot(torch.tensor([ptoi[(char1, char2)]]), num_classes=703).float().to(device)\n",
    "        \n",
    "        logits = torch.matmul(x_enc.to(device), W.to(device))\n",
    "        counts = logits.exp()\n",
    "        probs = torch.div(counts, torch.sum(counts, dim=1, keepdims=True))\n",
    "        \n",
    "        idx = torch.multinomial(probs, num_samples=1, replacement=True, generator=gen).item()\n",
    "        if idx == 0:\n",
    "            break\n",
    "        char1 = char2\n",
    "        char2 = itos[idx]\n",
    "        out.append(itos[idx])\n",
    "    output_gradient.append(\"\".join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "c6267e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting based:          \tGradient based: \n",
      "----------------------------------------------------\n",
      "junide                   \tkhen                \n",
      "jakasid                  \tephyling            \n",
      "prelay                   \tyulagolbiahen       \n",
      "adin                     \tramson              \n",
      "kairritoper              \tmacxonnan           \n",
      "sathen                   \trine                \n",
      "sameia                   \tdelenlian           \n",
      "yanileniassibduinrwin    \termarishan          \n",
      "lessiyanayla             \tany                 \n",
      "te                       \taleedon             \n"
     ]
    }
   ],
   "source": [
    "print(\"{:<25}\\t{}\".format(\"Counting based: \", \"Gradient based: \"))\n",
    "print(\"----------------------------------------------------\")\n",
    "for out1, out2 in zip(output_count, output_gradient):\n",
    "    print(\"{:<25}\\t{:<20}\".format(out1, out2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a307d4",
   "metadata": {},
   "source": [
    "### E02: split up the dataset randomly into 80% train set, 10% dev set, 10% test set. Train the bigram and trigram models only on the training set. Evaluate them on dev and test splits. What can you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "aaa81b80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([228146, 703]), torch.Size([228146]))"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_oh.shape, ys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "95865da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182517, 703]) torch.Size([182517])\n",
      "torch.Size([22815, 703]) torch.Size([22815])\n",
      "torch.Size([22814, 703]) torch.Size([22814])\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into train, validation and test sets\n",
    "train_idx, val_idx, test_idx = torch.utils.data.random_split(range(x_oh.shape[0]), [0.8, 0.1, 0.1])\n",
    "\n",
    "xs_train, ys_train = x_oh[train_idx], ys[train_idx]\n",
    "xs_val, ys_val = x_oh[val_idx], ys[val_idx]\n",
    "xs_test, ys_test = x_oh[test_idx], ys[test_idx]\n",
    "\n",
    "print(xs_train.shape, ys_train.shape)\n",
    "print(xs_val.shape, ys_val.shape)\n",
    "print(xs_test.shape, ys_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "83cac8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize new weights\n",
    "W = torch.randn((703, 27), requires_grad=True) # weights matrix\n",
    "R = 0.01 # regulatization loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "85d20070",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(inputs, weights):\n",
    "    \"\"\"Perform a matrix multiplication between the input and weights tensor and return probabilities after exponentiating and normalizing\"\"\"\n",
    "    logits = torch.matmul(inputs.to(device), weights.to(device))\n",
    "    counts = logits.exp()\n",
    "    probs = torch.div(counts, torch.sum(counts, dim=1, keepdims=True))\n",
    "    return probs\n",
    "\n",
    "def nll_loss(probs, n_inputs, labels):\n",
    "    \"\"\"Calculate the negative log likelihood loss and apply model smoothing with regularization\"\"\"\n",
    "    return -probs[torch.arange(n_inputs), labels].log().mean() + R * (W**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "4e0b6754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 2.4279 \t Val Loss: 2.4301\n",
      "Training Loss: 2.3998 \t Val Loss: 2.4028\n",
      "Training Loss: 2.3780 \t Val Loss: 2.3820\n",
      "Training Loss: 2.3607 \t Val Loss: 2.3656\n",
      "Training Loss: 2.3465 \t Val Loss: 2.3523\n",
      "Training Loss: 2.3347 \t Val Loss: 2.3414\n",
      "Training Loss: 2.3247 \t Val Loss: 2.3321\n",
      "Training Loss: 2.3162 \t Val Loss: 2.3243\n",
      "Training Loss: 2.3088 \t Val Loss: 2.3175\n",
      "Training Loss: 2.3023 \t Val Loss: 2.3116\n",
      "Training Loss: 2.2966 \t Val Loss: 2.3064\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "for i in range(1001):\n",
    "    probs = forward_pass(xs_train, W)\n",
    "    loss = nll_loss(probs, xs_train.shape[0], ys_train)\n",
    "    if i % 50 == 0:\n",
    "        val_probs = forward_pass(xs_val, W)\n",
    "        val_loss = nll_loss(val_probs, xs_val.shape[0], ys_val)\n",
    "        print(f\"Training Loss: {loss.item():.4f} \\t Val Loss: {val_loss.item():.4f}\")\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "    W.data += -30 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "63af3055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 2.3098\n"
     ]
    }
   ],
   "source": [
    "# Calculate the loss on the test set\n",
    "test_loss = nll_loss(forward_pass(xs_test, W), len(xs_test), ys_test)\n",
    "print(f\"Test loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869a69a3",
   "metadata": {},
   "source": [
    "### E03: use the dev set to tune the strength of smoothing (or regularization) for the trigram model - i.e. try many possibilities and see which one works best based on the dev set loss. What patterns can you see in the train and dev set loss as you tune this strength? Take the best setting of the smoothing and evaluate on the test set once and at the end. How good of a loss do you achieve?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

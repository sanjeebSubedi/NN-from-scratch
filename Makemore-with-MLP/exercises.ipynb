{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3afbc1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb311337",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../Makemore/names.txt\") as f:\n",
    "    names = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "943f9de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [name.strip() for name in names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "125ff2f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_set = sorted(list({c for name in names for c in name}))\n",
    "char_set.insert(0, '.')\n",
    "len(char_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "741f0d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = {char: i for i, char in enumerate(char_set)}\n",
    "itos = {i: char for char, i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2731153",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 3\n",
    "X, Y = [], []\n",
    "for name in names:\n",
    "    name = '.' * block_size + name + '.'\n",
    "    for i in range(len(name) - block_size):\n",
    "        context = name[i: i+block_size]\n",
    "        X.append([stoi[char] for char in context])\n",
    "        Y.append(stoi[name[i+block_size]])\n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d480a30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([228146, 3]), torch.Size([228146]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c754d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train samples: 182517\n",
      "Number of validation samples: 22815\n",
      "Number of test samples: 22814\n"
     ]
    }
   ],
   "source": [
    "train_idx, val_idx, test_idx = torch.utils.data.random_split(range(X.shape[0]), [0.8, 0.1, 0.1])\n",
    "X_train, Y_train = X[train_idx], Y[train_idx]\n",
    "X_val, Y_val = X[val_idx], Y[val_idx]\n",
    "X_test, Y_test = X[test_idx], Y[test_idx]\n",
    "print(f\"Number of train samples: {X_train.shape[0]}\")\n",
    "print(f\"Number of validation samples: {X_val.shape[0]}\")\n",
    "print(f\"Number of test samples: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "427c2e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((27, 20), generator=gen)\n",
    "W1 = torch.randn((60, 100), generator=gen)\n",
    "B1 = torch.randn(100, generator=gen)\n",
    "W2 = torch.randn((100, 100), generator=gen)\n",
    "B2 = torch.randn(100, generator=gen)\n",
    "W3 = torch.randn((100, 27), generator=gen)\n",
    "B3 = torch.randn(27, generator=gen)\n",
    "parameters = [C, W1, B1, W2, B2, W3, B3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "dc9b3de0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19467"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.nelement() for p in parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "7f0e155b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "9723e497",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def overall_train_loss(epoch_num):\n",
    "    out = torch.tanh(torch.matmul(C[X_train].view(-1, W1.shape[0]), W1) + B1)\n",
    "    out = torch.tanh(torch.matmul(out, W2) + B2)\n",
    "    logits = torch.matmul(out, W3) + B3\n",
    "    train_loss = F.cross_entropy(logits, Y_train)\n",
    "    print(f\"Epoch {epoch_num} \\t Loss: {train_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d42eb681",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60, 100])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "643fff84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 \t Loss: 20.473392486572266\n",
      "Epoch 5000 \t Loss: 2.4926857948303223\n",
      "Epoch 10000 \t Loss: 2.328019380569458\n",
      "Epoch 15000 \t Loss: 2.266166925430298\n",
      "Epoch 20000 \t Loss: 2.2342920303344727\n",
      "Epoch 25000 \t Loss: 2.2100491523742676\n",
      "Epoch 30000 \t Loss: 2.2153806686401367\n",
      "Epoch 35000 \t Loss: 2.1834499835968018\n",
      "Epoch 40000 \t Loss: 2.172321319580078\n",
      "Epoch 45000 \t Loss: 2.1695892810821533\n",
      "Epoch 50000 \t Loss: 2.164910316467285\n",
      "Epoch 55000 \t Loss: 2.1615078449249268\n",
      "Epoch 60000 \t Loss: 2.154019355773926\n",
      "Epoch 65000 \t Loss: 2.1530113220214844\n",
      "Epoch 70000 \t Loss: 2.1521811485290527\n",
      "Epoch 75000 \t Loss: 2.1407599449157715\n",
      "Epoch 80000 \t Loss: 2.1357221603393555\n",
      "Epoch 85000 \t Loss: 2.12994122505188\n",
      "Epoch 90000 \t Loss: 2.1308140754699707\n",
      "Epoch 95000 \t Loss: 2.128620147705078\n",
      "Epoch 100000 \t Loss: 2.126880407333374\n",
      "Epoch 105000 \t Loss: 2.119631052017212\n",
      "Epoch 110000 \t Loss: 2.1165781021118164\n",
      "Epoch 115000 \t Loss: 2.1115503311157227\n",
      "Epoch 120000 \t Loss: 2.1067399978637695\n",
      "Epoch 125000 \t Loss: 2.125293016433716\n",
      "Epoch 130000 \t Loss: 2.1123366355895996\n",
      "Epoch 135000 \t Loss: 2.115424633026123\n",
      "Epoch 140000 \t Loss: 2.108767032623291\n",
      "Epoch 145000 \t Loss: 2.1080567836761475\n",
      "Epoch 150000 \t Loss: 2.099893808364868\n",
      "Epoch 155000 \t Loss: 2.078667402267456\n",
      "Epoch 160000 \t Loss: 2.079122543334961\n",
      "Epoch 165000 \t Loss: 2.0752265453338623\n",
      "Epoch 170000 \t Loss: 2.075328826904297\n",
      "Epoch 175000 \t Loss: 2.076559066772461\n",
      "Epoch 180000 \t Loss: 2.0725882053375244\n",
      "Epoch 185000 \t Loss: 2.0721302032470703\n",
      "Epoch 190000 \t Loss: 2.0729880332946777\n",
      "Epoch 195000 \t Loss: 2.0717434883117676\n",
      "Epoch 200000 \t Loss: 2.068878412246704\n",
      "Epoch 205000 \t Loss: 2.069613218307495\n",
      "Epoch 210000 \t Loss: 2.068044424057007\n",
      "Epoch 215000 \t Loss: 2.0705292224884033\n",
      "Epoch 220000 \t Loss: 2.0688071250915527\n",
      "Epoch 225000 \t Loss: 2.0663485527038574\n",
      "Epoch 230000 \t Loss: 2.048068046569824\n",
      "Epoch 235000 \t Loss: 2.0471444129943848\n",
      "Epoch 240000 \t Loss: 2.046053886413574\n",
      "Epoch 245000 \t Loss: 2.047072649002075\n",
      "Epoch 250000 \t Loss: 2.0461883544921875\n",
      "Epoch 255000 \t Loss: 2.0451862812042236\n",
      "Epoch 260000 \t Loss: 2.046119213104248\n",
      "Epoch 265000 \t Loss: 2.044316530227661\n",
      "Epoch 270000 \t Loss: 2.0450618267059326\n",
      "Epoch 275000 \t Loss: 2.0447769165039062\n",
      "Epoch 280000 \t Loss: 2.045038938522339\n",
      "Epoch 285000 \t Loss: 2.04463529586792\n",
      "Epoch 290000 \t Loss: 2.0440895557403564\n",
      "Epoch 295000 \t Loss: 2.0437405109405518\n",
      "Epoch 300000 \t Loss: 2.043811559677124\n"
     ]
    }
   ],
   "source": [
    "for i in range(300001):\n",
    "    ix = torch.randint(0, X_train.shape[0], (64, ))\n",
    "    emb = C[X_train[ix]]\n",
    "    out = torch.tanh(torch.matmul(emb.view(-1, W1.shape[0]), W1) + B1)\n",
    "    out = torch.tanh(torch.matmul(out, W2) + B2)\n",
    "    logits = torch.matmul(out, W3) + B3\n",
    "    loss = F.cross_entropy(logits, Y_train[ix])\n",
    "    if i%5000 == 0:\n",
    "        overall_train_loss(i)\n",
    "    loss.backward()\n",
    "    lr_schedule = {0: 0.1, 150000: 0.05, 225000: 0.01}\n",
    "    for threshold, item in lr_schedule.items():\n",
    "        if i > threshold:\n",
    "            lr = item\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "        p.grad = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1031539b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 5.98593807220459\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    out = torch.tanh(torch.matmul(C[X_val].view(-1, W1.shape[0]), W1) + B1)\n",
    "    out = torch.tanh(torch.matmul(out, W2) + B2)\n",
    "    logits = torch.matmul(out, W3) + B3\n",
    "    val_loss = F.cross_entropy(logits, Y_val)\n",
    "    print(f\"Val loss: {val_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "fe0f1fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 2.144402265548706\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    out = torch.tanh(torch.matmul(C[X_test].view(-1, W1.shape[0]), W1) + B1)\n",
    "    out = torch.tanh(torch.matmul(out, W2) + B2)\n",
    "    logits = torch.matmul(out, W3) + B3\n",
    "    test_loss = F.cross_entropy(logits, Y_test)\n",
    "    print(f\"test loss: {test_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532678f0",
   "metadata": {},
   "source": [
    "### E01: I did not get around to seeing what happens when you initialize all weights and biases to zero. Try this and train the neural net. You might think either that 1) the network trains just fine or 2) the network doesn't train at all, but actually it is 3) the network trains but only partially, and achieves a pretty bad final performance. Inspect the gradients and activations to figure out what is happening and why the network is only partially training, and what part is being trained exactly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c2ee4167",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_zero = torch.randn((27, 20))\n",
    "w1_zero = torch.zeros((60, 300))\n",
    "b1_zero = torch.zeros(300)\n",
    "w2_zero = torch.zeros((300, 27))\n",
    "b2_zero = torch.zeros(27)\n",
    "params = [c_zero, w1_zero, b1_zero, w2_zero, b2_zero]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce42665d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in params:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6af65f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 \t Loss: 3.295837163925171\n",
      "Epoch 5001 \t Loss: 2.823450803756714\n",
      "Epoch 10001 \t Loss: 2.823431968688965\n",
      "Epoch 15001 \t Loss: 2.823232650756836\n",
      "Epoch 20001 \t Loss: 2.8230626583099365\n",
      "Epoch 25001 \t Loss: 2.8233225345611572\n",
      "Epoch 30001 \t Loss: 2.823059320449829\n",
      "Epoch 35001 \t Loss: 2.8232638835906982\n",
      "Epoch 40001 \t Loss: 2.823038339614868\n",
      "Epoch 45001 \t Loss: 2.823099136352539\n",
      "Epoch 50001 \t Loss: 2.8232247829437256\n",
      "Epoch 55001 \t Loss: 2.823425769805908\n",
      "Epoch 60001 \t Loss: 2.823354721069336\n",
      "Epoch 65001 \t Loss: 2.8230814933776855\n",
      "Epoch 70001 \t Loss: 2.823064088821411\n",
      "Epoch 75001 \t Loss: 2.823176622390747\n",
      "Epoch 80001 \t Loss: 2.8231592178344727\n",
      "Epoch 85001 \t Loss: 2.823040723800659\n",
      "Epoch 90001 \t Loss: 2.822995662689209\n",
      "Epoch 95001 \t Loss: 2.823046922683716\n"
     ]
    }
   ],
   "source": [
    "for i in range(100000):\n",
    "    ix = torch.randint(0, X_train.shape[0], (64, ))\n",
    "    emb = c_zero[X_train[ix]]\n",
    "#     print(f\"emb: {emb.shape}\")\n",
    "    out = torch.tanh(torch.matmul(emb.view(-1, w1_zero.shape[0]), w1_zero) + b1_zero)\n",
    "    logits = torch.matmul(out, w2_zero) + b2_zero\n",
    "    loss = F.cross_entropy(logits, Y_train[ix])\n",
    "    if i%5000 == 0:\n",
    "        with torch.no_grad():\n",
    "            out = torch.tanh(torch.matmul(c_zero[X_train].view(-1, w1_zero.shape[0]), w1_zero) + b1_zero)\n",
    "            logits = torch.matmul(out, w2_zero) + b2_zero\n",
    "            train_loss = F.cross_entropy(logits, Y_train)\n",
    "            print(f\"Epoch {i+1} \\t Loss: {train_loss}\")\n",
    "    for p in params:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    lr_schedule = {0: 0.1, 100000: 0.05, 200000: 0.01, 300000: 0.001}\n",
    "    for threshold, item in lr_schedule.items():\n",
    "        if i >= threshold:\n",
    "            lr = item\n",
    "    for p in params:\n",
    "#         print(f\"{lr=} {p.grad=}\")\n",
    "        p.data += -lr * p.grad\n",
    "#         p.grad = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "522b47c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n",
      "tensor(0.)\n",
      "tensor(0.)\n",
      "tensor(0.)\n",
      "tensor(-1.2340e-07)\n"
     ]
    }
   ],
   "source": [
    "for param in params:\n",
    "    print(torch.sum(param.grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5914ef",
   "metadata": {},
   "source": [
    "<p> It looks like all the gradients are zero except the gradients of the last bias tensor. That is why the optimization was quick and plateaued immediately at 2.82. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0479777f",
   "metadata": {},
   "source": [
    "### E02: BatchNorm, unlike other normalization layers like LayerNorm/GroupNorm etc. has the big advantage that after training, the batchnorm gamma/beta can be \"folded into\" the weights of the preceeding Linear layers, effectively erasing the need to forward it at test time. Set up a small 3-layer MLP with batchnorms, train the network, then \"fold\" the batchnorm gamma/beta into the preceeding Linear layer's W,b by creating a new W2, b2 and erasing the batch norm. Verify that this gives the same forward pass during inference. i.e. we see that the batchnorm is there just for stabilizing the training, and can be thrown out after training is done! pretty cool.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5690cc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((27, 15), generator=gen)\n",
    "W1 = torch.randn((45, 100), generator=gen)\n",
    "B1 = torch.randn(100, generator=gen)\n",
    "W2 = torch.randn((100, 100), generator=gen)\n",
    "B2 = torch.randn(100, generator=gen)\n",
    "W3 = torch.randn((100, 27), generator=gen)\n",
    "B3 = torch.randn(27, generator=gen)\n",
    "parameters = [C, W1, B1, W2, B2, W3, B3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2109263d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f17c18d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_norm = torch.nn.BatchNorm1d(100, eps=1e-5, momentum=0.22)\n",
    "batch_norm2 = torch.nn.BatchNorm1d(100, eps=1e-5, momentum=0.27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5a764c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.1329731941223145\n",
      "Loss: 2.1265265941619873\n",
      "Loss: 2.125951051712036\n",
      "Loss: 2.1251254081726074\n",
      "Loss: 2.1256470680236816\n",
      "Loss: 2.1250040531158447\n",
      "Loss: 2.1252458095550537\n",
      "Loss: 2.124931812286377\n",
      "Loss: 2.1244149208068848\n",
      "Loss: 2.1247334480285645\n",
      "Loss: 2.124326705932617\n",
      "Loss: 2.1239967346191406\n",
      "Loss: 2.1232070922851562\n",
      "Loss: 2.123250961303711\n",
      "Loss: 2.1231024265289307\n",
      "Loss: 2.123734951019287\n",
      "Loss: 2.1231186389923096\n",
      "Loss: 2.1229145526885986\n",
      "Loss: 2.122171401977539\n",
      "Loss: 2.122885227203369\n",
      "Loss: 2.122119903564453\n"
     ]
    }
   ],
   "source": [
    "for i in range(100001):\n",
    "    idx = torch.randint(0, X_train.shape[0], (64,))\n",
    "    emb = C[X_train[idx]]\n",
    "    out = torch.matmul(emb.view(-1, 45), W1) + B1\n",
    "    out = batch_norm(out)\n",
    "    out = torch.tanh(out)\n",
    "    out = torch.matmul(out, W2) + B2\n",
    "    out = batch_norm2(out)\n",
    "    out = torch.tanh(out)\n",
    "    out = torch.matmul(out, W3) + B3\n",
    "    loss = F.cross_entropy(out, Y_train[idx])\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    for p in parameters:\n",
    "        p.data += -0.01 * p.grad\n",
    "    if i % 5000 == 0:\n",
    "        with torch.no_grad():\n",
    "            out = torch.matmul(C[X_train].view(-1, W1.shape[0]), W1) + B1\n",
    "            out = batch_norm(out)\n",
    "            out = torch.tanh(out)\n",
    "            out = torch.matmul(out, W2) + B2\n",
    "            out = batch_norm2(out)\n",
    "            out = torch.tanh(out)\n",
    "            logits = torch.matmul(out, W3) + B3\n",
    "            train_loss = F.cross_entropy(logits, Y_train)\n",
    "            print(f\"Loss: {train_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "34f249a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 2.191786766052246\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    out = torch.matmul(C[X_val].view(-1, W1.shape[0]), W1) + B1\n",
    "    out = torch.tanh(batch_norm(out))\n",
    "    out = torch.matmul(out, W2) + B2\n",
    "    out = torch.tanh(batch_norm2(out))\n",
    "    logits = torch.matmul(out, W3) + B3\n",
    "    val_loss = F.cross_entropy(logits, Y_val)\n",
    "    print(f\"Val loss: {val_loss}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
